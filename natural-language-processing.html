<html>
    <head>
        <meta content="text/html; charset=UTF-8" http-equiv="content-type" />
        <link rel="stylesheet" href="css/natural-language-processing.css" />
        <link rel="stylesheet" href="css/page.css" />
    </head>
    <body class="c42">
        <p class="c2">
            <span class="c67">COMP3225 - Natural Language Processing</span>
        </p>
        <hr />
        <p class="c2 c25"><span class="c46 c75 c67 c29"></span></p>
        <h1 class="c74" id="h.2ze4caxrzpgs">
            <span class="c29 c34">Words</span>
        </h1>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Natural Language Processing: </span
            ><span
                >study of interactions between computers and human language, in
                particular how to program computers to process and analyse large
                amounts of </span
            ><span class="c49">natural language</span
            ><span class="c0">&nbsp;data</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <a id="t.11169bc6f13edd568016981a31e46130bafffee5"></a><a id="t.0"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c51" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">Sentence</span></p>
                    </td>
                    <td class="c48" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">Unit of written language</span>
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c51" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">Utterance</span></p>
                    </td>
                    <td class="c48" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">Unit of spoken language</span>
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c51" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">Word Form</span></p>
                    </td>
                    <td class="c48" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                >Inflected form as it appears in the corpus e.g. </span
                            ><span class="c8 c1">said</span>
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c51" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">Lemma</span></p>
                    </td>
                    <td class="c48" colspan="1" rowspan="1">
                        <p class="c2">
                            <span>Abstract root form of the word e.g. </span
                            ><span class="c8 c1">say</span>
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c51" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">Function Word</span></p>
                    </td>
                    <td class="c48" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                >Indicating a grammatical relationship e.g. </span
                            ><span class="c8 c1">by</span>
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c51" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">Types</span></p>
                    </td>
                    <td class="c48" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >Number of distinct words in a corpus</span
                            >
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c51" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">Tokens</span></p>
                    </td>
                    <td class="c48" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">Total number of words</span>
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Corpus:</span
            ><span class="c0"
                >&nbsp;collection of written texts having a specific author,
                language, time, place, and function</span
            >
        </p>
        <p class="c2">
            <span class="c11">Token:</span
            ><span class="c0"
                >&nbsp;symbol which determines how grammar and semantics can be
                understood</span
            >
        </p>
        <p class="c2">
            <span class="c11">Morpheme:</span
            ><span class="c0"
                >&nbsp;unit of language that cannot be divided further</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Regular Expressions</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_3l7voua00ry3-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>A </span><span class="c11">regular expression</span
                ><span class="c0"
                    >&nbsp;is an algebraic notation for characterising a set of
                    strings</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >Operators show how symbols are joined together</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >Capture groups find repeating groups in different
                    locations</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >Lookahead assertions match expressions without consuming
                    them</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0">Anchors mark particular places in text</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >Aliases a short versions of longer expressions</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Operators</span></p>
        <a id="t.fe71352ab5a5a372fca2ac6fe586288f47ceb8ec"></a><a id="t.1"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Operator</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Meaning</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Example</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">\</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">separator</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">/woodchuck/</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">[ ]</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">disjunction</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">/[abc]/</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">( )</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">precedence</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">/dogg(y|ies)/</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">{ }</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">repetition</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">/abc{3}/ or /abc{1,3}/</span>
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">?</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">optional</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2">
                            <span>/</span><span>colou?r</span
                            ><span class="c0">/</span>
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">^</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">negation</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">/^a/</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">.</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">wildcard</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">/.txt/</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">|</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">disjunction</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">/cat|dog/</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">*</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">zero or many</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">/[0-9]*/</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">+</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">one or many</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">/[0-9]+/</span></p>
                    </td>
                </tr>
                <tr class="c70">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">*?</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">non-greedy zero or many</span>
                        </p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">/[0-9]*?/</span></p>
                    </td>
                </tr>
                <tr class="c70">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">+?</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">non-greedy one or many</span>
                        </p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">/[0-9]+?/</span></p>
                    </td>
                </tr>
                <tr class="c70">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">\( \) \1</span></p>
                    </td>
                    <td class="c27" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">capture group</span></p>
                    </td>
                    <td class="c65" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >/I have a \(\w+\), do you want a \1?/</span
                            >
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Lookahead Assertions</span></p>
        <a id="t.187534adc40a04e2408e7a09df89054475b1649a"></a><a id="t.2"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Operator</span></p>
                    </td>
                    <td class="c19" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Meaning</span></p>
                    </td>
                    <td class="c79" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Example</span></p>
                    </td>
                </tr>
                <tr class="c61">
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">?:</span></p>
                        <p class="c2"><span class="c3">?=</span></p>
                        <p class="c2"><span class="c3">?&lt;=</span></p>
                        <p class="c2"><span class="c3">?!</span></p>
                        <p class="c2"><span class="c3">?&lt;!</span></p>
                    </td>
                    <td class="c19" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">non capturing group</span>
                        </p>
                        <p class="c2"><span class="c0">look ahead</span></p>
                        <p class="c2"><span class="c0">look behind</span></p>
                        <p class="c2">
                            <span class="c0">negative look ahead</span>
                        </p>
                        <p class="c2">
                            <span class="c0">negative look behind</span>
                        </p>
                    </td>
                    <td class="c79" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">(?:word)</span></p>
                        <p class="c2"><span class="c0">(?=word)</span></p>
                        <p class="c2"><span class="c0">(?&lt;=word)</span></p>
                        <p class="c2"><span class="c0">(?!word)</span></p>
                        <p class="c2"><span class="c0">(?&lt;!word)</span></p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Anchors</span></p>
        <a id="t.1dc7f01c11a1ac3c631056a690effcfad080d895"></a><a id="t.3"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c33" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Expression</span></p>
                    </td>
                    <td class="c58" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Meaning</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c33" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">^</span></p>
                    </td>
                    <td class="c58" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">start of line</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c33" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">$</span></p>
                    </td>
                    <td class="c58" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">end of line</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c33" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">\b</span></p>
                    </td>
                    <td class="c58" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">word boundary</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c33" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">\B</span></p>
                    </td>
                    <td class="c58" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">non-word boundary</span>
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Aliases</span></p>
        <a id="t.2ae34e58b69ed0aa9e9b2d74ac65f9e081284b11"></a><a id="t.4"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c33" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Expression</span></p>
                    </td>
                    <td class="c69" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Meaning</span></p>
                    </td>
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Expansion</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c33" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">\d</span></p>
                    </td>
                    <td class="c69" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">number</span></p>
                    </td>
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">[0-9]</span></p>
                    </td>
                </tr>
                <tr class="c70">
                    <td class="c33" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">\w</span></p>
                    </td>
                    <td class="c69" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">alphanumeric</span></p>
                        <p class="c2"><span class="c0">or underscore</span></p>
                    </td>
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">[a-z A-Z 0-9_]</span></p>
                    </td>
                </tr>
                <tr class="c84">
                    <td class="c33" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">\s</span></p>
                    </td>
                    <td class="c69" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">whitespace</span></p>
                    </td>
                    <td class="c47" colspan="1" rowspan="1">
                        <p class="c2">
                            <span>[</span><span>&#9251;</span
                            ><span class="c0">\r\t\n\f]</span>
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Text Normalisation</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Normalisation:</span
            ><span
                >&nbsp;words or tokens are put into standard format. For
                example, </span
            ><span class="c1">uh-huh</span><span>&nbsp;becomes </span
            ><span class="c8 c1">uh huh</span>
        </p>
        <p class="c2">
            <span class="c11">Case Folding:</span
            ><span class="c0"
                >&nbsp;all characters are mapped to lowercase</span
            >
        </p>
        <p class="c2">
            <span class="c11">Lemmatisation:</span
            ><span>&nbsp;reducing words to their root. For example, </span
            ><span class="c1">saying</span><span>&nbsp;becomes </span
            ><span class="c8 c1">say</span>
        </p>
        <ul class="c6 lst-kix_f5k0kqrp02lg-0 start">
            <li class="c2 c18 li-bullet-0">
                <span
                    >The Porter stemmer removes affixes from words. It is based
                    on a series of rewrite rules called a </span
                ><span class="c22">cascade </span
                ><span class="c0"
                    >which pipelines the output of each step into the next
                    step</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Sentence Segmentation:</span
            ><span class="c0"
                >&nbsp;separation of text into sentences using punctuation,
                markers, and symbols</span
            >
        </p>
        <ul class="c6 lst-kix_yv9fuvwl661k-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0">? and ! mark the end of sentences</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c1">. </span
                ><span class="c0"
                    >can mark the end of a sentence, an abbreviation, or
                    both</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c1">.</span
                ><span class="c0"
                    >&nbsp;takes precedence, then ? and !, then other
                    symbols</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_3fs7hbvg951t-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >UNIX can be used for na&iuml;ve word tokenisation and
                    normalisation</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >The command tr -sc &#39;A-Za-z&#39; &#39;\n&#39; &lt;
                    result.txt | sort | uniq finds and sorts all unique words in
                    text</span
                >
            </li>
        </ul>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">UNIX Commands</span></p>
        <a id="t.ad81823ddc7f3a6bc507b90bde770e843da3504f"></a><a id="t.5"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c30" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Command</span></p>
                    </td>
                    <td class="c15" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Meaning</span></p>
                    </td>
                    <td class="c44" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Example</span></p>
                    </td>
                </tr>
                <tr class="c70">
                    <td class="c30" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">tr</span></p>
                    </td>
                    <td class="c15" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">tokenise</span></p>
                    </td>
                    <td class="c44" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >tr -sc &#39;A-Za-z&#39; &#39;\n&#39;</span
                            >
                        </p>
                        <p class="c2">
                            <span class="c0"
                                >adds a new line after each character
                                sequence</span
                            >
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c30" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">sort</span></p>
                    </td>
                    <td class="c15" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">sort alphabetically</span>
                        </p>
                    </td>
                    <td class="c44" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >tr -sc &#39;A-Za-z&#39; &#39;\n&#39; |
                                sort</span
                            >
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c30" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">uniq</span></p>
                    </td>
                    <td class="c15" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">list unique elements</span>
                        </p>
                    </td>
                    <td class="c44" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >tr -sc &#39;A-Za-z&#39; &#39;\n&#39; | sort |
                                uniq</span
                            >
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Patterns</span></p>
        <a id="t.55fbabb0dd1811bfabab3c7f7b387335316c7305"></a><a id="t.6"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c40" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Expression</span></p>
                    </td>
                    <td class="c64" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Meaning</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c40" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">([A-Z]\.)+</span></p>
                    </td>
                    <td class="c64" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">abbreviations</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c40" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">\w+(-\w+)*</span></p>
                    </td>
                    <td class="c64" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">hyphenated words</span>
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c40" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">\&pound;?\d+(\.\d+)?</span>
                        </p>
                    </td>
                    <td class="c64" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">currency</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c40" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">\.\.\.</span></p>
                    </td>
                    <td class="c64" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">ellipsis</span></p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">BPE (Byte Pair Encoding):</span
            ><span class="c0"
                >&nbsp;form of data compression where the most common pair of
                consecutive bytes is replaced with a new byte</span
            >
        </p>
        <ul class="c6 lst-kix_zhkhjn2zdscp-0 start">
            <li class="c2 c18 li-bullet-0">
                <span
                    >starts with a vocabulary using characters [a-z] and tries
                    to learn </span
                ><span class="c1">k </span><span class="c0">new tokens</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>repeats </span><span class="c1">k </span
                ><span class="c0">times</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_zhkhjn2zdscp-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >selects the two most frequently adjacent symbols e.g.
                    (a,b)</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >adds a new merged symbol AB to the dictionary</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0">replaces each A B with AB</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Minimum Edit Distance</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Minimum edit distance</span
            ><span class="c0"
                >&nbsp;measures the similarity between strings, defined as the
                minimum number of editing operations, such as insertion,
                deletion, and substitution, needed to transform one string into
                another.</span
            >
        </p>
        <ul class="c6 lst-kix_txk0rv7whnwl-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c22">alignment </span
                ><span class="c0"
                    >shows the position of strings which provides the largest
                    similarity</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">i</span
                ><span class="c0">&nbsp;represents insertion</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">d</span
                ><span class="c0">&nbsp;represents deletion</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">s</span
                ><span class="c0">&nbsp;represents substitution</span>
            </li>
        </ul>
        <p class="c2">
            <span>The</span><span class="c22">&nbsp;Levenshtein distance</span
            ><span class="c0"
                >&nbsp;between two sequences is the simplest weighting factor in
                which each operation has a cost of 1.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_odeobao46r2g-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>Given two strings, </span><span class="c22">X</span
                ><span>&nbsp;of length </span><span class="c22">i</span
                ><span>&nbsp;and</span><span class="c22">&nbsp;Y</span
                ><span>&nbsp;of length </span><span class="c22">j</span
                ><span>, the edit distance is defined as </span
                ><span class="c28 c22">D(i,j)</span>
            </li>
        </ul>
        <p class="c2 c10">
            <span class="c11">D(i,j)</span><span>&nbsp;= min </span
            ><span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 238.67px;
                    height: 60px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image237.png"
                    style="
                        width: 238.67px;
                        height: 60px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 465.5px;
                    height: 337.12px;
                "
                ><img
                    alt="function MIN-EDIT-DISTANCE(source, target) returns min-distance 
n LENGTH(source) 
m LENGTH(target) 
Create a distance matrix distance[n+] ,m+l ] 
# Initialization: the zeroth and column is the distancefrom the empty string 
for each row i from 1 to n do 
D[i-l + 
for each column j from I to m do 
D[O,j-1] + ins-cost(targetlj]) 
# Recurrence relation: 
for each row i from 1 to n do 
for each column j from I to m do 
D[i&mdash;l,j&mdash;l] + sub-cost(source[i], targetW), 
D[i,j&mdash;l] + ins-cost(targetW)) 
# Termination 
return D[n,m] "
                    src="assets/natural-language-processing/image91.png"
                    style="
                        width: 465.5px;
                        height: 337.12px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c20">Evaluation</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Evaluation Techniques</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Extrinsic Evaluation: </span
            ><span class="c0"
                >evaluates the performance of an NLP component by embedding it
                in an application and measuring its performance</span
            >
        </p>
        <p class="c2">
            <span class="c11">Intrinsic Evaluation:</span
            ><span class="c0"
                >&nbsp;measures the quality of an NLP component independently of
                its application</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0">Steps in training and evaluation:</span>
        </p>
        <ul class="c6 lst-kix_yb7pr7lb5sao-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Linguistic Theories</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_yb7pr7lb5sao-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >AI develops linguistic theories about patterns, grammars,
                    and rules</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_yb7pr7lb5sao-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Statistical Patterns</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_yb7pr7lb5sao-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >AI infers statistical patterns and rules by examining text
                    corpora</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_yb7pr7lb5sao-0">
            <li class="c2 c18 li-bullet-0"><span class="c3">Training</span></li>
        </ul>
        <ul class="c6 lst-kix_yb7pr7lb5sao-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >a training dataset is used to fit a model</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0">typically 80% of data</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >uncovers dependencies and patterns, difference between
                    noise and signals, significance of words and phrases</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_yb7pr7lb5sao-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Validation</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_yb7pr7lb5sao-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >a validation dataset is used to provide an unbiased
                    evaluation of a model while tuning model
                    hyperparameters</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >allows tuning of hyperparameters to optimise a model</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0">typically 10% of data</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_yb7pr7lb5sao-0">
            <li class="c2 c18 li-bullet-0"><span class="c3">Testing</span></li>
        </ul>
        <ul class="c6 lst-kix_yb7pr7lb5sao-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >a test dataset is used to measure the validity and accuracy
                    of a model</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >measures only the final performance of a model</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0">typically 10% of data</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">corpora size</span
            ><span class="c0"
                >&nbsp;affects the quality of training on a model.</span
            >
        </p>
        <ul class="c6 lst-kix_2k9d4axiijeq-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >a larger corpora means a more varied language and better
                    training datda</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span
                    >the relationship between the size of the vocabulary and the
                    number of tokens N is given by the expression </span
                ><span class="c11">|V| = kN&beta;</span
                ><span class="c0"
                    >&nbsp;where k and &beta; are positive and &beta; is between
                    0 and 1</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >&beta; depends on the corpus size and context</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >|V| is usually limited to ~50,000 words due to GPU memory
                    restrictions</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >an OOV (out-of-vocabulary) word is a word which doesn&#39;t
                    appear in the training data</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >ignoring OOV words can lead to overfitting as they are not
                    analysed</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">K-Fold Cross-Validation:</span
            ><span class="c0"
                >&nbsp;statistical procedure used to evaluate the skill of a
                model on limited data</span
            >
        </p>
        <ul class="c6 lst-kix_eug57i1m4xez-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c22">Shuffle</span
                ><span class="c0">&nbsp;the dataset randomly</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c22">Split</span
                ><span class="c0">&nbsp;the dataset into K groups</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0">For each unique group:</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_eug57i1m4xez-1 start">
            <li class="c2 c32 li-bullet-0">
                <span>take the group as the </span
                ><span class="c28 c22">test set</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>take the remaining groups as the the </span
                ><span class="c28 c22">training set</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c22">fit</span
                ><span class="c0">&nbsp;a model on the training set</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c22">evaluate</span
                ><span class="c0">&nbsp;the model on the test set</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>retain the </span><span class="c22">evaluation score</span
                ><span class="c0">&nbsp;and discard the model</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_eug57i1m4xez-0">
            <li class="c2 c18 li-bullet-0">
                <span>Summarise the skill of the model as the </span
                ><span class="c28 c22">sample of model evaluation scores</span>
            </li>
        </ul>
        <p class="c2">
            <span class="c11">Random subsampling: </span
            ><span
                >evaluation technique similar to cross-validation, which selects
                subsets of data as the </span
            ><span class="c22">test set </span
            ><span class="c0"
                >randomly rather than using K fixed-size groups. This improves
                randomisation, but may not be representative of all data</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Evaluation Metrics</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">TP</span
            ><span class="c0"
                >&nbsp;(True Positive): &quot;yes&quot; data is correctly
                predicted as &quot;yes&quot;</span
            >
        </p>
        <p class="c2">
            <span class="c11">TN</span
            ><span class="c0"
                >&nbsp;(True Negative): &quot;no&quot; data is correctly
                predicted as &quot;no&quot;</span
            >
        </p>
        <p class="c2">
            <span class="c11">FP</span
            ><span class="c0"
                >&nbsp;(False Positive): &quot;no&quot; data is incorrectly
                predicted as &quot;yes&quot;</span
            >
        </p>
        <p class="c2">
            <span class="c11">FN</span
            ><span class="c0"
                >&nbsp;(False Negative): &quot;yes&quot; data in incorrectly
                predicted as &quot;no&quot;</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Accuracy </span><span class="c0">= </span>
        </p>
        <p class="c2">
            <span>proportion of </span><span class="c22">correct </span
            ><span class="c0">predictions</span>
        </p>
        <p class="c2">
            <span class="c81">&nbsp;</span
            ><span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 213.33px;
                    height: 29.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image198.png"
                    style="
                        width: 213.33px;
                        height: 29.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2">
            <span class="c11">Precision </span><span class="c0">=</span>
        </p>
        <p class="c2">
            <span>proportion of </span><span class="c22">correct positive </span
            ><span>predictions out of all </span
            ><span class="c22">positive </span><span>predictions</span
            ><span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 600px;
                    height: 45.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image158.png"
                    style="
                        width: 600px;
                        height: 45.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2">
            <span class="c0"
                >i.e. how many positive predictions were correct?</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Recall </span><span class="c0">=</span>
        </p>
        <p class="c2">
            <span>proportion of </span><span class="c22">correct positive </span
            ><span>predictions out of all </span
            ><span class="c22">correct </span><span>predictions</span
            ><span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 600px;
                    height: 45.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image50.png"
                    style="
                        width: 600px;
                        height: 45.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2">
            <span class="c0">i.e. how many correct predictions were made?</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">F1 </span
            ><span class="c0"
                >= &nbsp;harmonic mean of precision and recall</span
            >
        </p>
        <p class="c2">
            <span>&nbsp;</span
            ><span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 292px;
                    height: 42.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image159.png"
                    style="
                        width: 292px;
                        height: 42.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c25"><span class="c0"></span></p>
        <p class="c2">
            <span class="c11">ROUGE:</span
            ><span class="c0"
                >&nbsp;recall-based metric used to evaluate text
                summarisation</span
            >
        </p>
        <ul class="c6 lst-kix_fn7hs6jguyby-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >a good text summary includes many repeated phrases</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >a good text summary is similar to a human-generated
                    version</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >ROUGE-n is the proportion of n-grams / significant phrases
                    which match reference summaries</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">BLEU:</span
            ><span class="c0"
                >&nbsp;precision-based metric used to evaluate text
                translations</span
            >
        </p>
        <ul class="c6 lst-kix_jee10hc8gqo-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >measures how closely a machine translation matches a
                    human-generated version</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >measures how many n-word phrases match reference
                    translations</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >uses a function of n-gram precision over all
                    sentences</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >BP is the brevity penalty which penalises short
                    translations, p is precision</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >LF is the length of the reference translation, LS is the
                    length of the system translation</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>BLEU = BP </span
            ><span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 217.33px;
                    height: 25.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image59.png"
                    style="
                        width: 217.33px;
                        height: 25.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">BP = min(1, exp(1 - LF/LS))</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">PERPLEXITY:</span
            ><span class="c0"
                >&nbsp;statistical metric used to evaluate language models</span
            >
        </p>
        <ul class="c6 lst-kix_yt21lxf3k0mn-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >measures how well a vocabulary predicts target text</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >uses the probability of all words in text appearing in the
                    same order</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >the perplexity of a set is its inverse probability
                    normalized by the number of words</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >the perplexity of a set W equals PP(W) where &Pi;
                    represents the product</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >minimising the perplexity is equivalent to maximising the
                    set probability</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 210.67px;
                    height: 26.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image134.png"
                    style="
                        width: 210.67px;
                        height: 26.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 212px;
                    height: 46.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image111.png"
                    style="
                        width: 212px;
                        height: 46.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 266.67px;
                    height: 46.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image38.png"
                    style="
                        width: 266.67px;
                        height: 46.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c20">N-grams</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0"
                >Probabilities are useful for identifying words where the input
                is noisy, ambiguous, or broken up.</span
            >
        </p>
        <ul class="c6 lst-kix_5iqlwn2vmrc7-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Speech Recognition</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_5iqlwn2vmrc7-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >recognition and translation of spoken language into written
                    text</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >the model predicts which words are most likely to be
                    spoken</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_5iqlwn2vmrc7-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Spelling &amp; Grammar Correction</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_5iqlwn2vmrc7-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >checking for misspellings and grammar errors</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >the model predicts which spellings are more likely to be
                    correct</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_5iqlwn2vmrc7-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Machine Translation</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_5iqlwn2vmrc7-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0">translation of one language to another</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >the model predicts which translation is most likely to be
                    correct</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_5iqlwn2vmrc7-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Alternative Communication</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_5iqlwn2vmrc7-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >translation of physical movements such as eye gaze to
                    spoken text</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >the model predicts which words are most likely to be
                    spoken</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">N-Gram:</span
            ><span>&nbsp;contiguous sequence of </span><span class="c1">n</span
            ><span>&nbsp;items. The </span><span class="c11">n-gram model</span
            ><span class="c0"
                >&nbsp;calculates the probability of the next word in some text,
                given the previous n-1 words</span
            >
        </p>
        <ul class="c6 lst-kix_evjlamgjar3m-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c22">Unigram:</span
                ><span class="c0"
                    >&nbsp;single word e.g. &quot;sequence&quot;</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c22">Bigram:</span
                ><span class="c0"
                    >&nbsp;two-word sequence e.g. &quot;sequence of&quot;</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c22">Trigram:</span
                ><span class="c0"
                    >&nbsp;three-word sequence e.g. &quot;sequence of
                    words&quot;</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">N-gram Probabilities</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Assume </span><span class="c11">P(w|h)</span
            ><span>&nbsp;is the probability of word </span
            ><span class="c1">w</span><span>&nbsp;given the history </span
            ><span class="c1">h</span
            ><span>. Suppose the history is &quot;</span
            ><span class="c1">its water is so transparent that&quot;</span
            ><span>, </span><span class="c1">w </span><span>is </span
            ><span class="c1">&quot;the&quot;</span><span>, and </span
            ><span class="c1">C </span><span>means </span
            ><span class="c1">count</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 198.67px;
                    height: 36px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image243.png"
                    style="
                        width: 198.67px;
                        height: 36px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 502.67px;
                    height: 29.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image65.png"
                    style="
                        width: 502.67px;
                        height: 29.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Assume </span><span class="c11">P(w</span
            ><span class="c7 c11">1:n)</span
            ><span>&nbsp;is the probability of a sequence </span
            ><span class="c1">w</span><span class="c7 c1">1</span
            ><span class="c1">, &hellip;, w</span><span class="c7 c1">n</span
            ><span>. We can use</span
            ><span class="c22">&nbsp;joint probability</span
            ><span class="c0">&nbsp;to predict this value:</span>
        </p>
        <p class="c2 c10"><span class="c46 c66 c29">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c1">P(w</span><span class="c7 c1">1</span
            ><span class="c1">) = P(w</span><span class="c7 c1">1</span
            ><span class="c8 c1">)</span>
        </p>
        <p class="c2 c10">
            <span class="c1">P(w</span><span class="c7 c1">1</span
            ><span class="c1">,w</span><span class="c7 c1">2</span
            ><span class="c1">) = P(w</span><span class="c7 c1">1</span
            ><span class="c1">|w</span><span class="c7 c1">2</span
            ><span class="c1">) * P(w</span><span class="c7 c1">1</span
            ><span class="c1 c8">)</span>
        </p>
        <p class="c2 c10">
            <span class="c1">P(w</span><span class="c7 c1">1</span
            ><span class="c1">,w</span><span class="c7 c1">2</span
            ><span class="c1">,w</span><span class="c7 c1">3</span
            ><span class="c1">) = P(w</span><span class="c7 c1">3</span
            ><span class="c1">|w</span><span class="c7 c1">1</span
            ><span class="c1">,w</span><span class="c7 c1">2</span
            ><span class="c1">) * P(w</span><span class="c7 c1">1</span
            ><span class="c1">,w</span><span class="c7 c1">2</span
            ><span class="c8 c1">)</span>
        </p>
        <p class="c2 c10">
            <span class="c1">P(w</span><span class="c7 c1">1</span
            ><span class="c1">,w</span><span class="c7 c1">2</span
            ><span class="c1">,w</span><span class="c7 c1">3</span
            ><span class="c1">,w</span><span class="c7 c1">4</span
            ><span class="c1">) = P(w</span><span class="c7 c1">4</span
            ><span class="c1">|w</span><span class="c7 c1">1</span
            ><span class="c1">,w</span><span class="c7 c1">2</span
            ><span class="c1">,w</span><span class="c7 c1">3</span
            ><span class="c1">) * P(w</span><span class="c7 c1">1</span
            ><span class="c1">,w</span><span class="c7 c1">2</span
            ><span class="c1">,w</span><span class="c7 c1">3</span
            ><span class="c8 c1">)</span>
        </p>
        <p class="c2 c10">
            <span class="c1">=&gt; </span><span class="c11 c1">P(w</span
            ><span class="c7 c11 c1">1</span
            ><span class="c11 c1">, &hellip;, w</span
            ><span class="c7 c11 c1">n</span><span class="c11 c1">) = P(w</span
            ><span class="c7 c11 c1">n</span><span class="c11 c1">|w</span
            ><span class="c7 c11 c1">1</span
            ><span class="c11 c1">, &hellip;, w</span
            ><span class="c7 c11 c1">n-1</span
            ><span class="c11 c1">) * P(w</span><span class="c7 c11 c1">1</span
            ><span class="c11 c1">, &hellip;, w</span
            ><span class="c7 c11 c1">n-1</span
            ><span class="c38 c11 c1 c63">)</span>
        </p>
        <p class="c2 c10"><span class="c3">&nbsp;</span></p>
        <p class="c2">
            <span>Applying the </span
            ><span class="c22">chain rule of probability</span
            ><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 366.67px;
                    height: 81.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image143.png"
                    style="
                        width: 366.67px;
                        height: 81.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Assume </span><span class="c11">P(w</span
            ><span class="c7 c11">n</span><span class="c11">|w</span
            ><span class="c7 c11">n-1</span><span class="c11">)</span
            ><span
                >&nbsp;is the probability of a bigram, i.e. the probability of a
                word </span
            ><span class="c1">w</span><span class="c7 c1">n </span
            ><span class="c0"
                >given all previous words by using only the probability of the
                previous word.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">Markov model</span
            ><span class="c0"
                >&nbsp;is a probabilistic model which assumes it can predict the
                probability of some future event from its current state.</span
            >
        </p>
        <ul class="c6 lst-kix_kr17xbhrw8bw-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >A Markov assumption assumes that the probability of a word
                    depends only on the previous word</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span
                    >The bigram can be generalised to the n-gram, which looks </span
                ><span class="c1">n-1 </span
                ><span class="c0">words into the past</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >The N-gram approximation of the conditional probability of
                    the next word in a sequence is given by:</span
                >
            </li>
        </ul>
        <p class="c2 c55"><span class="c3">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c11 c1">P(w</span><span class="c7 c11 c1">n</span
            ><span class="c11 c1">|w</span><span class="c7 c11 c1">n-1</span
            ><span class="c11 c1">) &asymp; P(w</span
            ><span class="c7 c11 c1">n-N+1 : n-1</span
            ><span class="c11 c1">|w</span><span class="c7 c11 c1">n-1</span
            ><span class="c38 c11 c1 c63">)</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c1"
                >Given the bigram assumption for the probability of an
                individual word, we can calculate the probability of a sequence
                of words w1:n using </span
            ><span class="c1 c22">substitution</span
            ><span class="c8 c1">:</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 226.67px;
                    height: 65.33px;
                "
                ><img
                    alt="P(WkIWk-&#305;) "
                    src="assets/natural-language-processing/image64.png"
                    style="
                        width: 226.67px;
                        height: 65.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">MLE (Maximum Likelihood Estimation):</span
            ><span
                >&nbsp;selection of the most suitable set of bigram parameters
                to maximise the performance of a model in predicting the </span
            ><span class="c1">n</span><span class="c21 c1">th</span
            ><span class="c0">&nbsp;word in some text</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>To calculate the bigram probability of a word </span
            ><span class="c1">y </span><span>given the previous word </span
            ><span class="c1">x</span
            ><span>, we can calculate the count of the bigram </span
            ><span class="c1">C(xy) </span
            ><span>and normalise by the sum of all bigrams starting with </span
            ><span class="c1">x</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 230.67px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image186.png"
                    style="
                        width: 230.67px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >We can simplify this equation by simplifying the denominator to
                the unigram count for </span
            ><span class="c1">w</span><span class="c7 c1">n-1</span
            ><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 217.33px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image129.png"
                    style="
                        width: 217.33px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0"
                >For the general case of MLE n-gram parameter estimation:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 280px;
                    height: 69.33px;
                "
                ><img
                    alt="ven) "
                    src="assets/natural-language-processing/image80.png"
                    style="
                        width: 280px;
                        height: 69.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Large n-gram probabilities must be stored as </span
            ><span class="c11">log probabilities</span
            ><span class="c0"
                >&nbsp;to avoid numerical underflow. Adding in log space is
                equivalent to multiplying in linear space, so log probabilities
                are combined by addition. The original values can be restored by
                taking the exponential:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 388px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image199.png"
                    style="
                        width: 388px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Limitations</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c3">Unknown Words</span></p>
        <ul class="c6 lst-kix_3vu8eluuo1dq-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >OOV (out-of-vocabulary) words appear in test sets</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >open vocabulary systems replace OOV words with &lt;UNK&gt;
                    tokens</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >&lt;UNK&gt; tokens can be associated with OOV words based
                    on their frequency</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >&lt;UNK&gt; tokens can be used as a single word in the
                    vocabulary</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >&lt;UNK&gt; tokens often cause overfitting</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c3">Zero Probabilities</span></p>
        <ul class="c6 lst-kix_y8x3ola90hqy-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >models can predict a zero probability for n-grams which
                    appear in test sets but not training and validation
                    sets</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >any joint probabilities with these n-grams will also be
                    zero</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c3">Sparse Data</span></p>
        <ul class="c6 lst-kix_kfa12yeutu6i-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >Even when the vocabulary size |V| is large, only a tiny
                    minority of possible n-grams exist in any corpus</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>P(w</span><span class="c7">k</span><span>|w</span
                ><span class="c7">k-</span
                ><span>1) = 0 for most sequences w</span
                ><span class="c7">k-1</span><span>,w</span
                ><span class="c7 c29 c46">k</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0">This produces sparse matrices</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>More sparse as </span><span class="c1">n </span
                ><span class="c0">increases</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Smoothing</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Smoothing</span
            ><span class="c0"
                >&nbsp;prevents language models from assigning a probability of
                zero to unseen n-grams, by taking some probability from frequent
                n-grams and giving it to unseen n-grams.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>In </span><span class="c11">Laplace smoothing</span
            ><span
                >, one is added to all bigram counts before they are converted
                to probabilities, removing any zero values. The MLE for a
                unigram of word </span
            ><span class="c1">w</span><span class="c7 c1">i </span
            ><span>is its count </span><span class="c1">c</span
            ><span class="c7 c1">i </span
            ><span>normalised by the total number of tokens </span
            ><span class="c1">N</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 124px;
                    height: 29.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image84.png"
                    style="
                        width: 124px;
                        height: 29.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Laplace smoothing </span><span class="c22">adds one</span
            ><span
                >&nbsp;to each word count and normalises by the vocabulary size </span
            ><span class="c1">V</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 188px;
                    height: 32px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image234.png"
                    style="
                        width: 188px;
                        height: 32px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>It is more convenient to use the </span
            ><span class="c22">adjusted count </span
            ><span class="c1 c22">c*</span
            ><span class="c0"
                >&nbsp;to allow easier comparison with other MLE values.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 174.67px;
                    height: 32px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image213.png"
                    style="
                        width: 174.67px;
                        height: 32px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The level of smoothing is defined by the </span
            ><span class="c22">relative discount </span
            ><span class="c1 c22">d</span><span class="c7 c1 c22">c</span
            ><span class="c0"
                >, the ratio of adjusted counts to original counts.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 106.67px;
                    height: 32px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image53.png"
                    style="
                        width: 106.67px;
                        height: 32px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >For bigrams, the unigram count must be augmented by the
                vocabulary size </span
            ><span class="c1">V</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 277.33px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image244.png"
                    style="
                        width: 277.33px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>In </span><span class="c11">add-k smoothing</span
            ><span>, a fractional amount </span><span class="c1">k</span
            ><span class="c0"
                >&nbsp;is added to all bigram counts before they are converted
                to probabilities.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 273.33px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image149.png"
                    style="
                        width: 273.33px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >Interpolation combines several n-gram estimators to give a
                weighted value. In linear interpolation, n-grams are combined by
                applying a weight </span
            ><span class="c1">&lambda;</span
            ><span class="c0">&nbsp;where the sum of weights is 1.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 262.67px;
                    height: 86.67px;
                "
                ><img
                    alt="P(WnIWn-2Wn-1) &mdash; 
P(wn I 
4-A2P(Wn I Wn&mdash; 1 ) "
                    src="assets/natural-language-processing/image139.png"
                    style="
                        width: 262.67px;
                        height: 86.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Backoff</span
            ><span
                >&nbsp;estimates the probability of an n-gram which has no
                appearances by using a subset of the n-gram. For example, a
                trigram </span
            ><span class="c1">w</span><span class="c7 c1">n-2</span
            ><span class="c1">w</span><span class="c7 c1">n-1</span
            ><span class="c1">w</span><span class="c7 c1">n </span
            ><span>is replaced with the bigram </span><span class="c1">w</span
            ><span class="c7 c1">n-1</span><span class="c1">w</span
            ><span class="c7 c1">n </span><span>or the unigram </span
            ><span class="c1">w</span><span class="c7 c1">n</span
            ><span class="c0"
                >. If an n-gram has zero occurrences, the algorithm backs off to
                the (n-1)-gram until an occurrence is found.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Katz Backoff</span
            ><span>&nbsp;uses a parameter </span><span class="c1">&alpha;</span
            ><span class="c0"
                >&nbsp;to distribute the probability mass correctly.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 538.67px;
                    height: 54.67px;
                "
                ><img
                    alt="PBO(WnlWn-N+lm-l) = 
otherwise. "
                    src="assets/natural-language-processing/image187.png"
                    style="
                        width: 538.67px;
                        height: 54.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c8 c1">BO = backoff, N = n-gram length</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Kneser-Ney smoothing</span
            ><span class="c0"
                >&nbsp;is a commonly used n-gram smoothing method. It subtracts
                an absolute discount d from each count, which helps to scale
                lower-order n-grams.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 377.33px;
                    height: 46.67px;
                "
                ><img
                    alt="PAbsoluteDiscounting (Wi Wi&mdash; I ) &macr; 
EvC(Wi-l V) "
                    src="assets/natural-language-processing/image200.png"
                    style="
                        width: 377.33px;
                        height: 46.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title="" /></span
            ><sup><a href="#cmnt1" id="cmnt_ref1">[a]</a></sup>
        </p>
        <p class="c2">
            <span
                >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span
            ><span class="c1"
                >&nbsp;bigram &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span
            ><span class="c1">&lambda;</span
            ><span class="c8 c1">&nbsp;* unigram</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Kneser-Ney smoothing uses the </span
            ><span class="c11">continuation</span
            ><span
                >&nbsp;probability which estimates how frequently a word </span
            ><span class="c1">w</span
            ><span
                >&nbsp;appears as a unique continuation of an n-gram. It is
                calculated by the frequency of the n-gram containing </span
            ><span class="c1">w </span
            ><span>over the frequency of all n-grams with matching </span
            ><span class="c1">n</span><span>. For example, </span
            ><span class="c1">&quot;Great Britain&quot; </span
            ><span class="c0"
                >has a low continuation value because it only appears in a
                single context.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 326.67px;
                    height: 134.67px;
                "
                ><img
                    alt="The size of the set of words v, that w appears after, 
ie where the count of (v,w) is greater than O. 
: C(vw) &gt; 
PCONTINUATION(W) 
: C(U&#39;w&#39;) &gt; 
Equation 3.32 
The size of the set of adjacent word pairs (u&#39;, w&#39;) 
ie where the count of (u&#39;,w&#39;) is greater than O. "
                    src="assets/natural-language-processing/image90.png"
                    style="
                        width: 326.67px;
                        height: 134.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Stupid Backoff</span
            ><span
                >&nbsp;is a simpler solution used for large-scale text corpora,
                which is inexpensive to train on large datasets and approaches
                the accuracy of Kneser-Ney smoothing as the size of the training
                set increases. Stupid Backoff does not discount higher-order
                probabilities. If a higher-order n-gram has no occurrences, the
                algorithm backs off to a lower-order n-gram weighted by a fixed
                weight </span
            ><span class="c1">S</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 446.67px;
                    height: 82.67px;
                "
                ><img
                    alt="count(nf-&#39; ) 
otherwise 
count ( w) "
                    src="assets/natural-language-processing/image165.png"
                    style="
                        width: 446.67px;
                        height: 82.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c50">Parts of Speech</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">POS Tagging</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">part of speech</span
            ><span>&nbsp;is a </span><span class="c11">category</span
            ><span
                >&nbsp;to which a word is assigned in accordance with its </span
            ><span class="c11">syntactic functions</span
            ><span>. In English the main parts of speech are </span
            ><span class="c1"
                >noun, pronoun, adjective, determiner, verb, adverb,
                preposition, conjunction, </span
            ><span>and</span><span class="c8 c1">&nbsp;interjection.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Parts of speech fall into two categories. </span
            ><span class="c11">Closed classes</span
            ><span
                >&nbsp;have a fixed membership where specific words belong.
                These are typically function words such as </span
            ><span class="c1">of, it, and, you</span><span>.</span
            ><span class="c11">&nbsp;Open classes</span
            ><span class="c0"
                >&nbsp;such as nouns and verbs are flexible and are continually
                growing to accommodate new words.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">POS Tag:</span
            ><span class="c0"
                >&nbsp;label assigned to tokens in a text corpus to indicate the
                part of speech and often other grammatical categories such as
                tense, number, and case</span
            >
        </p>
        <p class="c2">
            <span class="c11">POS tagging:</span
            ><span
                >&nbsp;process of assigning a part of speech to each word in
                text. An</span
            ><span class="c22">&nbsp;input sequence </span
            ><span class="c1">x</span><span class="c7 c1">1</span
            ><span class="c1">, &hellip;, x</span><span class="c7 c1">n </span
            ><span>of </span><span class="c22">tokenised words</span
            ><span>&nbsp;is converted into an </span
            ><span class="c22">output sequence </span><span class="c1">y</span
            ><span class="c7 c1">1</span><span class="c1">, &hellip;, y</span
            ><span class="c7 c1">n </span><span>of </span
            ><span class="c22">tags</span><span>. Tagging is a </span
            ><span class="c22">disambiguation</span
            ><span class="c0"
                >&nbsp;task because words are ambiguous and may have various
                possible tags. POS tagging resolves ambiguities and selects the
                correct POS tag for each word.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 486.67px;
                    height: 277.33px;
                "
                ><img
                    alt="5 
Tag 
ADJ 
ADV 
NOUN 
VERB 
PROPN 
INT J 
ADP 
AUX 
CCONJ 
DET 
NUM 
PART 
PRON 
SCONJ 
PUNCT 
SYM 
Description 
Adjective: noun modifiers describing properties 
Adverb: verb modifiers of time, place, manner 
words for persons, places, things, etc. 
words for actions and processes 
Proper noun: name of a person, organization, place, etc.. 
Interjection: exclamation, greeting, yes/no response, etc. 
Adposition (Preposition/Postposition): marks a noun&#39;s 
spacial, temporal, or other relation 
Auxiliary: helping verb marking tense, aspect, mood, etc., 
Coordinating Conjunction: joins two phrases/clauses 
Determiner: marks noun phrase properties 
Numeral 
Particle: a preposition-like form used together with a verb 
Pronoun: a shorthand for referring to an entity or event 
Subordinating Conjunction: joins a main clause with a 
subordinate clause such as a sentential complement 
Punctuation 
Symbols like $ or emoji 
Other 
Example 
red, young, awesome 
very, slowly, home, yesterday 
algorithm, cat, mango, beauty 
draw, provide, go 
Regina, IBM, Colorado 
oh, um, yes, hello 
in, on, by under 
can, may, should, are 
and, or, but 
a, an, the, this 
one, two, first, second 
up, down, on, off, in, out, at, by 
she, who, I, others 
that, which 
asdf, qwfg "
                    src="assets/natural-language-processing/image236.png"
                    style="
                        width: 486.67px;
                        height: 277.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c3">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 486.67px;
                    height: 192px;
                "
                ><img
                    alt='Tag 
CC 
CD 
DT 
EX 
FW 
IN 
JJ 
J JR 
JJS 
IRS 
MD 
NN 
Description 
coord. conj. 
cardinal number 
determiner 
existential &#39;there&#39; 
foreign word 
preposi tion/ 
subordin-conj 
adjective 
comparative adj 
superlative adj 
list item marker 
modal 
Sing or mass noun 
Example 
and, but, or 
one, two 
a, the 
there 
mea culpa 
of, in, by 
ye Ilow 
bigger 
wildest 
1, 2, One 
can, should 
llama 
Tag 
NNP 
NNPS 
NNS 
PDT 
POS 
PRP 
PRP$ 
RB 
RBR 
RB S 
RP 
Description 
proper noun, Sing. 
proper noun, plu. 
noun, plural 
predeterminer 
possessive ending 
personal pronoun 
possess. pronoun 
adverb 
comparative adv 
superlatv. adv 
particle 
symbol 
Example 
IBM 
Carolinas 
llamas 
all, both 
&#39;s 
I, you, he 
you&#39;; one&#39;s 
quickly 
faster 
faste st 
up, off 
Tag 
TO 
UH 
VB 
VBD 
VBG 
VBN 
VBP 
VBZ 
WD T 
WP 
WPS 
WRB 
Description 
"to" 
interjection 
verb base 
verb past tense 
verb gerund 
verb past partici- 
ple 
verb non-3sg-pr 
verb 3sg pres 
wh-determ. 
wh-pronoun 
wh-possess. 
wh-adverb 
Example 
to 
ah, oops 
eat 
ate 
eating 
eaten 
eat 
eats 
which, that 
what, who 
whose 
how, where '
                    src="assets/natural-language-processing/image240.png"
                    style="
                        width: 486.67px;
                        height: 192px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c3">&nbsp;</span></p>
        <p class="c2"><span class="c26">Hidden Markov Models</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A</span
            ><span class="c11">&nbsp;hidden Markov model (HMM)</span
            ><span>&nbsp;is a </span><span class="c22">probabilistic </span
            ><span>sequence model. Given a sequence of </span
            ><span class="c22">tokens</span><span>, it calculates the </span
            ><span class="c22">probability distribution</span
            ><span>&nbsp;over possible sequences of </span
            ><span class="c22">tags</span
            ><span class="c0">&nbsp;and selects the best sequence.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">Markov chain</span
            ><span>&nbsp;is a model showing the </span
            ><span class="c22">probabilities</span><span>&nbsp;of </span
            ><span class="c22">transitions</span
            ><span>&nbsp;between random variables called</span
            ><span class="c22">&nbsp;states</span
            ><span class="c0"
                >. A Markov chain makes an assumption that any future sequence
                only depends on the current state.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">hidden Markov model</span
            ><span>&nbsp;makes two assumptions. The </span
            ><span class="c22">Markov assumption</span
            ><span>&nbsp;assumes that the probability of a future state</span
            ><span class="c1">&nbsp;q</span><span class="c7 c1">i</span
            ><span>&nbsp;depends only the previous state. The </span
            ><span class="c22">output independence</span
            ><span>&nbsp;assumes that an output observation </span
            ><span class="c1">o</span><span class="c7 c1">i </span
            ><span class="c0">depends only on its state.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 394.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image140.png"
                    style="
                        width: 394.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 428px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image37.png"
                    style="
                        width: 428px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 402.03px;
                    height: 169.59px;
                "
                ><img
                    alt="iform 
(b) 
A Markov chain for weather (a) and one for words (b), showing states and 
Figure 8.8 
transitions. A start distribution is required; setting = (0.1, 0.7, 0.2] for (a) would mean a 
probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state I (hot), etc. "
                    src="assets/natural-language-processing/image137.png"
                    style="
                        width: 402.03px;
                        height: 169.59px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Formally, a</span><span class="c11">&nbsp;Markov chain</span
            ><span class="c0"
                >&nbsp;is defined by the following components:</span
            >
        </p>
        <a id="t.e5fc79ce0d1f0bd6c68586f0214a60a3975c8bba"></a><a id="t.7"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c37" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 132px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image182.png"
                                    style="
                                        width: 132px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c53" colspan="1" rowspan="1">
                        <p class="c2">
                            <span>set of</span
                            ><span class="c3">&nbsp;states</span>
                        </p>
                    </td>
                </tr>
                <tr class="c70">
                    <td class="c37" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 144px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image154.png"
                                    style="
                                        width: 144px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c53" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c11"
                                >transition probability matrix</span
                            ><span class="c0">&nbsp;A</span>
                        </p>
                        <p class="c2">
                            <span>each value </span><span class="c1">ij </span
                            ><span
                                >represents the probability of moving from state </span
                            ><span class="c1">i </span><span>to state </span
                            ><span class="c8 c1">j</span>
                        </p>
                    </td>
                </tr>
                <tr class="c57">
                    <td class="c37" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 141.33px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image117.png"
                                    style="
                                        width: 141.33px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c53" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c11"
                                >initial probability distribution</span
                            ><span class="c0">&nbsp;over states</span>
                        </p>
                        <p class="c2">
                            <span
                                >&nbsp;is the probability the Markov chain will
                                start at state </span
                            ><span class="c1">i</span
                            ><span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 366.67px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image108.png"
                                    style="
                                        width: 366.67px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">hidden Markov model</span
            ><span>&nbsp;contains </span
            ><span class="c22">observed events </span
            ><span>(input tokens) and </span
            ><span class="c22">hidden events </span
            ><span class="c0"
                >(POS tags). A HMM is defined by the following components:</span
            >
        </p>
        <a id="t.fdb1c8d064eced6b853524e8ce87d55157f4430b"></a><a id="t.8"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c37" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 132px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image182.png"
                                    style="
                                        width: 132px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c4" colspan="1" rowspan="1">
                        <p class="c2">
                            <span>set of</span
                            ><span class="c3">&nbsp;states</span>
                        </p>
                    </td>
                </tr>
                <tr class="c70">
                    <td class="c37" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 144px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image163.png"
                                    style="
                                        width: 144px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c4" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c3"
                                >transition probability matrix</span
                            >
                        </p>
                        <p class="c2">
                            <span>each value </span><span class="c1">ij </span
                            ><span
                                >represents the probability of moving from state </span
                            ><span class="c1">i </span><span>to state </span
                            ><span class="c8 c1">j</span>
                        </p>
                    </td>
                </tr>
                <tr class="c70">
                    <td class="c37" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 130.67px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image60.png"
                                    style="
                                        width: 130.67px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c4" colspan="1" rowspan="1">
                        <p class="c2">
                            <span>set of </span
                            ><span class="c11">observations</span
                            ><span>&nbsp;from the vocabulary </span
                            ><span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 350.67px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image127.png"
                                    style="
                                        width: 350.67px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                </tr>
                <tr class="c82">
                    <td class="c37" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 122.67px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image61.png"
                                    style="
                                        width: 122.67px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c4" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c3">emission probability matrix</span>
                        </p>
                        <p class="c2">
                            <span
                                >expresses the probability of an observation </span
                            ><span class="c1">o</span
                            ><span class="c7 c1">t </span
                            ><span>being generated at state </span
                            ><span class="c1">q</span
                            ><span class="c46 c7 c1">i</span>
                        </p>
                    </td>
                </tr>
                <tr class="c57">
                    <td class="c37" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 141.33px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image117.png"
                                    style="
                                        width: 141.33px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c4" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c11"
                                >initial probability distribution</span
                            ><span class="c0">&nbsp;over states</span>
                        </p>
                        <p class="c2">
                            <span
                                >&nbsp;is the probability the Markov chain will
                                start at state </span
                            ><span class="c1">i</span
                            ><span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 366.67px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image108.png"
                                    style="
                                        width: 366.67px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">HMM Taggers</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">HMM</span
            ><span>&nbsp;uses two probabilities. </span
            ><span class="c11 c1">A </span><span>is the </span
            ><span class="c11">transition probability matrix</span
            ><span
                >&nbsp;representing the probability of a tag occurring given the
                previous tag. The model calculates the </span
            ><span class="c11">MLE</span
            ><span class="c0"
                >&nbsp;of probabilities by counting tag occurrences over
                previous tag occurrences.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 192px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image246.png"
                    style="
                        width: 192px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11 c1">B </span><span>is the </span
            ><span class="c11">emission probability matrix</span
            ><span
                >&nbsp;representing the probability of an observation being
                associated with a given tag. The model calculates the </span
            ><span class="c11">MLE</span
            ><span class="c0"
                >&nbsp;of probabilities by counting word-tag pair occurrences
                over all tag occurrences.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 174.67px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image181.png"
                    style="
                        width: 174.67px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Decoding</span
            ><span>&nbsp;is the process of determining values of </span
            ><span class="c22">hidden variables</span
            ><span>&nbsp;(tags). Given an</span
            ><span class="c22">&nbsp;input </span
            ><span class="c1">&#411;=(A,B)</span><span class="c1">&nbsp;</span
            ><span>and a set of </span><span class="c22">observations</span
            ><span>, find the most probable set of </span
            ><span class="c22">states</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >For POS tagging, the goal of decoding is to find the tag
                sequence </span
            ><span class="c1">t</span><span class="c7 c1">1</span
            ><span class="c1">&hellip;t</span><span class="c7 c1">n </span
            ><span>that is most likely given an observation sequence </span
            ><span class="c1">w</span><span class="c7 c1">1</span
            ><span class="c1">&hellip;w</span><span class="c7 c1">n</span
            ><span class="c8 c1">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 253.33px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image95.png"
                    style="
                        width: 253.33px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">Using Bayes rule:</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 312px;
                    height: 36px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image174.png"
                    style="
                        width: 312px;
                        height: 36px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0">Simplifying by dropping the denominator:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 310.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image43.png"
                    style="
                        width: 310.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A HMM tagger makes two assumptions. The first </span
            ><span class="c11">assumption</span
            ><span>&nbsp;states that the probability of a</span
            ><span class="c22">&nbsp;word</span
            ><span>&nbsp;occurring depends only on its </span
            ><span class="c22">tag</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 254.67px;
                    height: 45.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image220.png"
                    style="
                        width: 254.67px;
                        height: 45.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Second, the </span><span class="c11">bigram assumption</span
            ><span>&nbsp;states that the probability of a </span
            ><span class="c22">tag</span
            ><span>&nbsp;occuring is dependent only on the </span
            ><span class="c22">previous tag</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 214.67px;
                    height: 45.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image88.png"
                    style="
                        width: 214.67px;
                        height: 45.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >Using these assumptions, the following equation gives the </span
            ><span class="c11">most probable tag sequence </span
            ><span>for a</span><span class="c11">&nbsp;bigram tagger</span
            ><span
                >. The equation states that the probability of a tag set </span
            ><span class="c1">t</span><span class="c7 c1">1</span
            ><span class="c1">&hellip;t</span><span class="c7 c1">n </span
            ><span>given a word set </span><span class="c1">w</span
            ><span class="c7 c1">1</span><span class="c1">&hellip;w</span
            ><span class="c1 c7">n </span
            ><span>is defined by the product of </span
            ><span class="c22">emission</span><span>&nbsp;and</span
            ><span class="c22">&nbsp;transition</span
            ><span class="c0">&nbsp;values.</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 370.67px;
                    height: 56px;
                "
                ><img
                    alt="tl:n 
argmaxP(tl . 
..tnlW1. 
.. wn) &aelig; argmax 
emission transition 
P(WiIti) P(tilti-l) "
                    src="assets/natural-language-processing/image205.png"
                    style="
                        width: 370.67px;
                        height: 56px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Viterbi Algorithm</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">Viterbi</span
            ><span>&nbsp;algorithm is an efficient algorithm for </span
            ><span class="c22">HMM decoding</span><span>&nbsp;using </span
            ><span class="c22">dynamic programming</span
            ><span class="c0">.</span>
        </p>
        <ul class="c6 lst-kix_yoje8n2yi3ap-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>sets up a </span
                ><span class="c11">probability matrix</span
                ><span>&nbsp;with a </span><span class="c22">column</span
                ><span>&nbsp;for </span><span class="c22">observations</span
                ><span>&nbsp;and a </span><span class="c22">row</span
                ><span>&nbsp;for each </span><span class="c22 c28">state</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>each </span><span class="c11">cell</span
                ><span>&nbsp;of the matrix</span><span class="c1">&nbsp;</span
                ><span
                    >represents the probability that the HMM is in state </span
                ><span class="c1">j </span><span>after seeing the first </span
                ><span class="c1">t </span><span class="c0">observations</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the value of each</span><span class="c11">&nbsp;cell</span
                ><span class="c0"
                    >&nbsp;is calculated by taking the most probable path</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c11">cell probability </span
                ><span class="c1">v</span><span class="c1 c23">t</span
                ><span class="c1">(j) = max P(q</span
                ><span class="c1 c23">i</span><span class="c1">...q</span
                ><span class="c1 c23">t-1</span><span class="c1">, o</span
                ><span class="c1 c23">1</span><span class="c1">...o</span
                ><span class="c1 c23">t</span><span class="c1">, q</span
                ><span class="c1 c23">t</span
                ><span class="c1">&nbsp;= j | </span><span>&#x1d740;)</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c11">Viterbi probability</span
                ><span class="c0"
                    >&nbsp;is calculated by taking the most probable path
                    extension to the current cell</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>for state </span><span class="c1">q</span
                ><span class="c1 c23">j</span><span>&nbsp;at time</span
                ><span class="c1">&nbsp;t</span><span>, the value of </span
                ><span class="c1">v</span><span class="c1 c23">t</span
                ><span class="c1">(j) = max (i) a</span
                ><span class="c1 c23">ij </span><span class="c1">b</span
                ><span class="c1 c23">j </span><span class="c1">(o</span
                ><span class="c1 c23">t</span><span class="c1">)</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <a id="t.7ed426dac43459b77ca0159699c6aeab8b2c8ac7"></a><a id="t.9"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c73" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 66.67px;
                                    height: 17.33px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image114.png"
                                    style="
                                        width: 66.67px;
                                        height: 17.33px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c36" colspan="1" rowspan="1">
                        <p class="c2">
                            <span>previous</span
                            ><span class="c11">&nbsp;Viterbi</span
                            ><span class="c0">&nbsp;path probability</span>
                        </p>
                    </td>
                </tr>
                <tr class="c77">
                    <td class="c73" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 58.67px;
                                    height: 18.67px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image101.png"
                                    style="
                                        width: 58.67px;
                                        height: 18.67px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c36" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c11">transition probability</span
                            ><span>&nbsp;from state </span
                            ><span class="c1">q</span
                            ><span class="c7 c1">i </span><span>to state </span
                            ><span class="c1">q</span
                            ><span class="c46 c7 c1">j</span>
                        </p>
                    </td>
                </tr>
                <tr class="c77">
                    <td class="c73" colspan="1" rowspan="1">
                        <p class="c2">
                            <span
                                style="
                                    overflow: hidden;
                                    display: inline-block;
                                    margin: 0px 0px;
                                    border: 0px solid #000000;
                                    transform: rotate(0rad) translateZ(0px);
                                    -webkit-transform: rotate(0rad)
                                        translateZ(0px);
                                    width: 58.67px;
                                    height: 18.67px;
                                "
                                ><img
                                    alt=""
                                    src="assets/natural-language-processing/image24.png"
                                    style="
                                        width: 58.67px;
                                        height: 18.67px;
                                        margin-left: 0px;
                                        margin-top: 0px;
                                        transform: rotate(0rad) translateZ(0px);
                                        -webkit-transform: rotate(0rad)
                                            translateZ(0px);
                                    "
                                    title=""
                            /></span>
                        </p>
                    </td>
                    <td class="c36" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c11">observation probability</span
                            ><span>&nbsp;for </span><span class="c1">o</span
                            ><span class="c7 c1">t </span
                            ><span>given the state</span
                            ><span class="c8 c1">&nbsp;j</span>
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <hr />
        <p class="c2 c25"><span class="c0"></span></p>
        <p class="c2 c25"><span class="c0"></span></p>
        <p class="c2">
            <span class="c24 c22">function </span
            ><span class="c24">VITERBI (</span
            ><span class="c24 c1">observations</span
            ><span class="c24">&nbsp;of </span><span class="c24">len </span
            ><span class="c24 c1">T</span><span class="c24">, </span
            ><span class="c24 c1">state-graph </span
            ><span class="c24">of len </span><span class="c24 c1">N</span
            ><span class="c24">) </span><span class="c24 c22">returns </span
            ><span class="c13 c1">best-path, path-prob</span>
        </p>
        <p class="c2"><span class="c13 c29">&nbsp;</span></p>
        <p class="c2">
            <span class="c24">create a </span
            ><span class="c24 c11">path probability matrix </span
            ><span class="c24 c11 c1 c38">viterbi [N,T]</span>
        </p>
        <p class="c2"><span class="c13 c29">&nbsp;</span></p>
        <p class="c2">
            <span class="c24 c22">for </span
            ><span class="c24 c11 c1">state s </span
            ><span class="c24">from </span><span class="c24 c1">1 to N </span
            ><span class="c76 c24 c75 c29 c22">do</span>
        </p>
        <p class="c2 c10">
            <span class="c24 c1">viterbi [state,1] = </span
            ><span class="c24 c11 c1">start probability </span
            ><span class="c1">&pi;</span><span class="c7 c1">state </span
            ><span>&times; </span
            ><span class="c24 c11 c1">emission probability</span
            ><span class="c24 c1">&nbsp;b</span><span class="c35 c1 c23">s</span
            ><span class="c24 c1">(o</span><span class="c35 c1 c23">1</span
            ><span class="c13 c1">)</span>
        </p>
        <p class="c2">
            <span class="c24 c22">for </span
            ><span class="c24 c11 c1">time t </span
            ><span class="c24">from </span><span class="c24 c1">2 to T </span
            ><span class="c76 c24 c75 c29 c22">do</span>
        </p>
        <p class="c2 c10">
            <span class="c24 c22">for </span
            ><span class="c24 c11 c1">state s </span
            ><span class="c24">from </span><span class="c24 c1">1 to N </span
            ><span class="c24 c75 c29 c22 c76">do</span>
        </p>
        <p class="c2 c55">
            <span class="c24 c1">viterbi [state, time] = </span
            ><span class="c24">max</span
            ><span class="c24 c1"
                >&nbsp;viterbi [s&#39;, t-1] &nbsp;&times; </span
            ><span class="c24 c11 c1">transition probability</span
            ><span class="c24 c1">&nbsp;a</span
            ><span class="c1 c23 c35">s&#39;-&gt;s </span
            ><span class="c24 c1">&times; </span
            ><span class="c24 c11 c1">emission probability</span
            ><span class="c24 c1">&nbsp;b</span><span class="c35 c1 c23">s</span
            ><span class="c24 c1">(o</span><span class="c35 c1 c23">t</span
            ><span class="c13 c1">)</span>
        </p>
        <p class="c2"><span class="c13 c29">&nbsp;</span></p>
        <p class="c2">
            <span class="c24 c11 c1">best path probability</span
            ><span class="c24 c1">&nbsp;= </span><span class="c24">max </span
            ><span class="c13 c1">viterbi [s,T]</span>
        </p>
        <p class="c2">
            <span class="c24 c11 c1">best path pointer</span
            ><span class="c24">&nbsp;= argmax </span
            ><span class="c13 c1">viterbi [s,T]</span>
        </p>
        <p class="c2">
            <span class="c24 c11 c1">best path</span
            ><span class="c24 c1">&nbsp;= </span
            ><span class="c24">path at state </span
            ><span class="c1 c24">bestpointer</span
            ><span class="c13 c29"
                >, following pointers to states over time</span
            >
        </p>
        <p class="c2"><span class="c13 c29">&nbsp;</span></p>
        <p class="c2">
            <span class="c24 c22">return </span
            ><span class="c13 c1">best path, best path probability</span>
        </p>
        <p class="c2 c25"><span class="c13 c1"></span></p>
        <hr />
        <p class="c2 c25"><span class="c13 c1"></span></p>
        <p class="c2 c25"><span class="c13 c1"></span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 624px;
                    height: 322.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image133.png"
                    style="
                        width: 624px;
                        height: 322.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c25"><span class="c0"></span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c20">Named Entity Recognition</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">NER Tagging</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Named Entity:</span
            ><span class="c0"
                >&nbsp;word or phrase referred to with a proper name such as a
                person, location, or organisation</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 462.67px;
                    height: 66.67px;
                "
                ><img
                    alt="People 
Organization 
Location 
Geo-Political Entity 
Tag 
PER 
ORG 
LOC 
GPE 
Sample Categories 
people, characters 
companies, sports teams 
regions, mountains, seas 
countries, states 
Example sentences 
&#39;lilring is a giant of computer science. 
The IPCC warned about the cyclone. 
Mt. Sanitas is in Sunshine Canyon. 
Palo Alto is raising the fees for parking. "
                    src="assets/natural-language-processing/image179.png"
                    style="
                        width: 462.67px;
                        height: 66.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Named entity recognition (NER)</span
            ><span
                >&nbsp;aims to find spans of text that constitute proper names
                and tag them accordingly.</span
            ><span class="c11">&nbsp;BIO tagging</span
            ><span class="c0"
                >&nbsp;is a method that allows us to treat NER as a sequence
                labelling task by capturing the boundaries of the sequence. I
                and O represent words inside and outside the entity, B and E
                represent the beginning and end, and S represents a single
                entity.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <a id="t.1faa1727f395a1b962fbb88e6350397cab0db829"></a><a id="t.10"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c54" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Variant</span></p>
                    </td>
                    <td class="c54" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Tags</span></p>
                    </td>
                    <td class="c41" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Meaning</span></p>
                    </td>
                    <td class="c72" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Example</span></p>
                    </td>
                </tr>
                <tr class="c70">
                    <td class="c54" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">IO</span></p>
                    </td>
                    <td class="c54" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">I</span></p>
                        <p class="c2"><span class="c0">O</span></p>
                    </td>
                    <td class="c41" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Inside</span></p>
                        <p class="c2"><span class="c0">Outside</span></p>
                    </td>
                    <td class="c72" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >Jane of United Airlines Holding discussed the
                                Chicago route</span
                            >
                        </p>
                        <p class="c2">
                            <span class="c0"
                                >I-PER O I-ORG &nbsp; I-ORG &nbsp; &nbsp; I-ORG
                                &nbsp; &nbsp;
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;O
                                &nbsp; &nbsp;
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;O
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I-LOC
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;O</span
                            >
                        </p>
                    </td>
                </tr>
                <tr class="c57">
                    <td class="c54" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">BIO</span></p>
                    </td>
                    <td class="c54" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">B</span></p>
                        <p class="c2"><span class="c0">I</span></p>
                        <p class="c2"><span class="c0">O</span></p>
                    </td>
                    <td class="c41" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Begin</span></p>
                        <p class="c2"><span class="c0">Inside</span></p>
                        <p class="c2"><span class="c0">Outside</span></p>
                    </td>
                    <td class="c72" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >Jane of United Airlines Holding discussed the
                                Chicago route</span
                            >
                        </p>
                        <p class="c2">
                            <span class="c0"
                                >B-PER O B-ORG &nbsp;I-ORG &nbsp;I-ORG &nbsp;
                                &nbsp;
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;O
                                &nbsp; &nbsp;
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;O
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B-LOC
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;O</span
                            >
                        </p>
                    </td>
                </tr>
                <tr class="c61">
                    <td class="c54" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">BIOES</span></p>
                    </td>
                    <td class="c54" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">B</span></p>
                        <p class="c2"><span class="c0">I</span></p>
                        <p class="c2"><span class="c0">O</span></p>
                        <p class="c2"><span class="c0">E</span></p>
                        <p class="c2"><span class="c0">S</span></p>
                    </td>
                    <td class="c41" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Begin</span></p>
                        <p class="c2"><span class="c0">Inside</span></p>
                        <p class="c2"><span class="c0">Outside</span></p>
                        <p class="c2"><span class="c0">End</span></p>
                        <p class="c2"><span class="c0">Single</span></p>
                    </td>
                    <td class="c72" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >Jane of United Airlines Holding discussed the
                                Chicago route</span
                            >
                        </p>
                        <p class="c2">
                            <span class="c0"
                                >S-PER O B-ORG &nbsp;I-ORG &nbsp;E-ORG &nbsp;
                                &nbsp;
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;O
                                &nbsp; &nbsp;
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;O
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;S-LOC
                                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;O</span
                            >
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Conditional Random Fields</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Conditional Random Field (CRF):</span
            ><span class="c0"
                >&nbsp;statistical modelling technique used to predict the most
                probable sequence of NER tags for a given sequence of
                words</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 136px;
                    height: 34.67px;
                "
                ><img
                    alt="argmax &#12289; (YIX &#19968; 
Yew "
                    src="assets/natural-language-processing/image87.png"
                    style="
                        width: 136px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Linear Chain CRF:</span
            ><span
                >&nbsp;log-linear model that assigns a probability to an output
                tag sequence </span
            ><span class="c1">Y</span
            ><span>, out of all possible output sequences </span
            ><span>&#x1d4b4;</span><span>, given an input word sequence </span
            ><span class="c1">X</span><span>. Assuming a </span
            ><span class="c11">word sequence </span
            ><span class="c11 c1">X = x1,&hellip;,x</span
            ><span class="c11 c1 c22">n</span
            ><span>, linear chain CRF produces a </span
            ><span class="c11">tag sequence </span
            ><span class="c11 c1">Y = y1,&hellip;,yn </span
            ><span>by maximising the probability </span
            ><span class="c1">P(Y|X)</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 564px;
                    height: 116px;
                "
                ><img
                    alt="exp 
p(YlX) 
EWkFk(X, Y&#39;) 
&mdash; &mdash;exp 
E exp "
                    src="assets/natural-language-processing/image170.png"
                    style="
                        width: 564px;
                        height: 116px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c1 c22">K </span
            ><span class="c22">functions </span><span class="c1 c22">F</span
            ><span class="c7 c1 c22">K</span><span class="c1 c22">(X,Y) </span
            ><span>are the </span><span class="c11">global features</span
            ><span>&nbsp;for the sequences </span><span class="c1">X</span
            ><span>&nbsp;and </span><span class="c1">Y</span
            ><span>, calculated by taking the</span
            ><span class="c22">&nbsp;sum of </span
            ><span class="c11">local features</span
            ><span>&nbsp;at each position </span><span class="c1">i </span
            ><span>in </span><span class="c1">Y</span
            ><span>. Each local feature </span><span class="c1">f</span
            ><span class="c7 c1">k </span><span>uses the current token </span
            ><span class="c1">y</span><span class="c7 c1">i</span
            ><span class="c1">, </span><span>previous token </span
            ><span class="c1">y</span><span class="c7 c1">i-1</span
            ><span>, input sequence </span><span class="c1">X</span
            ><span>, and position </span><span class="c1">i</span
            ><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 197.33px;
                    height: 50.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image49.png"
                    style="
                        width: 197.33px;
                        height: 50.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Features</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Feature templates </span><span>populate </span
            ><span class="c22">features</span
            ><span>&nbsp;with specific values. The template &#10094;y</span
            ><span class="c23">i</span><span>,x</span><span class="c23">i</span
            ><span>&#10095;, &#10094;y</span><span class="c23">i</span
            ><span>,y</span><span class="c23">i-1</span
            ><span>&#10095;, &#10094;y</span><span class="c23">i</span
            ><span>,x</span><span class="c23">i-1</span><span>,x</span
            ><span class="c23">i+2</span
            ><span>&#10095; uses &nbsp;information from </span
            ><span class="c1">y</span><span class="c7 c1">i-1</span
            ><span class="c1">, y</span><span class="c7 c1">i</span
            ><span class="c1">, X, i</span><span>. </span
            ><span class="c11">Word shape</span
            ><span>&nbsp;features represent the </span
            ><span class="c22">abstract letter pattern</span
            ><span>&nbsp;of words by mapping lowercase letters to </span
            ><span class="c1">x,</span><span>&nbsp;uppercase letters to </span
            ><span class="c1">X, </span><span>and numbers to </span
            ><span class="c1">d</span
            ><span class="c0">. For example, DC2021 -&gt; XXdddd.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">gazetteer</span
            ><span
                >&nbsp;is a list of place names, often providing millions of
                entries for locations with detailed geographical and political
                information. </span
            ><span class="c11">Name lists</span
            ><span class="c0"
                >&nbsp;containing organisations and products can also be
                used.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26"
                >Typical features for a feature-based NER tagger</span
            >
        </p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 401.33px;
                    height: 128px;
                "
                ><img
                    alt="identity of Wi, identity of neighboring words 
embeddings for Wi, embeddings for neighboring words 
part of speech of Wi, part of speech of neighboring words 
presence of Wi in a gazetteer 
Wi contains a particular prefix (from all prefixes of length 4) 
Wi contains a particular suffix (from all suffixes of length 4) 
word shape of Wi, word shape of neighboring words 
short word shape of Wi, short word shape of neighboring words 
gazetteer features "
                    src="assets/natural-language-processing/image34.png"
                    style="
                        width: 401.33px;
                        height: 128px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Inference and Training</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">best tag sequence </span
            ><img src="assets/natural-language-processing/image1.png" /><span>for a given input </span
            ><span class="c1">X</span><span class="c0">&nbsp;is given by:</span>
        </p>
        <ul class="c6 lst-kix_rlxo1p6n2iq3-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>ignoring the </span><span class="c11 c1">exp </span
                ><span>function and denominator </span
                ><span class="c11 c1">Z(X) </span><span>as these are </span
                ><span class="c11">constant</span
                ><span class="c0">&nbsp;and do not affect comparisons</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>expanding </span><span class="c11 c1">F</span
                ><span class="c7 c11 c1">K</span
                ><span class="c11 c1">(X,Y) </span><span>to </span
                ><img src="assets/natural-language-processing/image2.png" /><span class="c31 c29"
                    >&nbsp;</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0">rearranging the expression</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 278.67px;
                    height: 238.67px;
                "
                ><img
                    alt="argmaxP(YlX) 
argmax 
exp 
argmax exp 
argmax VVk (M&mdash; I X , &#304;) 
argmax "
                    src="assets/natural-language-processing/image46.png"
                    style="
                        width: 278.67px;
                        height: 238.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>To decode the optimal tag sequence we use the</span
            ><span class="c11">&nbsp;Viterbi algorithm</span
            ><span
                >, which works because a linear chain CRF depends only on the
                previous token at each time step. The algorithm fills in an </span
            ><span class="c1 c22">N </span><span class="c22">&times; </span
            ><span class="c1 c22">T </span><span class="c22">array</span
            ><span>&nbsp;with appropriate values while maintaining </span
            ><span class="c22">backpointers</span
            ><span>, then follows the pointers to retrieve the </span
            ><span class="c22">optimal tag sequence</span
            ><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c20">Lexical and Vector Semantics</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Lexical Semantics</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Lemma:</span
            ><span>&nbsp;root of a word e.g. </span
            ><span class="c8 c1">mouse</span>
        </p>
        <p class="c2">
            <span class="c11">Word Form:</span
            ><span>&nbsp;variant of a lemma e.g. </span
            ><span class="c8 c1">mice</span>
        </p>
        <p class="c2">
            <span class="c11">Word Sense:</span
            ><span>&nbsp;meaning of a lemma given its context, e.g. </span
            ><span class="c1">mouse </span><span>as an animal or </span
            ><span class="c1">mouse </span><span class="c0">as a device</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Word Similarity:</span
            ><span class="c0"
                >&nbsp;measure of similarity between the meanings of two words.
                Can be calculated by human judgement or by word
                co-occurrence</span
            >
        </p>
        <p class="c2">
            <span class="c11">Word Relatedness:</span
            ><span>&nbsp;measure of association between words e.g. </span
            ><span class="c1">cup </span><span>and </span
            ><span class="c8 c1">coffee</span>
        </p>
        <p class="c2">
            <span class="c11">Sentiment:</span
            ><span
                >&nbsp;positive or negative emotions suggested by words e.g. </span
            ><span class="c8 c1">happy, sad</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Semantic Field:</span
            ><span
                >&nbsp;set of words which cover a semantic domain and share
                structured relations. For example, words in the semantic field
                of </span
            ><span class="c1">foods </span><span>include </span
            ><span class="c8 c1">bread, butter, cake, donuts</span>
        </p>
        <p class="c2">
            <span class="c11">Semantic Frame:</span
            ><span
                >&nbsp;set of words which denote perspectives or participants in
                an event. For example, the semantic frame for a </span
            ><span class="c1">commercial transaction</span
            ><span>&nbsp;includes </span
            ><span class="c8 c1">buy, sell, pay, money</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Vector Semantics</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Vector semantics</span
            ><span>&nbsp;aim to represent a given word as a </span
            ><span class="c22">point in a multidimensional space</span
            ><span>&nbsp;using the </span
            ><span class="c22">distributions of neighbouring words</span
            ><span>. Vectors for representing words are called</span
            ><span class="c11">&nbsp;embeddings</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 362.67px;
                    height: 130.67px;
                "
                ><img
                    alt="to by 
that now 
than 
with 
&#39;s 
is 
not good 
bad 
dislike 
incredibly bad 
very good incredibly good 
fantastic 
wonderful 
good "
                    src="assets/natural-language-processing/image29.png"
                    style="
                        width: 362.67px;
                        height: 130.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>In a </span><span class="c11">term-document matrix</span
            ><span
                >, each row represents a word and each column represents a
                document. The matrix has </span
            ><span class="c1">|V| </span><span>rows and </span
            ><span class="c1">D </span><span>columns. </span
            ><span class="c11">Information retrieval (IR)</span
            ><span>&nbsp;is the task of finding a document </span
            ><span class="c1">d </span><span>from </span
            ><span class="c1">D </span
            ><span>documents according to some query </span
            ><span class="c1">q</span><span>&nbsp;represented by a vector </span
            ><span class="c1">|V|</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 404px;
                    height: 66.67px;
                "
                ><img
                    alt="battle 
good 
fool 
wit 
As You Like It 
6 
0 
Twelfth Night 
80 
58 
15 
Julius Caesar 
62 
2 
Henry V 
89 
4 
3 "
                    src="assets/natural-language-processing/image241.png"
                    style="
                        width: 404px;
                        height: 66.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">term-term matrix</span
            ><span>&nbsp;is a </span><span class="c1">|V| &times; |V|</span
            ><span class="c0"
                >&nbsp;matrix encoding the occurences of a target word against
                several contexts. Each row represents a word and each column
                represents a possible context.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 405.33px;
                    height: 70.67px;
                "
                ><img
                    alt="cherry 
strawberry 
digital 
information 
aardvark 
o 
o 
o 
computer 
2 
1670 
3325 
data 
8 
1683 
3982 
result 
9 
85 
378 
pie 
442 
60 
5 
5 
sugar 
25 
19 
4 
13 "
                    src="assets/natural-language-processing/image193.png"
                    style="
                        width: 405.33px;
                        height: 70.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>To measure the </span><span class="c11">similarity</span
            ><span>&nbsp;between two words </span><span class="c1">v </span
            ><span>and </span><span class="c1">w</span
            ><span
                >, we can use cosine similarity to produce a value between 0 and
                1. The dot product </span
            ><span class="c11 c1">v&middot;w </span><span>is </span
            ><span class="c11">normalised</span
            ><span class="c0">&nbsp;by the magnitude of each vector.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 406.67px;
                    height: 53.33px;
                "
                ><img
                    alt="dot product(v, w) 
ViVVi 
+V2W2+. 
+ VNWN "
                    src="assets/natural-language-processing/image97.png"
                    style="
                        width: 406.67px;
                        height: 53.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 276px;
                    height: 118.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image123.png"
                    style="
                        width: 276px;
                        height: 118.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11"
                >TF-IDF (Term Frequency-Inverse Document Frequency)</span
            ><span class="c0"
                >&nbsp;evaluates the relevance of a term to a document.</span
            >
        </p>
        <ul class="c6 lst-kix_3e9gkmwdsm7e-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Term Frequency (TF)</span
                ><span class="c0"
                    >&nbsp;- frequency of a term t in a document d, typically
                    represented logarithmically</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Document Frequency (DF) -</span
                ><span class="c0"
                    >&nbsp;number of documents a term occurs in</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Inverse Document Frequency (IDF) -</span
                ><span class="c0"
                    >&nbsp;number of documents divided by document
                    frequency</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">TF-IDF -</span
                ><span class="c0"
                    >&nbsp;calculated by multiplying TF and IDF.</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 198.67px;
                    height: 28px;
                "
                ><img
                    alt="tfr,d "
                    src="assets/natural-language-processing/image109.png"
                    style="
                        width: 198.67px;
                        height: 28px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 138.67px;
                    height: 52px;
                "
                ><img
                    alt="dit 
idft "
                    src="assets/natural-language-processing/image125.png"
                    style="
                        width: 138.67px;
                        height: 52px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 121.33px;
                    height: 25.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image120.png"
                    style="
                        width: 121.33px;
                        height: 25.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Pointwise Mutual Information (PMI)</span
            ><span
                >&nbsp;measures the association between two words by counting
                their co-occurrences. Given two words </span
            ><span class="c1">w </span><span>and </span><span class="c1">c</span
            ><span
                >, we measure how often they occur together compared to how
                often they occur independently. </span
            ><span class="c11"
                >Positive Pointwise Mutual Information (PPMI)</span
            ><span class="c0">&nbsp;replaces negative values with 0.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 190.67px;
                    height: 50.67px;
                "
                ><img
                    alt="PMl(w, &#1089;) "
                    src="assets/natural-language-processing/image81.png"
                    style="
                        width: 190.67px;
                        height: 50.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 257.33px;
                    height: 45.33px;
                "
                ><img
                    alt="= max(Iog2 "
                    src="assets/natural-language-processing/image102.png"
                    style="
                        width: 257.33px;
                        height: 45.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c50">Word2Vec</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Embeddings and Skip-Grams</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A word </span><span class="c11">embedding</span
            ><span class="c0"
                >&nbsp;is a learned representation for text where words that
                have the same meaning have a similar representation</span
            >
        </p>
        <ul class="c6 lst-kix_n3o0apw06lgf-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c22">dense</span
                ><span>&nbsp;- number of dimensions </span
                ><span class="c1">d </span
                ><span class="c0">ranging from 50-1000</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c22">static</span
                ><span class="c0"
                    >&nbsp;- single embedding per word that does not
                    change</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c22">binary</span
                ><span class="c0"
                    >&nbsp;- uses logistic regression to predict whether a word
                    A is associated with word B</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Skip-grams</span
            ><span
                >&nbsp;are a generalisation of n-grams in which terms do not
                need to be consecutive, but can be separated by a number of
                terms. Formally, a </span
            ><span class="c11 c1">k</span><span class="c11">-skip-</span
            ><span class="c11 c1">n</span><span class="c11">-gram</span
            ><span>&nbsp;is a subsequence of </span
            ><span class="c22">length </span><span class="c1 c22">n </span
            ><span>where terms occur at most </span
            ><span class="c1 c22">k </span><span class="c22">terms away</span
            ><span class="c0">&nbsp;from each other.</span>
        </p>
        <ul class="c6 lst-kix_sqpnixlhcr2e-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >target words and context words are taken as positive data
                    points</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >random sampling generates negative data points</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >logistic regression is used to train the classifier</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0">the model learns weights for embeddings</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Word2Vec</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Word2Vec:</span
            ><span class="c0"
                >&nbsp;neural network model that learns word associations from a
                given text corpus. After training, it can detect synonymous
                words using vector embeddings</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Given an input of a </span
            ><span class="c11">target word </span><span class="c11 c1">w </span
            ><span>and a </span><span class="c11">window size </span
            ><span class="c11 c1">L </span><span>to find </span
            ><span class="c11">context words </span><span class="c11 c1">c</span
            ><span>, the classifier will return the probability that </span
            ><span class="c1">c </span><span>is a context word for </span
            ><span class="c1">w</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c8 c1"
                >&hellip;lemon a [tablespoon of apricot jam, a]
                pinch&hellip;</span
            >
        </p>
        <p class="c2 c10">
            <span
                >&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span
            ><span class="c8 c1"
                >c1 &nbsp; &nbsp; &nbsp; &nbsp; c2
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;c3 &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;w &nbsp;
                &nbsp;c4</span
            >
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">P(+|w,c)</span><span>&nbsp;= probability </span
            ><span class="c1">c</span><span>&nbsp;is a context word for </span
            ><span class="c8 c1">w</span>
        </p>
        <p class="c2">
            <span class="c11">P(-|w,c)</span><span>&nbsp;= probability </span
            ><span class="c1">c </span><span>is not a context word for </span
            ><span class="c8 c1">w</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">similarity</span
            ><span>&nbsp;between words is calculated using </span
            ><span class="c11">embeddings</span
            ><span>&nbsp;normalised by the </span
            ><span class="c11">logistic function</span
            ><span class="c0">&nbsp;to return a value between 0 and 1:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c8 c1">Similarity(w,c) &asymp; c &middot; w</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 180px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image138.png"
                    style="
                        width: 180px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 290.67px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image223.png"
                    style="
                        width: 290.67px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 386.67px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image233.png"
                    style="
                        width: 386.67px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Assuming that all context words are </span
            ><span class="c11">independent</span><span>, the </span
            ><span class="c11">probability for all context words </span
            ><span class="c1">c </span><span>in the window </span
            ><span class="c1">L </span
            ><span class="c0">can be calculated by taking the product:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 221.33px;
                    height: 46.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image85.png"
                    style="
                        width: 221.33px;
                        height: 46.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 257.33px;
                    height: 46.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image225.png"
                    style="
                        width: 257.33px;
                        height: 46.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>In summary, Word2Vec trains a</span
            ><span class="c11">&nbsp;probabilistic classifier</span
            ><span>&nbsp;that takes a </span
            ><span class="c11">target word </span><span class="c11 c1">w</span
            ><span>&nbsp;and its </span
            ><span class="c11">context window of </span
            ><span class="c11 c1">L </span><span class="c11">words </span
            ><span class="c11 c1">c</span><span class="c7 c11 c1">1:L</span
            ><span>&nbsp;and assigns a probability based on the</span
            ><span class="c11">&nbsp;similarity</span
            ><span
                >&nbsp;between the context window and target word. This is based
                on the </span
            ><span class="c11">logistic function</span><span>&nbsp;and the</span
            ><span class="c11">&nbsp;dot product</span
            ><span>&nbsp;of word embeddings. Word2Vec keeps embeddings for</span
            ><span class="c22">&nbsp;every target word </span
            ><span class="c1 c22">w</span><span>&nbsp;and </span
            ><span class="c22">every context word </span
            ><span class="c1 c22">c</span><span class="c0">.</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 381.33px;
                    height: 184px;
                "
                ><img
                    alt="l..d 
aardvark 
apricot 
zebra 
aardvark 
apricot 
zebra 
W 
IVI 
IVI+I 
C 
target words 
context &amp; noise 
words "
                    src="assets/natural-language-processing/image215.png"
                    style="
                        width: 381.33px;
                        height: 184px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Learning Skip-Gram Embeddings</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Skip-gram embeddings</span
            ><span>&nbsp;are learned by starting with </span
            ><span class="c11">random vectors</span><span>&nbsp;and </span
            ><span class="c11">iteratively shifting</span
            ><span>&nbsp;the values for each word </span
            ><span class="c1">w </span
            ><span>to become more like the embeddings of associated words.</span
            ><span class="c11">&nbsp;Positive</span
            ><span>&nbsp;examples are </span
            ><span class="c22"
                >context words associated with the target word</span
            ><span>. </span><span class="c11">Negative</span
            ><span>&nbsp;examples are noise words found using </span
            ><span class="c22">random sampling</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c8 c1"
                >&hellip;lemon a [tablespoon of apricot jam, a]
                pinch&hellip;</span
            >
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 337.33px;
                    height: 88px;
                "
                ><img
                    alt="positive examples + 
negative examples - 
apricot 
apricot 
apricot 
apricot 
tablespoon 
of 
jam 
a 
apricot 
apricot 
apricot 
apricot 
Cneg 
aardvark 
my 
where 
coaxial 
apricot 
apricot 
apricot 
apricot 
seven 
forever 
dear 
if "
                    src="assets/natural-language-processing/image150.png"
                    style="
                        width: 337.33px;
                        height: 88px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Random words are chosen according to their </span
            ><span class="c11">weighted unigram frequency </span
            ><span class="c11 c1">P</span><span class="c7 c11 c1">&alpha;</span
            ><span class="c11 c1">(w) </span><span>where </span
            ><span class="c1">&alpha;</span
            ><span>&nbsp;is a weight. This is set to </span
            ><span class="c1">&alpha;=0.75</span
            ><span>&nbsp;by default as </span><span class="c1">P</span
            ><span class="c7 c1">&alpha;</span
            ><span class="c1">(w) &gt; P(w) </span
            ><span class="c0">for rare words:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 205.33px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image219.png"
                    style="
                        width: 205.33px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0"
                >Given the set of positive and negative training instances, the
                goal of the algorithm is to adjust these embeddings to:</span
            >
        </p>
        <ul class="c6 lst-kix_3k803ic1wa1-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c11">maximise</span
                ><span>&nbsp;the similarity of the </span
                ><span class="c22">target word</span><span>&nbsp;and </span
                ><span class="c22">context word</span><span>&nbsp;pairs</span
                ><span class="c1">&nbsp;(w,c+) </span><span>from </span
                ><span class="c11">positive</span
                ><span class="c0">&nbsp;samples</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">minimise</span
                ><span>&nbsp;the similarity of </span
                ><span class="c22">noise word</span><span>&nbsp;and </span
                ><span class="c22">context word</span><span>&nbsp;pairs </span
                ><span class="c1">(w,c-) </span><span>from</span
                ><span class="c11">&nbsp;negative</span
                ><span class="c0">&nbsp;samples</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Given a </span
            ><span class="c11">positive word-context pair </span
            ><span class="c11 c1">(w,c+) </span><span>and </span
            ><span class="c11 c1">k </span><span class="c11">noise words </span
            ><span class="c11 c1">c-</span><span class="c7 c11 c1">1</span
            ><span class="c11 c1">&hellip;c-</span
            ><span class="c7 c11 c1">k</span
            ><span>, these goals can be expressed as the </span
            ><span class="c11">loss function </span
            ><span class="c11 c1">L </span><span>to be </span
            ><span class="c11">minimised</span
            ><span>. The first term expresses that the </span
            ><span class="c22">positive context word </span
            ><span class="c1 c22">c+ </span><span>should have a </span
            ><span class="c22">high probability of being a neighbour</span
            ><span>, while the second term expresses that the </span
            ><span class="c22">negative noise words </span
            ><span class="c1 c22">c-</span><span class="c7 c1 c22">&nbsp;</span
            ><span>should have a </span
            ><span class="c22">high probability of being a non-neighbou</span
            ><span class="c0">r.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 385.33px;
                    height: 230.67px;
                "
                ><img
                    alt="LCE &mdash; 
- -log 
logP(+ cpos) -F logP(&mdash; Ivv, cnegi) "
                    src="assets/natural-language-processing/image218.png"
                    style="
                        width: 385.33px;
                        height: 230.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Visualising Embeddings</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_28zm0yxafs26-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Hierarchical Clustering</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_28zm0yxafs26-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >shows a hierarchical representation of the similarity of
                    words in an embedding space</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>projects </span><span class="c1">n </span
                ><span class="c0">dimensions down to 2 dimensions</span>
            </li>
        </ul>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 198.67px;
                    height: 254.67px;
                "
                ><img
                    alt="WRIST 
ANKLE 
SHOULDER 
NOSE 
FINGER 
FACE 
KITTEN 
MOUSE 
OYSTER 
BULL 
CHCAGO 
ATLANTA 
MONTREAL 
NASHVILLE 
CHINA 
RUSSIA 
AFRICA 
EUROPE 
AMERICA 
BRAZIL 
HAWAII "
                    src="assets/natural-language-processing/image162.png"
                    style="
                        width: 198.67px;
                        height: 254.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_elfkn47uu424-0 start">
            <li class="c2 c18 li-bullet-0"><span class="c3">Analogy</span></li>
        </ul>
        <ul class="c6 lst-kix_elfkn47uu424-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0">shows the relational similarity of words</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0">displayed using the parallelogram model</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>uses the analogy </span
                ><span class="c8 c1">a is to b as c is to ?</span>
            </li>
        </ul>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 356px;
                    height: 125.33px;
                "
                ><img
                    alt="WOMAN 
&Mu;&Alpha;&Nu; 
lJNCLE 
AlJNT 
QlJEENS 
KlNGS 
KlNG 
QlJEEN 
QlJEEN 
KlNG "
                    src="assets/natural-language-processing/image156.png"
                    style="
                        width: 356px;
                        height: 125.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Bias in Embeddings</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0"
                >Embeddings encode the bias within training sets used to compute
                them. A historical corpus is likely to encode the bias of its
                time.</span
            >
        </p>
        <ul class="c6 lst-kix_nmiwj1taxwqd-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Bias Amplification</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_nmiwj1taxwqd-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >bias where embeddings exaggerate patterns and make them
                    more extreme</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >implicit bias includes racism, ageism, sexism etc.</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_nmiwj1taxwqd-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Allocation Harm</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_nmiwj1taxwqd-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >bias resulting in unfair real world outcomes</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_nmiwj1taxwqd-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Representational Harm</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_nmiwj1taxwqd-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >harm caused by a system demeaning some social or racial
                    groups</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Debiasing</span
            ><span class="c0"
                >&nbsp;is the process of manipulating embeddings to remove
                harmful bias.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c20">Recurrent Neural Networks</span></p>
        <p class="c80 c83"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Neural Units</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A neural </span><span class="c11">unit</span
            ><span>&nbsp;takes a </span><span class="c22">set of </span
            ><span class="c11">real numbers</span><span>&nbsp;and a </span
            ><span class="c11">bias term</span
            ><span>&nbsp;as input, computes the </span
            ><span class="c11">weighted sum</span
            ><span
                >&nbsp;of inputs, and outputs the result. Given a set of inputs </span
            ><span class="c1">x</span><span class="c7 c1">1</span
            ><span class="c1">&hellip;x</span><span class="c7 c1">n</span
            ><span>, a unit has a set of weights </span><span class="c1">w</span
            ><span class="c7 c1">1</span><span class="c1">&hellip;w</span
            ><span class="c7 c1">n </span><span>and a bias term </span
            ><span class="c1">b</span
            ><span>, which produces a weighted sum </span
            ><span class="c1">z</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 157.33px;
                    height: 40px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image66.png"
                    style="
                        width: 157.33px;
                        height: 40px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">Using vector notation:</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 138.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image82.png"
                    style="
                        width: 138.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Instead of using the linear function </span
            ><span class="c1">z</span><span>, neural units can apply a </span
            ><span class="c11">non-linear function </span
            ><span class="c1">f</span><span>&nbsp;to </span
            ><span class="c1">z</span
            ><span>. The output of this function is the </span
            ><span class="c11">activation</span><span>&nbsp;value </span
            ><span class="c1">a</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 138.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image197.png"
                    style="
                        width: 138.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Using the</span><span class="c11">&nbsp;sigmoid function</span
            ><span class="c0">, the output of a neural unit is:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 218.67px;
                    height: 32px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image45.png"
                    style="
                        width: 218.67px;
                        height: 32px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 302.67px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image67.png"
                    style="
                        width: 302.67px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Schematic of a basic neural unit:</span>
        </p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 206.67px;
                    height: 165.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image54.png"
                    style="
                        width: 206.67px;
                        height: 165.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c38 c29 c43">&nbsp;</span></p>
        <p class="c2"><span class="c26">Neural Networks</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>An </span
            ><span class="c11">artificial neural network (ANN)</span
            ><span class="c0"
                >&nbsp;is a computing system based on biological neural
                networks.</span
            >
        </p>
        <ul class="c6 lst-kix_vb62q6rx7k27-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>consists of many </span><span class="c22">neurons</span
                ><span>&nbsp;connected together by</span
                ><span class="c28 c22">&nbsp;edges</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>neurons use </span
                ><span class="c22">mathematical functions</span
                ><span>&nbsp;and </span><span class="c22">weights</span
                ><span class="c0">&nbsp;to compute their outputs</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c22">thresholds</span><span>,</span
                ><span class="c22">&nbsp;aggregate signals</span
                ><span>, and</span><span class="c22">&nbsp;layers</span
                ><span class="c0"
                    >&nbsp;can be added to artificial neural networks</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">multi-layer perceptron (MLP)</span
            ><span
                >&nbsp;consists of an input layer, hidden layers, and an output
                layer. Each layer is fully connected to the next layer. The
                input layer is composed of input neurons, while the hidden and
                output layers are composed of TLUs. Bias neurons are also
                included in the network. When an ANN contains many hidden layers
                it is called a </span
            ><span class="c11">deep neural network (DNN)</span
            ><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Feed-Forward Neural Networks</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A</span><span class="c11">&nbsp;feed-forward network</span
            ><span
                >&nbsp;is a multilayer neural network where connections are
                acyclic (no cycles). The outputs from units in each layer are
                passed to units in the next layer. No outputs are passed back to
                previous layers. In a </span
            ><span class="c11">fully connected feedforward network</span
            ><span class="c0"
                >, information only travels forwards and each layer is fully
                connected to the next.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 405.33px;
                    height: 185.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image245.png"
                    style="
                        width: 405.33px;
                        height: 185.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c46 c29 c71">&nbsp;</span></p>
        <p class="c2">
            <span>A </span
            ><span class="c11">fully connected feed-forward network</span
            ><span>&nbsp;takes </span><span class="c11">inputs </span
            ><span class="c11 c1">x</span><span class="c7 c11 c1">1</span
            ><span class="c11 c1">&hellip;x</span
            ><span class="c7 c11 c1">n</span><span>&nbsp;and a </span
            ><span class="c11">bias term </span><span class="c11 c1">b</span
            ><span>. It has a </span><span class="c11">hidden layer</span
            ><span>&nbsp;with units </span><span class="c11 c1">h</span
            ><span class="c7 c11 c1">1</span
            ><span class="c11 c1">&hellip;h</span
            ><span class="c7 c11 c1">n</span><span>&nbsp;and an </span
            ><span class="c11">output layer</span><span>&nbsp;with units </span
            ><span class="c11 c1">y</span><span class="c7 c11 c1">1</span
            ><span class="c11 c1">&hellip;y</span
            ><span class="c7 c11 c1">n</span
            ><span>. The weights and bias term are combined into a </span
            ><span class="c11">weight matrix </span><span class="c11 c1">W</span
            ><span>. The </span><span class="c11">sigmoid function</span
            ><span>&nbsp;is used in the hidden layer to output the </span
            ><span class="c11">output </span><span class="c11 c1">h</span
            ><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 150.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image217.png"
                    style="
                        width: 150.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The hidden layer has a </span
            ><span class="c11">weight matrix </span><span class="c11 c1">U</span
            ><span>, which is multiplied by the input to produce an </span
            ><span class="c11">output </span><span class="c11 c1">z</span
            ><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 104px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image26.png"
                    style="
                        width: 104px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">softmax function</span
            ><span class="c0"
                >&nbsp;normalises the output to a binary vector following a
                probability distribution where all values sum to 1:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 198.67px;
                    height: 38.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image28.png"
                    style="
                        width: 198.67px;
                        height: 38.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The final </span><span class="c11">output</span
            ><span class="c11 c1">&nbsp;y </span
            ><span>of the network is the normalised vector </span
            ><span class="c1">z</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 158.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image70.png"
                    style="
                        width: 158.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>To calculate the output of the N</span
            ><span class="c21">th</span
            ><span class="c0">&nbsp;layer in a neural network:</span>
        </p>
        <ul class="c6 lst-kix_mcwl0f51uynt-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c11">output </span
                ><span class="c11 c1">z</span
                ><span class="c21 c11 c1">[i] </span
                ><span>is calculated using &nbsp;its </span
                ><span class="c11">weight matrix </span
                ><span class="c11 c1">W</span><span class="c21 c11 c1">[i]</span
                ><span>, </span><span class="c11">bias term </span
                ><span class="c11 c1">b</span><span class="c21 c11 c1">[i]</span
                ><span>, and the </span><span class="c11">output </span
                ><span class="c11 c1">a</span
                ><span class="c21 c11 c1">[i-1] </span
                ><span class="c0">of the previous layer</span>
            </li>
        </ul>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 189.33px;
                    height: 18.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image196.png"
                    style="
                        width: 189.33px;
                        height: 18.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_hdhjceopofe-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c11">previous output </span
                ><span class="c11 c1">a</span
                ><span class="c11 c1 c21">[i] </span
                ><span>is calculated using an </span
                ><span class="c11">activation function </span
                ><span class="c11 c1">g</span><span class="c21 c11 c1">[i]</span
                ><span>&nbsp;and the </span><span class="c11">output </span
                ><span class="c11 c1">z</span
                ><span class="c21 c11 c1">[i] </span
                ><span class="c0">of that layer</span>
            </li>
        </ul>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 137.33px;
                    height: 18.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image79.png"
                    style="
                        width: 137.33px;
                        height: 18.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c13 c1"
                >where square bracket notation [i] represents a layer
                number</span
            >
        </p>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Each </span><span class="c11">layer</span
            ><span class="c0"
                >&nbsp;of the network is represented in the following
                form:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 156px;
                    height: 132px;
                "
                ><img
                    alt="[2] 
+ b12] "
                    src="assets/natural-language-processing/image39.png"
                    style="
                        width: 156px;
                        height: 132px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The algorithm for computing the</span
            ><span class="c11">&nbsp;forward step</span
            ><span
                >&nbsp;in an n-layer feed-forward network given the input vector </span
            ><span class="c1">a</span><span class="c21 c1">[0] </span
            ><span class="c0">is given by:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 190.67px;
                    height: 80px;
                "
                ><img
                    alt="for i in l..n 
zii W&#39; all&macr; 
dil gli) (ZIO) "
                    src="assets/natural-language-processing/image226.png"
                    style="
                        width: 190.67px;
                        height: 80px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Training Neural Networks</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A feed-forward neural network is a </span
            ><span class="c11">supervised learning</span
            ><span>&nbsp;task using an observation </span
            ><span class="c1">x </span><span>and target </span
            ><span class="c1">y</span
            ><span
                >&nbsp;at each step. This system produces an estimate of the </span
            ><span class="c11">true output value </span
            ><img src="assets/natural-language-processing/image3.png" /><span>.</span
            ><span class="c11">&nbsp;</span
            ><span>The training procedure learns the </span
            ><span class="c11">parameters </span><span class="c11 c1">W</span
            ><span class="c21 c11 c1">[i] </span><span class="c11">and </span
            ><span class="c11 c1">b</span><span class="c21 c11 c1">[i]</span
            ><span>&nbsp;for each layer </span><span class="c1">i </span
            ><span>to make </span><img src="assets/natural-language-processing/image3.png" /><span
                >&nbsp;closer to </span
            ><span class="c1">y</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The</span><span class="c11">&nbsp;loss function</span
            ><span
                >&nbsp;measures the prediction error by calculating the
                difference between true values and prediction values. The </span
            ><span class="c11">cross-entropy </span
            ><span class="c0">loss function for logistic regression is:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 394.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image121.png"
                    style="
                        width: 394.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >For a neural network with a probability distribution vector </span
            ><span class="c1">C</span><span>, the </span
            ><span class="c11">cross-entropy </span
            ><span class="c0">loss is:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 208px;
                    height: 46.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image136.png"
                    style="
                        width: 208px;
                        height: 46.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Neural Language Models</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Neural language models use a </span
            ><span class="c11">moving window</span
            ><span
                >&nbsp;to move over a given text. Assuming the vector embeddings
                have already been </span
            ><span class="c11">pretrained</span><span>, each group of </span
            ><span class="c11 c1">N </span><span class="c11">embeddings</span
            ><span>&nbsp;are concatenated as a </span
            ><span class="c11">one-hot vector</span
            ><span
                >. (A one-hot vector is a vector where one element is equal to 1
                and all other elements are set to zero). The </span
            ><span class="c11">embedding matrix </span
            ><span class="c11 c1">E </span
            ><span>is a dictionary of embeddings that contains a </span
            ><span class="c1">d</span
            ><span
                >-dimensional column vector for each word. At each time step </span
            ><span class="c1">t</span><span>, </span><span class="c1">N </span
            ><span>embeddings are taken and multiplied by </span
            ><span class="c11 c1">E</span
            ><span>, then all three are converted into a </span
            ><span class="c11 c1">d</span
            ><span class="c11">-dimensional embedding</span
            ><span>. Each of these is multiplied by the </span
            ><span class="c11">weight matrix </span
            ><span class="c11 c1">W </span><span>and put into an </span
            ><span class="c11">activation function</span
            ><span>&nbsp;to produce the </span
            ><span class="c11">hidden layer </span><span class="c11 c1">h</span
            ><span>. This is multiplied by its </span
            ><span class="c11">weight matrix </span
            ><span class="c11 c1">U </span><span>and put into the </span
            ><span class="c11">softmax function</span
            ><span class="c0">&nbsp;to produce the final output.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >This network predicts the probability that the next word </span
            ><span class="c1">w</span><span class="c7 c1">t </span
            ><span>will be a given vocabulary word </span
            ><span class="c1">Vi</span
            ><span>&nbsp;and uses the window size </span
            ><span class="c1">N=3</span><span class="c0">.</span>
        </p>
        <ul class="c6 lst-kix_3i8muu1fq84k-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Select 3 embeddings from </span
                ><span class="c38 c11 c1 c63">E</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_3i8muu1fq84k-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >takes the next three words and creates three one-hot
                    vectors</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>multiplies each vector by the embedding matrix </span
                ><span class="c8 c1">E</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>each row of</span><span class="c1">&nbsp;E</span
                ><span
                    >&nbsp;is a word embedding and the input is a one-hot column
                    vector </span
                ><span class="c1">x</span><span class="c46 c7 c1">i</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>so the projection layer for input </span
                ><span class="c1">w</span><span>&nbsp;is </span
                ><span class="c1">Ex</span><span class="c7 c1">i</span
                ><span class="c1">&nbsp;= e</span
                ><span class="c46 c7 c1">i</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0">the three embeddings are concatenated</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_3i8muu1fq84k-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Multiply by </span
                ><span class="c38 c11 c1 c63">W</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_3i8muu1fq84k-1 start">
            <li class="c2 c32 li-bullet-0">
                <span>multiply the </span><span class="c1">d-</span
                ><span>dimensional embedding by the weight matrix </span
                ><span class="c8 c1">W</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span
                    >pass through the activation function to get the hidden
                    layer </span
                ><span class="c8 c1">h</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_3i8muu1fq84k-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Multiply by U</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_3i8muu1fq84k-1 start">
            <li class="c2 c32 li-bullet-0">
                <span>multiply the hidden layer </span><span class="c1">h</span
                ><span>&nbsp;by </span><span class="c1">U </span
                ><span class="c0">to get the output layer</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_3i8muu1fq84k-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Apply softmax</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_3i8muu1fq84k-1 start">
            <li class="c2 c32 li-bullet-0">
                <span
                    >use the softmax function to produce the final output layer </span
                ><span
                    style="
                        overflow: hidden;
                        display: inline-block;
                        margin: 0px 0px;
                        border: 0px solid #000000;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                        width: 389.33px;
                        height: 17.33px;
                    "
                    ><img
                        alt=""
                        src="assets/natural-language-processing/image211.png"
                        style="
                            width: 389.33px;
                            height: 17.33px;
                            margin-left: 0px;
                            margin-top: 0px;
                            transform: rotate(0rad) translateZ(0px);
                            -webkit-transform: rotate(0rad) translateZ(0px);
                        "
                        title=""
                /></span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>each node </span><span class="c1">I </span
                ><span>in the output layer estimates the probability </span
                ><span class="c1">P(w</span><span class="c7 c1">t</span
                ><span class="c1">&nbsp;= i|w</span
                ><span class="c7 c1">t&minus;1</span><span class="c1">,w</span
                ><span class="c7 c1">t&minus;2</span><span class="c1">,w</span
                ><span class="c7 c1">t&minus;3</span
                ><span class="c8 c1">)</span>
            </li>
        </ul>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 444px;
                    height: 302.67px;
                "
                ><img
                    alt="p(aardvarkl ...) p(fishl... 
Output layer y I 
Y42 
softmax 
Hidden layer 
Projection layer 
embedding for 
word 35 
and thanks 
for 
wt-3 
p(forl...) 
Y59 
embedding for 
word 9925 
p(zebral...) 
YIV IVIx1 
IVIXdh 
dhX1 
dhX3d 
0 3dx1 
embedding for 
word 45180 
the "
                    src="assets/natural-language-processing/image214.png"
                    style="
                        width: 444px;
                        height: 302.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>To train a neural language model, the </span
            ><span class="c11">parameters </span
            ><span class="c11 c1">W, U, E, b </span
            ><span>must be set using </span
            ><span class="c11">gradient descent</span><span>. </span
            ><span class="c11">Backpropagation</span
            ><span
                >&nbsp;is performed on the network to compute the gradient and
                set </span
            ><span class="c1">W, U</span><span>&nbsp;and </span
            ><span class="c1">E</span
            ><span>. Training proceeds by starting with </span
            ><span class="c11">random weights</span><span>&nbsp;and </span
            ><span class="c11">iteratively adjusting</span
            ><span
                >&nbsp;them as the network scans the input text. The
                cross-entropy loss for each word </span
            ><span class="c1">w</span><span class="c7 c1">t </span
            ><span class="c0">is:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 252px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image92.png"
                    style="
                        width: 252px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Recurrent Neural Networks</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">recurrent neural network</span
            ><span class="c0"
                >&nbsp;is a neural network that contains a cycle within its
                connections. Recurrent units are dependent on their own output
                as an input.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>An</span><span class="c11">&nbsp;Elman network</span
            ><span
                >&nbsp;or simple recurrent network is a single unit containing
                an input layer </span
            ><span class="c1">x</span><span class="c7 c1">t</span
            ><span>, hidden layer </span><span class="c1">h</span
            ><span class="c7 c1">t</span><span>, and output layer </span
            ><span class="c1">y</span><span class="c7 c1">t</span
            ><span
                >. The hidden layer contains a recurrent connection where the
                previous output becomes the current input. </span
            ><span class="c1">W </span
            ><span>is the weight matrix for the input layer. </span
            ><span class="c1">U </span
            ><span
                >is the matrix of hidden weights, which connects the hidden
                layer from the previous time step to the current time step. </span
            ><span class="c1">V </span
            ><span class="c0">is the matrix of output weights.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 354.67px;
                    height: 158.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image207.png"
                    style="
                        width: 354.67px;
                        height: 158.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">activation function</span
            ><span>&nbsp;from the </span><span class="c11">hidden layer </span
            ><span class="c11 c1">h</span><span class="c7 c11 c1">t </span
            ><span>is used to compute an </span><span class="c11">output </span
            ><span class="c11 c1">y</span><span class="c7 c11 c1">t </span
            ><span>for </span><span class="c11">input </span
            ><span class="c11 c1">x</span><span class="c7 c11 c1">t</span
            ><span class="c0"
                >. The hidden layer is calculated by passing the weighted values
                of the previous time step and the input of the current time step
                into the activation function. The output layer is calculated by
                passing the weighted values from the hidden layer into some
                activation function:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 188px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image57.png"
                    style="
                        width: 188px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 133.33px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image202.png"
                    style="
                        width: 133.33px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0"
                >For soft classification (using class probabilities) the softmax
                function can be used:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 177.33px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image89.png"
                    style="
                        width: 177.33px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The algorithm for computing the</span
            ><span class="c11">&nbsp;forward step</span
            ><span>&nbsp;in an n-layer </span><span class="c11">recurrent</span
            ><span class="c0">&nbsp;feed-forward network is given by:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 394.67px;
                    height: 122.67px;
                "
                ><img
                    alt="function FORWARDRNN(x, network) returns output sequence y 
for if&mdash; I to LENGTH(x) do 
hi&mdash;I + 
hi) 
return y "
                    src="assets/natural-language-processing/image227.png"
                    style="
                        width: 394.67px;
                        height: 122.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Modern deep learning models </span
            ><span class="c11"
                >unroll the recurrent neural network in time to compute
                loss</span
            ><span>. The</span><span class="c22">&nbsp;learning template</span
            ><span
                >&nbsp;specifies the network structure, parameters, weight
                matrices, activation and output functions. Given an </span
            ><span class="c11">input sequence</span><span>, the model </span
            ><span class="c11"
                >generates an unrolled feed-forward network over time</span
            ><span>. Regular </span><span class="c22">inference</span
            ><span>&nbsp;or </span><span class="c22">backpropagation</span
            ><span class="c0">&nbsp;can now be performed on the network.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">RNN Language Models</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">RNN language models</span
            ><span>&nbsp;process inputs one word at a time. They </span
            ><span class="c11">predict</span
            ><span
                >&nbsp;the next word in the sequence using the current word and
                hidden state. The </span
            ><span class="c11">input </span><span class="c11 c1">x </span
            ><span>consists of</span
            ><span class="c11">&nbsp;word embeddings</span
            ><span>&nbsp;represented as one-hot vectors and </span
            ><span class="c11">output predictions </span
            ><span class="c11 c1">y</span
            ><span>. At each step, the model uses the </span
            ><span class="c11">embedding matrix </span
            ><span class="c11 c1">E </span
            ><span
                >to retrieve the embedding for the current word and combines it
                with the previous </span
            ><span class="c11">hidden layer</span
            ><span
                >&nbsp;to compute the new hidden layer. This passes through the </span
            ><span class="c11">output layer</span><span>&nbsp;and </span
            ><span class="c11">softmax function</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 118.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image247.png"
                    style="
                        width: 118.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 186.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image73.png"
                    style="
                        width: 186.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 177.33px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image169.png"
                    style="
                        width: 177.33px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Given the output </span><span class="c1">y</span
            ><span>, the </span><span class="c11">probability of a word </span
            ><span class="c11 c1">i </span><span>occurring</span
            ><span>&nbsp;next is its corresponding </span
            ><span class="c11 c1">y</span
            ><span class="c11">&nbsp;component</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 278.67px;
                    height: 45.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image235.png"
                    style="
                        width: 278.67px;
                        height: 45.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>In </span><span class="c11">teacher forcing</span
            ><span>, an RNN language model is trained using the</span
            ><span class="c11">&nbsp;cross-entropy</span
            ><span class="c0">&nbsp;loss function:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 190.67px;
                    height: 40px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image238.png"
                    style="
                        width: 190.67px;
                        height: 40px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>For</span><span class="c11">&nbsp;language modelling</span
            ><span>, the </span><span class="c11">cross-entropy</span
            ><span class="c0"
                >&nbsp;loss is determined by the probability of the next
                word:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 204px;
                    height: 18.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image176.png"
                    style="
                        width: 204px;
                        height: 18.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Applications of RNN Language Models</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_wwl3xkjbpr6o-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c3"
                    >Autoregressive Generation (Text Generation)</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_wwl3xkjbpr6o-1 start">
            <li class="c2 c32 li-bullet-0">
                <span>takes an input sequence </span><span class="c1">x </span
                ><span>and outputs the next word </span
                ><span class="c8 c1">y</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >uses pre-trained word embeddings and cross-entropy</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >looks at word associations to find related words</span
                >
            </li>
        </ul>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 358.67px;
                    height: 234.67px;
                "
                ><img
                    alt="Sampled Word 
Embeddi ng 
Input Word 
hole 
RNN 
hole "
                    src="assets/natural-language-processing/image147.png"
                    style="
                        width: 358.67px;
                        height: 234.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_7qwk93cjgrpy-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Sequence Labelling (POS Tagging)</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_7qwk93cjgrpy-1 start">
            <li class="c2 c32 li-bullet-0">
                <span>takes an input sequence </span><span class="c1">x</span
                ><span>&nbsp;and outputs POS tag probabilities </span
                ><span class="c8 c1">y</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >uses pre-trained word embeddings and cross-entropy</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span
                    >the RNN block represents an unrolled recurrent neural
                    network consisting of an input layer, hidden layer, and
                    output layer at each time step, with weight matrices </span
                ><span class="c8 c1">U, V, W</span>
            </li>
        </ul>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 354.67px;
                    height: 208px;
                "
                ><img
                    alt="Soft m 
Em "
                    src="assets/natural-language-processing/image201.png"
                    style="
                        width: 354.67px;
                        height: 208px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_jxe0j891onjc-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Sequence Classification</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_jxe0j891onjc-1 start">
            <li class="c2 c32 li-bullet-0">
                <span>takes an input sequence </span><span class="c1">x </span
                ><span>and outputs a class probability </span
                ><span class="c8 c1">y</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >input is processed one word at a time, generating a new
                    hidden layer at each time step</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>the final hidden layer </span><span class="c1">h</span
                ><span class="c7 c1">n </span
                ><span class="c0"
                    >is passed into the softmax function to find class
                    probabilities</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 350.67px;
                    height: 216px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image30.png"
                    style="
                        width: 350.67px;
                        height: 216px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Stacked and Bidirectional RNNs</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>In a </span><span class="c11">stacked RNN</span
            ><span class="c0"
                >, the output of one RNN is used as the input to another. Adding
                layers boosts the performance of the stack and allows
                abstraction of inputs.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 385.33px;
                    height: 218.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image128.png"
                    style="
                        width: 385.33px;
                        height: 218.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>In a simple recurrent network, the </span
            ><span class="c11">hidden state</span><span>&nbsp;at time </span
            ><span class="c1">t </span><span>represents</span
            ><span class="c22">&nbsp;all the information in the network</span
            ><span>&nbsp;at that time step, as a result of some function </span
            ><span class="c1">f</span
            ><span class="c0">&nbsp;applied to the inputs:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 188px;
                    height: 21.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image118.png"
                    style="
                        width: 188px;
                        height: 21.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>If the network is trained in </span
            ><span class="c11">reverse</span><span>, the</span
            ><span class="c11">&nbsp;hidden state</span
            ><span>&nbsp;at time </span><span class="c1">t </span
            ><span>represents </span
            ><span class="c22">the information to the righ</span
            ><span class="c0">t of the current input:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 188px;
                    height: 18.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image224.png"
                    style="
                        width: 188px;
                        height: 18.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">bidirectional RNN</span
            ><span>&nbsp;combines the </span
            ><span class="c22">forward and backward networks</span
            ><span>&nbsp;into a single representation that captures the </span
            ><span class="c22">left and right context</span
            ><span class="c0">&nbsp;of an input at each point in time:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span>&oplus; </span
            ><span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 138.67px;
                    height: 20px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image107.png"
                    style="
                        width: 138.67px;
                        height: 20px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 388px;
                    height: 208px;
                "
                ><img
                    alt="&#12298; &#20023; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#20886; &#8545; &#19968; &#19968; ~ &#12301; &#19968; NNE "
                    src="assets/natural-language-processing/image77.png"
                    style="
                        width: 388px;
                        height: 208px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c20">Sequence Processing</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Memory Gates</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_homgknctxqdt-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">LSTM (Long Short-Term Memory)</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_homgknctxqdt-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >RNN architecture containing feedback connections, composed
                    of a memory cell, input gate, output gate, and forget
                    gate</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >Remembers values over time while three gates regulate the
                    flow of information into/out of the cell</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >Can add or remove information to the memory cell</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c22">Input Gate:</span
                ><span class="c0"
                    >&nbsp;provides input data from previous layers</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c22">Output Gate:</span
                ><span class="c0"
                    >&nbsp;provides output data to future layers</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c22">Memory Cell:</span
                ><span class="c0">&nbsp;internal memory of the neuron</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c22">Forget Gate:</span
                ><span class="c0">&nbsp;controls signals</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 357.33px;
                    height: 245.33px;
                "
                ><img
                    alt="Long Short-Term Memory (LSTM) 
Other part of the network 
Signal control 
Output Gate 
the output gate 
(Other part of 
the network) 
Signal control 
the input gate 
(Other part of 
the network) 
Other part of the network 
Special Neuron: 
4 inputs, 
1 output 
Signal control 
the forget gate 
(Other part of 
the network) 
LSTM "
                    src="assets/natural-language-processing/image58.png"
                    style="
                        width: 357.33px;
                        height: 245.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 356px;
                    height: 222.67px;
                "
                ><img
                    alt="Forget gate 
Input gate 
t) 
o 
Output gate 
I-STM cell 
Element-wise 
multiplication 
&#39; Addition 
logistic 
tanh "
                    src="assets/natural-language-processing/image232.png"
                    style="
                        width: 356px;
                        height: 222.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_3w8iy1i4tiep-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">GRU (Gated Recurrent Unit)</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_3w8iy1i4tiep-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >Simplification of LSTM, but has fewer parameters and lacks
                    an output gate</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >Contains a memory cell, input gate, and forget gate</span
                >
            </li>
        </ul>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 354.67px;
                    height: 234.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image195.png"
                    style="
                        width: 354.67px;
                        height: 234.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c46 c71 c29">&nbsp;</span></p>
        <p class="c2"><span class="c26">Attention</span></p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Self-attention</span
            ><span
                >&nbsp;allows a network to extract and use information from a
                given context without using recurrent connections. To compute an </span
            ><span class="c11">attention score</span
            ><span
                >, it compares an item to a collection of other items to
                calculate its </span
            ><span class="c11">relevance</span
            ><span class="c0"
                >&nbsp;in the current context. This uses a dot product
                comparison:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 185.33px;
                    height: 18.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image110.png"
                    style="
                        width: 185.33px;
                        height: 18.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >Once all the scores are calculated, they are normalised with
                the </span
            ><span class="c11">softmax</span
            ><span>&nbsp;function to create a </span
            ><span class="c11">vector of weights </span
            ><span>indicating the proportional relevance of each input </span
            ><span class="c1">i</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 198.67px;
                    height: 38.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image103.png"
                    style="
                        width: 198.67px;
                        height: 38.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 388px;
                    height: 38.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image206.png"
                    style="
                        width: 388px;
                        height: 38.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Given the </span><span class="c11">score vector </span
            ><span class="c11 c1">&alpha;</span><span>, it generates an </span
            ><span class="c11">output </span><span class="c11 c1">y</span
            ><span class="c7 c11 c1">i </span><span>by calculating the </span
            ><span class="c11">sum of weighted inputs</span
            ><span class="c0"
                >&nbsp;(multiplying the inputs and relevance values):</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 140px;
                    height: 42.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image25.png"
                    style="
                        width: 140px;
                        height: 42.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Transformers</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Query: </span
            ><span class="c22">current focus of attention</span
            ><span class="c0"
                >&nbsp;that is being compared to all other previous inputs</span
            >
        </p>
        <p class="c2">
            <span class="c11">Key: </span
            ><span class="c22">preceding input</span
            ><span class="c0">&nbsp;that is being compared to the query</span>
        </p>
        <p class="c2">
            <span class="c11">Value: </span><span class="c22">embedding</span
            ><span class="c0"
                >&nbsp;used to compute the output for the current focus of
                attention</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Transformers use three sets of </span
            ><span class="c11">weights </span><span class="c11 c1">W</span
            ><span class="c21 c11 c1">Q</span><span class="c11">,</span
            ><span class="c11 c1">&nbsp;W</span><span class="c21 c11 c1">K</span
            ><span class="c11 c1">, </span><span class="c11">and </span
            ><span class="c11 c1">W</span><span class="c21 c11 c1">V</span
            ><span>&nbsp;which are multiplied with inputs </span
            ><span class="c1">x</span><span class="c7 c1">i</span
            ><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 270.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image192.png"
                    style="
                        width: 270.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 256px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image86.png"
                    style="
                        width: 256px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">self-attention score</span
            ><span>&nbsp;between the current input </span
            ><span class="c1">x</span><span class="c7 c1">i </span
            ><span>and some element </span><span class="c1">x</span
            ><span class="c7 c1">j</span><span>&nbsp;is the </span
            ><span class="c11">dot product of the query and key vectors</span
            ><span>, normalised by the number of </span
            ><span class="c11">dimensions</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 185.33px;
                    height: 38.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image35.png"
                    style="
                        width: 185.33px;
                        height: 38.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 320px;
                    height: 42.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image230.png"
                    style="
                        width: 320px;
                        height: 42.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The final </span><span class="c11">softmax</span
            ><span>&nbsp;calculation remains the same but is based on the </span
            ><span class="c11">weighted sum of the value vectors </span
            ><span class="c11 c1">v</span><span class="c0">:</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 140px;
                    height: 42.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image184.png"
                    style="
                        width: 140px;
                        height: 42.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 330.67px;
                    height: 296px;
                "
                ><img
                    alt="Weight and Sum 
value vectors 
Softmax 
Key/Query 
Comparisons 
Generate 
key, query value 
vectors 
Output Vector 
x 
x "
                    src="assets/natural-language-processing/image183.png"
                    style="
                        width: 330.67px;
                        height: 296px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Self-Attention Networks</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">transformer</span
            ><span
                >&nbsp;is an encoder-decoder architecture that handles
                long-range sequences by using </span
            ><span class="c11">attention</span><span>&nbsp;to compute the </span
            ><span class="c22">relevance of each input </span
            ><span
                >to produce an output. Transformers map sequences of input
                vectors </span
            ><span class="c1">x</span><span class="c7 c1">1</span
            ><span class="c1">&hellip;x</span><span class="c7 c1">n </span
            ><span>to output sequences </span><span class="c1">y</span
            ><span class="c7 c1">1</span><span class="c1">&hellip;y</span
            ><span class="c7 c1">n </span
            ><span>of the same length. They are made up of </span
            ><span class="c11">stacks</span
            ><span>&nbsp;of network layers consisting of </span
            ><span class="c22"
                >simple layers, self-attention layers, feed-forward networks,
                and connections</span
            ><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Multi-head self-attention layers</span
            ><span>&nbsp;are </span
            ><span class="c22">groups of self-attention layers</span
            ><span>&nbsp;called </span><span class="c11">heads</span
            ><span>&nbsp;which work in </span><span class="c11">parallel</span
            ><span>&nbsp;at the same </span><span class="c11">depth</span
            ><span>&nbsp;in a model, each with their own set of </span
            ><span class="c11">parameters</span
            ><span>. Given these distinct parameters, each head can </span
            ><span class="c22"
                >learn different aspects of the text at the same level of
                abstraction</span
            ><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 434.67px;
                    height: 254.67px;
                "
                ><img
                    alt="Multihead Attention 
Layer 
x, 
Concat 
- ttentlon 
N heads 
x "
                    src="assets/natural-language-processing/image83.png"
                    style="
                        width: 434.67px;
                        height: 254.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Positional Embeddings</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Transformer inputs are combined with </span
            ><span class="c11">positional embeddings</span
            ><span class="c0"
                >&nbsp;so that the network can keep track of each element and
                its location.</span
            >
        </p>
        <ul class="c6 lst-kix_scpbwkg7cc5l-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >transformers do not encode the positions of words, so use
                    positional embeddings</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>these can be</span
                ><span class="c11">&nbsp;learned during training</span
                ><span>&nbsp;or generated using a </span
                ><span class="c3">positional encoding function</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>positional embeddings have the same </span
                ><span class="c11">dimensions</span
                ><span class="c0">&nbsp;as word embeddings</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the positional embedding and word embedding are </span
                ><span class="c11">summed</span
                ><span class="c0">&nbsp;to give the full embedding</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c50">Machine Translation</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Language Divergence and Typology</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Some aspects of language are </span
            ><span class="c11">universal</span
            ><span
                >&nbsp;and hold true for every language. These include
                greetings, pronouns, politeness, questions, nouns and verbs.
                However, languages also differ in many ways. The study of
                linguistic similarities and differences is called</span
            ><span class="c11">&nbsp;linguistic typology</span
            ><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Word order typology</span
            ><span class="c0"
                >&nbsp;is the difference in the order of verbs, subjects, and
                objects in different languages. Languages such as English,
                French, and German are SVO (Subject-Verb-Object), Hindi and
                Japanese are SOV (Subject-Object-Verb), and Irish and Arabic are
                VSO (Verb-Subject-Object).</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 464px;
                    height: 49.33px;
                "
                ><img
                    alt="Subject-Verb-Object (SVO) 
Subject-Object-Verb (SOV) 
Verb-Subject-Object (VSO) 
English, German, French, Mandarin 
Japanese, Hindi 
Arabic, Irish "
                    src="assets/natural-language-processing/image167.png"
                    style="
                        width: 464px;
                        height: 49.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 464px;
                    height: 105.33px;
                "
                ><img
                    alt="The reen wi 
Diese W 
ho 
this wee 
ist ie 
r&uuml;ne Hexe zu Hause 
Cheng long dao xiang gang qu 
Jackie Chan went to Hong Kong 
(b) "
                    src="assets/natural-language-processing/image142.png"
                    style="
                        width: 464px;
                        height: 105.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Referential density</span
            ><span
                >&nbsp;refers to the frequencies of omission in different
                languages. </span
            ><span class="c22">Cold</span
            ><span
                >&nbsp;languages can omit pronouns and require the reader to
                infer the subject of the sentence, while </span
            ><span class="c22">hot </span
            ><span class="c0">languages are explicit.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">The Encoder-Decoder Model</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Encoder-decoder</span
            ><span class="c0"
                >&nbsp;networks are sequence-to-sequence networks capable of
                generating translated output sequences. The encoder takes the
                input sequence and creates a contextualised representation of it
                using word embeddings. The context is passed to the decoder,
                which generates a translated output sequence.</span
            >
        </p>
        <ul class="c6 lst-kix_idr7566nmstq-0 start">
            <li class="c2 c18 li-bullet-0"><span class="c3">Encoder</span></li>
        </ul>
        <ul class="c6 lst-kix_idr7566nmstq-1 start">
            <li class="c2 c32 li-bullet-0">
                <span>accepts an input sequence </span
                ><img src="assets/natural-language-processing/image4.png" /><span class="c0">&nbsp;</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span
                    >generates a corresponding sequence of contextualised
                    representations </span
                ><img src="assets/natural-language-processing/image5.png" />
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >can be implemented using LSTM, GRU, transformers,
                    stacks</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_idr7566nmstq-0">
            <li class="c2 c18 li-bullet-0"><span class="c3">Context</span></li>
        </ul>
        <ul class="c6 lst-kix_idr7566nmstq-1 start">
            <li class="c2 c32 li-bullet-0">
                <span>the context vector </span><span class="c1">c</span
                ><span>&nbsp;is a function of </span
                ><img src="assets/natural-language-processing/image5.png" /><span class="c0">&nbsp;</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c1">c </span
                ><span class="c0">is passed to the decoder as input</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_idr7566nmstq-0">
            <li class="c2 c18 li-bullet-0"><span class="c3">Decoder</span></li>
        </ul>
        <ul class="c6 lst-kix_idr7566nmstq-1 start">
            <li class="c2 c32 li-bullet-0">
                <span>accepts </span><span class="c1">c </span
                ><span>as input and generates a sequence of hidden states</span
                ><img src="assets/natural-language-processing/image6.png" /><span class="c0">&nbsp;</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>uses</span><img src="assets/natural-language-processing/image6.png" /><span
                    >&nbsp;to generate a sequence of output states</span
                ><img src="assets/natural-language-processing/image7.png" /><span class="c0">&nbsp;</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 461.33px;
                    height: 150.67px;
                "
                ><img
                    alt="Context "
                    src="assets/natural-language-processing/image124.png"
                    style="
                        width: 461.33px;
                        height: 150.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Using the </span><span class="c11">source text </span
            ><span class="c11 c1">x </span><span>and</span
            ><span class="c11">&nbsp;target </span><span class="c11 c1">y</span
            ><span>, the encoder-decoder model</span
            ><span class="c22"
                >&nbsp;predicts the next target word in sequence </span
            ><span class="c1 c22">y</span
            ><span class="c22"
                >&nbsp;based on the previous target word and source sequence </span
            ><span class="c1 c22">x</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 432px;
                    height: 26.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image100.png"
                    style="
                        width: 432px;
                        height: 26.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>This </span><span class="c11">encoder-decoder network</span
            ><span class="c0"
                >&nbsp;translates from English to Spanish. To translate the
                source text, we run it through the encoder to generate hidden
                states and the context vector. The decoder uses the context and
                the previous hidden state to generate each successive word in
                the translation.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 546.67px;
                    height: 222.67px;
                "
                ><img
                    alt="Decoder 
(output is ignored during encoding) 
softmax 
hidden 
n = hdO 
he 
layer(s) 
embedding 
layer 
Encoder 
hd 
hd "
                    src="assets/natural-language-processing/image151.png"
                    style="
                        width: 546.67px;
                        height: 222.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">encoder</span
            ><span>&nbsp;takes an </span><span class="c11">input sequence </span
            ><span class="c11 c1">x </span
            ><span
                >and generates contextualised representations for each time
                step. The </span
            ><span class="c11">final state </span
            ><img src="assets/natural-language-processing/image8.png" /><span>&nbsp;is the </span
            ><span class="c11">context </span><span class="c11 c1">c</span
            ><span>, which is passed to the decoder. The </span
            ><span class="c11">decoder</span><span>&nbsp;uses </span
            ><span class="c11 c1">c </span><span>as the</span
            ><span class="c11">&nbsp;first hidden state </span
            ><img src="assets/natural-language-processing/image9.png" /><span>. It generates a </span
            ><span class="c11">sequence of outputs </span
            ><img src="assets/natural-language-processing/image10.png" /><span class="c11">&nbsp;</span
            ><span
                >until an end-of-sequence marker is generated. Each hidden state
                is trained on the previous output, previous hidden state, and
                context</span
            ><span class="c1">&nbsp;c</span><span class="c0">. </span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 148px;
                    height: 120px;
                "
                ><img
                    alt="hid 
Ifn 
softmax(zr) "
                    src="assets/natural-language-processing/image157.png"
                    style="
                        width: 148px;
                        height: 120px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">output </span
            ><span class="c11 c1">y </span
            ><span>at each time step is passed through a </span
            ><span class="c11">softmax function</span
            ><span class="c0"
                >&nbsp;over the set of all possible outputs to calculate the
                most likely output using the argmax:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 232px;
                    height: 22.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image116.png"
                    style="
                        width: 232px;
                        height: 22.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Training the Encoder-Decoder Model</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Encoder-decoder networks are trained using</span
            ><span class="c11">&nbsp;source-target pairs</span
            ><span
                >&nbsp;as training data. In training, it is common to use </span
            ><span class="c11">teacher forcing</span
            ><span
                >&nbsp;in the decoder. This means that the system is forced to
                use the </span
            ><span class="c11">correct target word</span
            ><span>&nbsp;as the </span><span class="c11">next input </span
            ><span class="c11 c1">x</span><span class="c7 c11 c1">t+1</span
            ><span>, rather than relying on its </span
            ><span class="c11">predicted output </span
            ><img src="assets/natural-language-processing/image11.png" /><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 504px;
                    height: 266.67px;
                "
                ><img
                    alt="Total loss is the average 
cross-entropy Oss per L = 
target word: 
Ilep&ouml; 
PO&#39; 
arrived 
la 
Ileg&ouml; 
Decoder 
bruja 
la 
verde 
bruja 
Ys 
verde 
answers 
per- word 
soft max 
hidden 
layer(s) 
embedding 
layer 
the 
green 
witch 
Encoder "
                    src="assets/natural-language-processing/image190.png"
                    style="
                        width: 504px;
                        height: 266.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Attention</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The context </span><span class="c1">c</span
            ><span
                >&nbsp;may act as a bottleneck for the encoder, as it must
                equally represent all data in the source text. The</span
            ><span class="c11">&nbsp;attention</span
            ><span class="c0"
                >&nbsp;mechanism is a more accurate method of implementing
                long-term memory using all states in the encoder.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">context </span
            ><span class="c11 c1">c </span
            ><span
                >is a fixed-length vector which is a function of the hidden
                states i.e. </span
            ><img src="assets/natural-language-processing/image12.png" /><span
                >. The function calculates the</span
            ><span class="c11">&nbsp;weighted sum</span
            ><span>&nbsp;of all the encoder&#39;s</span
            ><span class="c11">&nbsp;hidden states </span
            ><img src="assets/natural-language-processing/image5.png" /><span>. </span
            ><span>The context is </span
            ><span class="c11">dynamically derived</span
            ><span>&nbsp;at each point during decoding. This vector c</span
            ><span class="c7">i</span
            ><span>&nbsp;is generated again at each step </span
            ><span class="c1">i </span
            ><span class="c0"
                >and takes all of the hidden states into account.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 162.67px;
                    height: 30.67px;
                "
                ><img
                    alt="~ &#8545; g(ft-l,hf-l&#39;ci) "
                    src="assets/natural-language-processing/image32.png"
                    style="
                        width: 162.67px;
                        height: 30.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 502.67px;
                    height: 96px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image172.png"
                    style="
                        width: 502.67px;
                        height: 96px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Attention</span
            ><span>&nbsp;also looks at the relevance of each</span
            ><span class="c11">&nbsp;encoder state</span
            ><span>&nbsp;to the </span><span class="c11">decoder state </span
            ><img src="assets/natural-language-processing/image13.png" /><span
                >. The relevance is captured by computing the </span
            ><span class="c11">dot-product attention score</span
            ><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 220px;
                    height: 29.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image47.png"
                    style="
                        width: 220px;
                        height: 29.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c24 c1"
                >= decoder hidden state, = encoder hidden state</span
            ><span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 329.33px;
                    height: 20px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image229.png"
                    style="
                        width: 329.33px;
                        height: 20px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c13 c29">&nbsp;</span></p>
        <p class="c2">
            <span
                >The score vector across all encoder hidden states provides the
                relevance of each decoder state to the current step of the
                decoder. These are normalised with the </span
            ><span class="c11">softmax function</span
            ><span>&nbsp;to create a </span
            ><span class="c11">weight vector </span
            ><img src="assets/natural-language-processing/image14.png" /><span>that calculates the</span
            ><span class="c11">&nbsp;proportional relevance</span
            ><span>&nbsp;of each</span
            ><span class="c11">&nbsp;encoder hidden state </span
            ><img src="assets/natural-language-processing/image15.png" /><span class="c1">&nbsp;</span
            ><span>to the </span
            ><span class="c11">previous decoder hidden state </span
            ><img src="assets/natural-language-processing/image13.png" /><span class="c3">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 248px;
                    height: 90.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image209.png"
                    style="
                        width: 248px;
                        height: 90.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Given the distribution in</span
            ><img src="assets/natural-language-processing/image16.png" /><span>, the </span
            ><span class="c11">fixed-length context vector </span
            ><span class="c11 c1">c</span><span class="c7 c11 c1">i </span
            ><span>can be computed by taking the </span
            ><span class="c11">weighted average</span
            ><span class="c0">&nbsp;over all encoder hidden states:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 120px;
                    height: 45.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image74.png"
                    style="
                        width: 120px;
                        height: 45.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Beam Search</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Beam search</span
            ><span
                >&nbsp;is a heuristic search algorithm that explores a graph by
                expanding the most promising node in a limited set. Beam search
                uses breadth-first-search to build its </span
            ><span class="c11">search tree</span
            ><span
                >. At each level, it generates the successors of each state and
                sorts them in terms of </span
            ><span class="c11">heuristic cost</span
            ><span>. It selects the </span><img src="assets/natural-language-processing/image17.png" /><span
                class="c11"
                >&nbsp;best states</span
            ><span>&nbsp;to expand at each level, where </span
            ><img src="assets/natural-language-processing/image17.png" /><span>is the </span
            ><span class="c11">beam width</span
            ><span
                >. Rather than performing greedy search, which only selects the
                most promising node, beam search selects </span
            ><img src="assets/natural-language-processing/image17.png" /><span
                >&nbsp;possible nodes at each step. In the first step of
                decoding, it computes a </span
            ><span class="c11">softmax distribution</span
            ><span
                >&nbsp;over the vocabulary, assigning a probability to each
                word. It selects the </span
            ><img src="assets/natural-language-processing/image17.png" /><span class="c11"
                >&nbsp;best options</span
            ><span
                >&nbsp;and expands each node, where the selected nodes are
                called the </span
            ><span class="c11">hypotheses</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Each of </span><img src="assets/natural-language-processing/image18.png" /><span
                class="c11"
                >&nbsp;hypotheses</span
            ><span>&nbsp;is scored by</span><span class="c11 c1">&nbsp;P(y</span
            ><span class="c11 c1 c23">i</span><span class="c11 c1">|x,y</span
            ><span class="c11 c1 c23">&lt;i</span><span class="c11 c1">)</span
            ><span
                >, which is the probability of the current word multiplied by
                the probability of the path leading to it. The beam </span
            ><span class="c11">prunes</span
            ><span>&nbsp;hypotheses down to the </span
            ><img src="assets/natural-language-processing/image17.png" /><span class="c11"
                >best hypotheses</span
            ><span
                >&nbsp;until an end token &lt;/s&gt; is generated, indicating a
                complete output. The </span
            ><span class="c11">solution is removed</span
            ><span
                >&nbsp;from the tree and beam search continues until the </span
            ><span class="c11">beam width is reduced to 0</span
            ><span class="c0">. </span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>We can use the chain rule of probability to break down </span
            ><span class="c1">P(y|x) </span
            ><span class="c0"
                >into the product of each word probability given its context,
                which we can convert into a sum of logs:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 512px;
                    height: 101.33px;
                "
                ><img
                    alt="score(y) 
logP(ylx) 
&mdash; log (P(y&#305; 
(11.20) "
                    src="assets/natural-language-processing/image63.png"
                    style="
                        width: 512px;
                        height: 101.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0"
                >This can be normalised to a fixed length by dividing by the
                number of words:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 362.67px;
                    height: 49.33px;
                "
                ><img
                    alt="score(y) = &mdash;logP(ylx) = &mdash; 
&macr; IOgP(YilYl , "
                    src="assets/natural-language-processing/image106.png"
                    style="
                        width: 362.67px;
                        height: 49.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c50">Constituency Grammars</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Constituency</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Syntactic constituency</span
            ><span
                >&nbsp;is the idea that groups of words can behave as single
                units called </span
            ><span class="c11">constituents</span
            ><span>. Words are grouped into</span
            ><span class="c11">&nbsp;phrases</span
            ><span class="c0"
                >&nbsp;such as noun phrases or verb phrases and are generated
                using grammars.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A</span><span class="c11">&nbsp;context-free grammar</span
            ><span>&nbsp;consists of a set of </span
            ><span class="c22">symbols</span><span>&nbsp;and a set of </span
            ><span class="c22">rules or productions</span
            ><span>&nbsp;of the form </span><span class="c1">A </span
            ><span class="c1">&rarr; </span><span class="c1">a </span
            ><span>where </span><span class="c1">A </span
            ><span>is non-terminal and </span><span class="c1">a </span
            ><span class="c0"
                >is a sequence of symbols. Productions are written in BNF:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c1">NP </span><span class="c1">&rarr; </span
            ><span class="c8 c1">Det Nominal</span>
        </p>
        <p class="c2 c10">
            <span class="c1">NP </span><span class="c1">&rarr;</span
            ><span class="c8 c1">&nbsp;ProperNoun</span>
        </p>
        <p class="c2 c10">
            <span class="c1">Nominal </span><span class="c1">&rarr;</span
            ><span class="c8 c1">&nbsp;Noun | Nominal Noun</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >Derivations of context-free grammars can be represented as
                a</span
            ><span class="c11">&nbsp;parse tree</span
            ><span
                >. The root node dominates nodes below it. A parse tree often
                begins with the start symbol </span
            ><span class="c1">S</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 438.67px;
                    height: 225.33px;
                "
                ><img
                    alt="Pro 
I 
Verb 
prefer 
Det 
a 
N om 
Nom Noun 
Noun flight 
mornmg "
                    src="assets/natural-language-processing/image130.png"
                    style="
                        width: 438.67px;
                        height: 225.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Grammar Rules</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_ds4gm65k59ee-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >A sentence can consist of a noun phrase and verb
                    phrase</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c11 c1">S </span><span class="c1">&rarr;</span
            ><span class="c8 c1"
                >&nbsp;NP VP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;I prefer a
                morning flight</span
            >
        </p>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_958xy41wyb4m-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >A verb phrase consists of a verb following by other
                    constituents</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c11 c1">VP </span><span class="c1">&rarr;</span
            ><span class="c8 c1"
                >&nbsp;Verb NP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp; &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;prefer a
                morning flight</span
            >
        </p>
        <p class="c2 c10">
            <span class="c11 c1">VP </span><span class="c1">&rarr;</span
            ><span class="c8 c1"
                >&nbsp;Verb NP PP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;leave
                Boston in the morning</span
            >
        </p>
        <p class="c2 c10">
            <span class="c11 c1">VP </span><span class="c1">&rarr;</span
            ><span class="c8 c1"
                >&nbsp;Verb PP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp; &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;leave on
                Thursday</span
            >
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_vjgigpf2tvlg-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >A preposition is followed by a noun phrase</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c11 c1">PP </span><span class="c1">&rarr;</span
            ><span class="c8 c1"
                >&nbsp;Preposition NP &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;from
                England</span
            >
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_kc6vgrsctmrn-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >A nominal is a head noun with extra modifiers</span
                >
            </li>
        </ul>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c11 c1">Nominal </span><span class="c1">&rarr; </span
            ><span class="c8 c1"
                >Noun &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;flight</span
            >
        </p>
        <p class="c2 c10">
            <span class="c11 c1">Nominal </span><span class="c1">&rarr;</span
            ><span class="c8 c1">&nbsp;Nominal Noun &nbsp; the flight</span>
        </p>
        <p class="c2 c10">
            <span class="c11 c1">Nominal </span><span class="c1">&rarr;</span
            ><span class="c8 c1"
                >&nbsp;Num
                Nominal&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;6
                flights</span
            >
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_u1j780j1ojlf-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>A </span><span class="c11">determiner</span
                ><span>&nbsp;is a word that appears before a noun such as </span
                ><span class="c1">a, any, some, the, this, those, these</span
                ><span>. It can also be a possessive expression such as </span
                ><span class="c1">my, your, their</span><span>, or </span
                ><span class="c1">Reece&#39;s</span><span class="c0">.</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 425.33px;
                    height: 229.33px;
                "
                ><img
                    alt="Grammar Rules 
Nominal &mdash;Y 
Pronoun 
Proper-Noun 
Det Nominal 
Nominal Noun 
Noun 
Verb 
Verb NP 
Verb NP PP 
Verb PP 
Preposition NP 
Examples 
I + want a morning flight 
Los Angeles 
a + flight 
morning + flight 
flights 
do 
want + a flight 
leave + Boston + in the morning 
leaving + on Thursday 
from + Los Angeles "
                    src="assets/natural-language-processing/image62.png"
                    style="
                        width: 425.33px;
                        height: 229.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_2zcwpeb5ul44-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>A </span><span class="c11">declarative</span
                ><span class="c0"
                    >&nbsp;sentence contains a subject noun phrase followed by a
                    verb phrase</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c11 c1">S </span><span class="c1">&rarr;</span
            ><span class="c1"
                >&nbsp;NP VP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span
            ><span class="c52 c1">The flight</span
            ><span class="c1">&nbsp; </span
            ><span class="c68 c52 c1 c63">leaves at 6pm</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_gs4qujx0nm97-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>An </span><span class="c11">imperative</span
                ><span class="c0"
                    >&nbsp;sentence contains a verb phrase with no subject</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c11 c1">S </span><span class="c1">&rarr;</span
            ><span class="c1"
                >&nbsp;VP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span
            ><span class="c68 c52 c1 c63">Show me the flight at 6pm</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_ol79pu74bqo2-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>A </span><span class="c11">yes-no question</span
                ><span class="c0"
                    >&nbsp;contains an auxiliary verb, subject noun phrase, and
                    verb phrase</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c11 c1">S </span><span class="c1">&rarr;</span
            ><span class="c1"
                >&nbsp;Aux NP VP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp; &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span
            ><span class="c52 c1">Are any flights</span
            ><span class="c1">&nbsp; </span
            ><span class="c68 c52 c1 c63">available today?</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_ppisq5mlww09-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>A </span><span class="c11">wh-subject-question</span
                ><span>&nbsp;contains a wh-word </span
                ><span class="c1"
                    >(who,whose,when,where,what,which how,why), </span
                ><span class="c0">noun phrase and verb phrase</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c11 c1">S </span><span class="c1">&rarr;</span
            ><span class="c1"
                >&nbsp;Wh-NP VP &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;
                &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span
            ><span class="c1 c52">What flights</span
            ><span class="c1">&nbsp; </span
            ><span class="c68 c52 c1 c63">leave at 6pm?</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_u8bd9ghv0b4m-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>A </span><span class="c11">wh-non-subject question</span
                ><span class="c0"
                    >&nbsp;contains a wh-phrase and two subjects</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c11 c1">S </span><span class="c1">&rarr;</span
            ><span class="c1"
                >&nbsp;Wh-NP Aux NP VP &nbsp;
                &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span
            ><span class="c52 c1">What flights</span
            ><span class="c1">&nbsp; </span><span class="c52 c1">do</span
            ><span class="c1">&nbsp; </span><span class="c52 c1">you</span
            ><span class="c1">&nbsp; </span
            ><span class="c52 c1 c63 c68">have at 6pm?</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A wh-non-subject-question contains a</span
            ><span class="c11">&nbsp;long-distance dependency</span
            ><span>&nbsp;because the </span><span class="c1">Wh-NP </span
            ><span>constituent is far away from the predicate </span
            ><span class="c8 c1">VP.</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">clause</span
            ><span>&nbsp;represents a </span
            ><span class="c1">&#39;complete thought&#39; </span
            ><span class="c0"
                >and is made up of two or more of the following
                components:</span
            >
        </p>
        <ul class="c6 lst-kix_wlpd2c1tuqbw-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Subject</span
                ><span class="c0">&nbsp;- what the clause is about</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Verb</span
                ><span class="c0"
                    >&nbsp;- action done to the object by the subject</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Object</span
                ><span class="c0">&nbsp;- person, place, thing or idea</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Complement</span
                ><span class="c0"
                    >&nbsp;- extra subject or object which completes the
                    phrase</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Adverbial</span
                ><span class="c0"
                    >&nbsp;- adjunct (additional information)</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Treebanks</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">treebank</span
            ><span class="c0"
                >&nbsp;is a syntactically annotated corpus. In a treebank, every
                sentence in the collection is paired with a corresponding parse
                tree.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 365.33px;
                    height: 174.67px;
                "
                ><img
                    alt="( (VBD was) 
(NP-SBJ ()T That) 
( &#12301; P , P ( &#12301; &#12301; 11 ) 
( &#21269; empty) ( sky) ) 
()P ()N of) 
()P ()N fire) 
()N light) ) ) ) ) 
()C and) 
()P should/MD 
(NP-SBJ The/DT flight/NN ) 
()P arrive/VB 
()P TMP at/IN 
()P eleven/CD a m/RB ) ) "
                    src="assets/natural-language-processing/image42.png"
                    style="
                        width: 365.33px;
                        height: 174.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 366.67px;
                    height: 226.67px;
                "
                ><img
                    alt="NP-SBJ 
That 
JJ 
cold 
JJ 
empty 
sky 
VBD 
was 
ADJP-PRD 
JJ 
full 
IN 
of 
fire 
cc 
and 
light "
                    src="assets/natural-language-processing/image168.png"
                    style="
                        width: 366.67px;
                        height: 226.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A -</span><span class="c1">NONE- </span
            ><span>token represents a </span
            ><span class="c11">long-distance dependency</span
            ><span
                >&nbsp;or a syntactic movement. For example a speech phrase such
                as </span
            ><span class="c1">&quot;Let&#39;s go,&quot; he said</span
            ><span class="c0"
                >&nbsp;separates the speech from the non-spoken phrase.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Treebanks tend to</span
            ><span class="c11">&nbsp;implicitly encode a grammar</span
            ><span class="c0">&nbsp;has rules are repeated over the data.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 441.33px;
                    height: 349.33px;
                "
                ><img
                    alt='Grammar 
s + NPVP. 
s + NPVP 
S -NONE- 
IV" + DTIVN 
IV" + DTIVNS 
IV" + NNCCNN 
IV" + PRP 
NP + -NONE- 
VP VBD ADJP 
V" + VBDS 
VBN PP 
V" + VBS 
VP VB SBAR 
VBP VP 
VBN PP 
SBAR + IN S 
ADJP JJ PP 
IDP&mdash;IN NP 
Lexicon 
PRP we I he 
DT+ the I that those 
JJ cold I empty I full 
NN&mdash;+ sky I fire light I flight 
NNS + assets 
CC and 
IN + of I atl untill on 
CD + eleven 
VB arrive I have I wait 
VBD was said 
VBP + have 
VBN + collected 
MD should I would 
tomorrow '
                    src="assets/natural-language-processing/image131.png"
                    style="
                        width: 441.33px;
                        height: 349.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">lexical head</span
            ><span
                >&nbsp;is the most grammatically important word in a phrase. For
                example, </span
            ><span class="c1">Noun </span><span>is the head of </span
            ><span class="c1">NP</span><span>&nbsp;and </span
            ><span class="c1">Verb </span><span>is the head of </span
            ><span class="c1">VP</span
            ><span class="c0"
                >. Each context-free rule is associated with a lexical head
                which appears in the parse tree.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Combinatory Categorical Grammar</span
            ><span
                >&nbsp;is a generative grammar that allows both left-to-right
                and word-by-word composition. It consists of a set of </span
            ><span class="c22">symbols</span><span>, </span
            ><span class="c22">categories</span><span>, and a set of </span
            ><span class="c22">rules or productions</span
            ><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The notation </span><span class="c1 c22">(X / Y) </span
            ><span>denotes a function that takes </span
            ><span class="c1">Y </span><span>as an argument to the </span
            ><span class="c22">right</span><span>&nbsp;and returns </span
            ><span class="c1">X</span><span>. The notation </span
            ><span class="c1 c22">(X \ Y) </span
            ><span>is the same but takes its argument from the </span
            ><span class="c22">left</span
            ><span class="c0">. The following rules apply:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c1">(X / Y) &nbsp;Y </span
            ><span class="c1">&rarr; </span><span class="c8 c1">X</span>
        </p>
        <p class="c2 c10">
            <span class="c1">Y (X \ Y) </span><span class="c1">&rarr;</span
            ><span class="c8 c1">&nbsp;X</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0"
                >A combination of more complex rules allows derivations to be
                formed:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 290.67px;
                    height: 110.67px;
                "
                ><img
                    alt="the flight 
United diverted 
NP/N N 
NP 
S/NP 
NP\NP "
                    src="assets/natural-language-processing/image48.png"
                    style="
                        width: 290.67px;
                        height: 110.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c20">Constituency Parsing</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Syntactic Parsing:</span
            ><span class="c0"
                >&nbsp;technique by which tokenised POS-tagged text is assigned
                a structure that reveals relationships between tokens governed
                by syntax rules</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Structural ambiguity</span
            ><span class="c0"
                >&nbsp;occurs when a grammar can assign more than one structure
                to a sentence.</span
            >
        </p>
        <ul class="c6 lst-kix_w5472vj8bvm6-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Attachment ambiguity</span
                ><span class="c0"
                    >&nbsp;- a constituent can be attached in multiple
                    places</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Coordination ambiguity</span
                ><span class="c0"
                    >&nbsp;- phrases can be conjoined by conjunctions in
                    multiple ways</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">CKY Parsing</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">CKY algorithm</span
            ><span
                >&nbsp;is a dynamic programming technique operating on
                context-free grammars given in CNF to derive the most probable
                parse trees. It fills in a table of sub-problems to complete the
                original problem as a whole. CKY indexes sentences using </span
            ><span class="c22">fenceposts</span
            ><span>&nbsp;between tokens e.g. </span><span class="c7 c1">0</span
            ><span class="c1">&nbsp;Book </span><span class="c7 c1">1</span
            ><span class="c1">&nbsp;that </span><span class="c7 c1">2</span
            ><span class="c1">&nbsp;flight </span><span class="c7 c1">3</span
            ><span
                >. The parse table is filled in left-right and bottom-up. Each </span
            ><span class="c22">superdiagonal</span
            ><span class="c0"
                >&nbsp;row contains the POS tags for the current word. The other
                cells are filled by analysing phrases within the sentence.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c22">outermost loop</span
            ><span>&nbsp;iterates over the </span
            ><span class="c22">columns</span><span>&nbsp;and the </span
            ><span class="c22">next loop</span
            ><span>&nbsp;iterates over the </span><span class="c22">rows</span
            ><span>. The</span><span class="c22">&nbsp;innermost loop</span
            ><span
                >&nbsp;ranges over cells where a substring may be split. At
                each</span
            ><span class="c22">&nbsp;cell </span><span class="c1">[i,j]</span
            ><span class="c0"
                >&nbsp;the algorithm considers whether the contents of the two
                cells can be combined via the grammar rules. If a rule exists,
                the left non-terminal is entered into the table.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 238.67px;
                    height: 244px;
                "
                ><img
                    alt='dad 
zxdA"s 
u6n0JuJ 
unoN 
&bull;leU!LUON 
dN '
                    src="assets/natural-language-processing/image146.png"
                    style="
                        width: 238.67px;
                        height: 244px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 444px;
                    height: 142.67px;
                "
                ><img
                    alt='function CKY-PARSE(words, grammar) returns table 
forje&mdash;from I to LENGTH(words) do 
for all {A I A &mdash;+ words[jl e grammar} 
table[j&mdash; I table[j &mdash; l, j] U A 
for i "&mdash;from j &mdash; 2 down to O do 
for to j &mdash; do 
for all {A IA BC e grammar andB e andC 
table[ij] table[ij] IJ A '
                    src="assets/natural-language-processing/image208.png"
                    style="
                        width: 444px;
                        height: 142.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Span-Based Neural Consistency Parsing</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Span-Based Neural Consistency Parsing</span
            ><span>&nbsp;or</span><span class="c11">&nbsp;neural CKY </span
            ><span class="c22">disambiguates</span
            ><span class="c0"
                >&nbsp;among the possible parse trees to select the most
                probable tree. It trains a neural classifier to assign scores to
                constituents, then uses CKY to find the best-scoring parse
                tree.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">Neural CKY architecture:</span></p>
        <ul class="c6 lst-kix_n8j76kafp1g-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c11">input</span
                ><span class="c0"
                    >&nbsp;tokens are embedded using a pretrained model such as
                    BERT</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">embeddings</span
                ><span class="c0"
                    >&nbsp;are passed through post-processing layers such as
                    transformers</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>encoder outputs </span><span class="c1">y</span
                ><span class="c7 c1">t </span><span>are used to compute a</span
                ><span class="c3">&nbsp;span score</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">span vector </span><span class="c1">v </span
                ><span>is passed through an </span
                ><span class="c11">MLP classifier</span
                ><span class="c0"
                    >&nbsp;containing two fully connected layers and a
                    ReLU</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c11">dimensionality</span
                ><span class="c0"
                    >&nbsp;of the output is equal to the number of possible
                    non-terminal symbols</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the MLP outputs a</span
                ><span class="c11">&nbsp;score</span
                ><span class="c0">&nbsp;for each symbol</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 350.67px;
                    height: 256px;
                "
                ><img
                    alt="CKY for computing best parse 
Compute score for span 
Represent span 
[STARTI Book 
M LP 
h -hi 
map back to words 
the 
BERT 
to sub words 
flight through Houston [ENDI "
                    src="assets/natural-language-processing/image36.png"
                    style="
                        width: 350.67px;
                        height: 256px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A</span><span class="c11">&nbsp;parse tree </span
            ><span class="c1">T </span><span>is a set of </span
            ><span class="c1">|T| </span><span>spans where the </span
            ><span class="c1">t</span><span class="c21 c1">th </span
            ><span>span starts at position </span><span class="c1">i</span
            ><span class="c7 c1">t </span><span>and ends at position </span
            ><span class="c1">j</span><span class="c7 c1">t </span
            ><span>with label </span><span class="c1">l</span
            ><span class="c7 c1">t</span><span>. Given a span </span
            ><span class="c1">[i,j]</span
            ><span>&nbsp;with non-terminal symbol</span
            ><span class="c1">&nbsp;k</span
            ><span>, the classifier assigns a </span
            ><span class="c11">score </span><span class="c11 c1">s(i,j,l)</span
            ><span class="c0">&nbsp;to the span.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 229.33px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image51.png"
                    style="
                        width: 229.33px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The parser uses these scores to compute the </span
            ><span class="c11">score</span><span>&nbsp;for the whole </span
            ><span class="c11">parse tree </span><span class="c1">s(T) </span
            ><span class="c0"
                >by summing over the scores of its constituent spans.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 186.67px;
                    height: 42.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image135.png"
                    style="
                        width: 186.67px;
                        height: 42.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c22">most probable parse</span
            ><span>&nbsp;tree is the tree with the </span
            ><span class="c11">maximum</span
            ><span class="c0"
                >&nbsp;score. The algorithm greedily selects the highest scoring
                label for each span. This has a 95% accuracy in finding valid
                trees.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 165.33px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image144.png"
                    style="
                        width: 165.33px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">variant CKY</span
            ><span class="c11">&nbsp;algorithm</span
            ><span>&nbsp;finds the full parse tree. It defines </span
            ><span class="c1">S</span><span class="c7 c1">best</span
            ><span class="c1">(i,j) </span
            ><span>as the score of the best subtree spanning </span
            ><span class="c1">(i,j)</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">For spans of length 1:</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 246.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image112.png"
                    style="
                        width: 246.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>For other spans </span><span class="c1">(i,j)</span
            ><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 377.33px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image155.png"
                    style="
                        width: 377.33px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c20">Dependency Parsing</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Dependency Grammars &amp; Relations</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Dependency grammars</span
            ><span class="c0"
                >&nbsp;are defined by a set of directed binary grammatical
                relations that hold among words in a sentence. They are useful
                for languages that are morphologically rich and have free word
                order. They can also be used in conference resolution, question
                answering, and information extraction.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Relations are represented by labelled directed</span
            ><span class="c11">&nbsp;arcs</span><span>&nbsp;from </span
            ><span class="c11">heads</span><span>&nbsp;to </span
            ><span class="c11">dependents</span><span>. This is a </span
            ><span class="c22">typed dependency structure</span
            ><span
                >&nbsp;as labels are drawn from a fixed set of relations.
                The</span
            ><span class="c11">&nbsp;root</span
            ><span class="c0">&nbsp;node marks the head of the tree.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 278.67px;
                    height: 100px;
                "
                ><img
                    alt="nsubj 
dow 
n mod 
nmod 
I prefer the morning flight through Denver "
                    src="assets/natural-language-processing/image55.png"
                    style="
                        width: 278.67px;
                        height: 100px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Dependency relations</span
            ><span
                >&nbsp;define labels for head and dependent word groups. </span
            ><span class="c22">Clausal</span
            ><span
                >&nbsp;relations involve a predicate (often a verb) and
                modifiers attached to it. </span
            ><span class="c22">Nominal </span
            ><span class="c0"
                >relations involve modifiers for nominal (often noun)
                words.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 404px;
                    height: 236px;
                "
                ><img
                    alt="Clausal Argument Relations 
NSUBJ 
DOBJ 
[OBJ 
CCOMP 
XCOMP 
Nominal Modifier Relations 
NMOD 
AMOD 
NUMMOD 
APPOS 
DET 
CASE 
Other Notable Relations 
CONJ 
cc 
Description 
Nominal subject 
Direct object 
Indirect object 
Clausal complement 
Open clausal complement 
Description 
Nominal modifier 
Adjectival modifier 
Numeric modifier 
Appositional modifier 
Determiner 
Prepositions, postpositions and other case markers 
Description 
Conjunct 
Coordinating conjunction "
                    src="assets/natural-language-processing/image216.png"
                    style="
                        width: 404px;
                        height: 236px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">dependency graph</span
            ><span>&nbsp;is a </span><span class="c22">directed graph </span
            ><span class="c1 c22">G = (V, A) </span><span>where </span
            ><span class="c1 c22">V </span
            ><span>is the set of vertices and </span
            ><span class="c1 c22">A </span
            ><span
                >is the set of ordered pairs of vertices, called arcs. Arcs
                capture head-dependent and grammatical relationships between
                vertices. A</span
            ><span class="c11">&nbsp;dependency tree</span
            ><span class="c0"
                >&nbsp;is a directed graph satisfying the following
                constraints:</span
            >
        </p>
        <ol class="c6 lst-kix_380qckwdf9e9-0 start" start="1">
            <li class="c2 c18 li-bullet-0">
                <span>there is a single </span><span class="c22">root</span
                ><span class="c0">&nbsp;node that has no incoming arcs</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>each </span><span class="c22">vertex</span
                ><span class="c0"
                    >&nbsp;except the root has one incoming arc</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>there exists a unique </span><span class="c22">path</span
                ><span>&nbsp;from the root to each vertex in </span
                ><span class="c8 c1">V</span>
            </li>
        </ol>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>An arc from a head to a dependent is </span
            ><span class="c11">projective</span
            ><span class="c0"
                >&nbsp;if there is a path from the head to each word between the
                head and dependent. A dependency tree is projective if all the
                arcs inside it are also projective.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Transition-Based Dependency Parsing</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Shift-reduce</span
            ><span>&nbsp;dependency parsing uses a </span
            ><span class="c22"
                >context-free grammar, a stack, and a list of input tokens</span
            ><span
                >. Input tokens are successively shifted onto the stack and the
                top two elements are matched against the grammar rules. When a
                match is found, a </span
            ><span class="c22">dependency relation</span
            ><span class="c0">&nbsp;is added to the parse tree.</span>
        </p>
        <ul class="c6 lst-kix_ldlzuambsbij-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0"
                    >training data is provided as pairs of the form
                    (configuration, transition)</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the configuration consists of a</span
                ><span class="c22">&nbsp;stack</span><span>, </span
                ><span class="c22">input tokens</span><span>, and </span
                ><span class="c28 c22">set of relations</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c0">transitions represent actions and labels</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_ldlzuambsbij-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c11">Shift:</span
                ><span class="c0"
                    >&nbsp;remove the word from the input and push it onto the
                    stack</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c11">Left Arc:</span
                ><span>&nbsp;assert a head-dependent relation </span
                ><span class="c1">A </span><span class="c1">&rarr;</span
                ><span class="c1">&nbsp;B</span><span>&nbsp;where</span
                ><span class="c1">&nbsp;A</span
                ><span>&nbsp;is the head and </span><span class="c1">B </span
                ><span class="c0">is the dependent</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c11">Right Arc:</span
                ><span>&nbsp;assert a head-dependent relation </span
                ><span class="c1">A </span><span>&larr;</span
                ><span class="c1">&nbsp;B</span><span>&nbsp;where</span
                ><span class="c1">&nbsp;B</span
                ><span>&nbsp;is the head and </span><span class="c1">A </span
                ><span class="c0">is the dependent</span>
            </li>
        </ul>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 306.67px;
                    height: 184px;
                "
                ><img
                    alt="Input buffer 
Oracle 
Stack "
                    src="assets/natural-language-processing/image52.png"
                    style="
                        width: 306.67px;
                        height: 184px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >The algorithm for shift-reduce dependency parsing reduces the
                stack by creating dependency relations, until only the root node
                is left. The </span
            ><span class="c11">oracle</span
            ><span class="c0"
                >&nbsp;analyses the top two items in the stack to determine if
                the parser should create a new relation or shift a new token
                onto the stack. The oracle has access to the following
                information:</span
            >
        </p>
        <ul class="c6 lst-kix_rw5gyt5sdl95-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>the current configuration with stack </span
                ><span class="c1">S </span><span>and set of relations </span
                ><span class="c1">R</span><span class="c46 c7 c1">c</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the training data with vertices </span
                ><span class="c1">V</span
                ><span>&nbsp;and set of relations </span
                ><span class="c1">R</span><span class="c46 c7 c1">p</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Given this information, the oracle selects a </span
            ><span class="c11">left arc</span
            ><span
                >&nbsp;if it produces a correct head-dependent relation, a</span
            ><span class="c11">&nbsp;right arc</span
            ><span
                >&nbsp;if it produces a correct head-dependent relation and all
                the dependents at the top of the stack have already been
                assigned, or a </span
            ><span class="c11">shift</span
            ><span class="c0">&nbsp;otherwise.</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 229.33px;
                    height: 18.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image191.png"
                    style="
                        width: 229.33px;
                        height: 18.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 504px;
                    height: 18.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image104.png"
                    style="
                        width: 504px;
                        height: 18.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 164px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image153.png"
                    style="
                        width: 164px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 448px;
                    height: 126.67px;
                "
                ><img
                    alt="function DEPENDENCYPARSE(words) returns dependency tree 
state f&mdash; {[root], [words], [ ] } ; initial configuration 
while state not final 
; choose a transition operator to apply 
t 4&mdash; ORACLE(state) 
state state) ; apply it, creating a new state 
return state "
                    src="assets/natural-language-processing/image78.png"
                    style="
                        width: 448px;
                        height: 126.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Features</span
            ><span
                >&nbsp;such as word forms, lemmas, and POS tags are used to
                train the oracle. Feature templates are abstract sets of
                predefined feature types to use during training. Features are
                denoted as</span
            ><span class="c1">&nbsp;location.property</span
            ><span>&nbsp;where </span><span class="c1">s </span
            ><span>is the stack, </span><span class="c1">b </span
            ><span>is the buffer, and </span><span class="c1">r </span
            ><span>is the set of relations. </span><span class="c1">w </span
            ><span>is a word form, </span><span class="c1">l </span
            ><span>is a lemma, </span><span class="c1">t </span
            ><span>is a POS tag, and </span><span class="c1">op </span
            ><span class="c0">is the transition operator.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 306.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image180.png"
                    style="
                        width: 306.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 258.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image99.png"
                    style="
                        width: 258.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Graph-Based Dependency Parsing</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Graph-based</span
            ><span
                >&nbsp;dependency parsing searches through a space of possible
                parse trees to maximise their score. The search space is
                represented as a directed graph which employs graph theory to
                search for optimal solutions. Given a sentence </span
            ><span class="c1">S</span
            ><span
                >, the parser searches for the best tree in the set of all
                possible trees </span
            ><span class="c1">G</span><span class="c7 c1">S</span
            ><span class="c0"
                >. Graph-based methods are useful as they can produce
                non-projective trees and have higher accuracy with long-distance
                dependencies.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 220px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image210.png"
                    style="
                        width: 220px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 214.67px;
                    height: 40px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image94.png"
                    style="
                        width: 214.67px;
                        height: 40px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Given an input sentence, the algorithm constructs a</span
            ><span class="c11"
                >&nbsp;fully connected weighted directed graph</span
            ><span>&nbsp;where the </span><span class="c22">vertices</span
            ><span>&nbsp;are the input words and the</span
            ><span class="c22">&nbsp;edges</span
            ><span
                >&nbsp;represent all possible head-dependent assignments. A </span
            ><span class="c22">root</span
            ><span
                >&nbsp;node is included with outgoing edges to all other
                vertices. The </span
            ><span class="c22">weight</span
            ><span class="c0">&nbsp;for each edge represents its score.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 360px;
                    height: 197.33px;
                "
                ><img
                    alt="root 
12 
Book 
that 
flight "
                    src="assets/natural-language-processing/image141.png"
                    style="
                        width: 360px;
                        height: 197.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>For each </span><span class="c11">vertex</span
            ><span>&nbsp;in the graph, the algorithm selects the incoming </span
            ><span class="c11">edge</span><span>&nbsp;with the </span
            ><span class="c11">highest score</span
            ><span>. If the resulting </span><span class="c22">subgraph </span
            ><span class="c1 c22">T = (V, F) </span><span>is a</span
            ><span class="c11">&nbsp;spanning tree</span
            ><span
                >&nbsp;(has no cycles and each vertex except the root has one
                incoming edge) the tree is returned. Otherwise, the selection
                process continues. During</span
            ><span class="c11">&nbsp;cleanup</span
            ><span class="c0"
                >, all weights are adjusted by subtracting the score of the
                maximum edge from the score of all other edges entering that
                vertex. If any edges are equal to zero, the cycle is collapsed
                into a single node.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 414.67px;
                    height: 306.67px;
                "
                ><img
                    alt='function MAXSPANNINGTREE(G=(V,E), root, score) returns spanning tree 
score &#39; 4_ [l 
for each v e V do 
bestInEdge argmax 
E score[e] 
F "&mdash;F best1nEdge 
for each e=(u,v) e E do 
score &#39;[e] score[e] &mdash; score[bestInEdge] 
if T=(V,F) is a spanning tree then return it 
Cea cycle in F 
T&#39; root, score&#39;) 
return T 
function CONTRACT(G, C) returns contracted graph 
function EXPAN D(T, C) returns expanded graph '
                    src="assets/natural-language-processing/image160.png"
                    style="
                        width: 414.67px;
                        height: 306.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >The score for each edge can be reduced to a weighted sum
                of</span
            ><span class="c11">&nbsp;features</span
            ><span class="c0"
                >. Common features include word forms, lemmas, POS tags, NER
                tags, embeddings, relations, dependencies, and dependency
                distances.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 230.67px;
                    height: 40px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image98.png"
                    style="
                        width: 230.67px;
                        height: 40px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c25 c10"><span class="c0"></span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 564px;
                    height: 62.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image41.png"
                    style="
                        width: 564px;
                        height: 62.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <p class="c2 c55"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c20">Word Senses and WordNet</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Word Senses</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Words are </span><span class="c11">ambiguous</span
            ><span>&nbsp;and can have different meanings. A </span
            ><span class="c11">word sense</span
            ><span class="c0"
                >&nbsp;is a representation of one aspect of the meaning of a
                word.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 441.33px;
                    height: 182.67px;
                "
                ><img
                    alt='The noun "bass" has 8 senses in WordNet. 
I. bassi - (the lowest part of the musical range) 
2. bass2, bass partl - (the lowest part in polyphonic music) 
. base, bassol - (an adult male singer with the lowest voice) 
sea bassi, bass - (the lean flesh of a saltwater fish of the family Serranidae) 
4. 
5. freshwater bassi , bass - (any of various North American freshwater fish with 
lean flesh (especially of the genus Micropterus)) 
6. bass6, bass voicel , bass02 - (the lowest adult male singing voice) 
7. bass&#39; - (the member with the lowest range of a family of musical instruments) 
. bass* - (nontechnical name for any of numerous edible marine and 
freshwater spiny-finned fishes) '
                    src="assets/natural-language-processing/image115.png"
                    style="
                        width: 441.33px;
                        height: 182.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c0">Relations between senses:</span></p>
        <ul class="c6 lst-kix_ybor7lhio2m1-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c0">Synonymity &amp; Antonymity</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_ybor7lhio2m1-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c11">Synonym:</span
                ><span class="c0"
                    >&nbsp;word which is identical to another</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c11">Antonym:</span
                ><span class="c0">&nbsp;word which is opposite to another</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_ybor7lhio2m1-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c0">Hyponymity &amp; Hypernymity</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_ybor7lhio2m1-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c11">Hyponym:</span
                ><span class="c0"
                    >&nbsp;word which is a subclass of another</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c11">Hypernym:</span
                ><span class="c0"
                    >&nbsp;word which is a superclass of another</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_ybor7lhio2m1-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c0">Meronymity &amp; Holonymity</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_ybor7lhio2m1-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c11">Meronym:</span
                ><span class="c0">&nbsp;word which is part of another</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c11">Holonym:</span
                ><span class="c0">&nbsp;word which is built from others</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 510.67px;
                    height: 140px;
                "
                ><img
                    alt="Relation 
Hypernym 
Hyponym 
Instance Hypernym 
Instance Hyponym 
Part Meronym 
Part Holonym 
Antonym 
Derivation 
Also Called 
Superordinate 
Subordinate 
Instance 
Has-Instance 
Has-Part 
Pan-Of 
Definition 
From concepts to superordinates 
From concepts to subtypes 
From instances to their concepts 
From concepts to their instances 
From wholes to parts 
From parts to wholes 
Semantic opposition between lemmas 
Lemmas w/same morphological root 
Example 
breakfastl &mdash;i meall 
meall &mdash;i lunch&#39; 
Austen&#39; &mdash;s authorl 
composerl &mdash;+ Bachl 
table2 &mdash;i leg3 
course &mdash;i meal I 
leaderl follower 
destruction I destroy "
                    src="assets/natural-language-processing/image126.png"
                    style="
                        width: 510.67px;
                        height: 140px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">WordNet</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">WordNet</span><span>&nbsp;is a</span
            ><span class="c22">&nbsp;lexical database</span
            ><span
                >&nbsp;containing semantic relations between words. Each word is
                described by a </span
            ><span class="c22">gloss</span
            ><span
                >&nbsp;(dictionary-style definition), list of synonyms for the
                sense, and usage examples. The set of synonyms for a sense is
                called a </span
            ><span class="c22">synset</span
            ><span>. The lexographic category for a synset is called a </span
            ><span class="c22">supersense</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 498.67px;
                    height: 150.67px;
                "
                ><img
                    alt="Category 
ACT 
ANIMAL 
ARTIFACT 
ATTRIBUTE 
BODY 
COGNITION 
COMMUNICATION 
FEELING 
FOOD 
Example 
service 
dog 
car 
quality 
hair 
way 
rev/ ew 
dis comfort 
food 
Category 
GROUP 
LOCATION 
MOTIVE 
NATURAL EVENT 
NATURAL OBJECT 
OTHER 
PERSON 
PHENOMENON 
Example 
plac e 
reas on 
exp err ence 
wer 
stuff 
people 
ms u It 
Category 
PLANT 
POSSESSION 
PROCESS 
QUANTITY 
RELATION 
SHAPE 
STATE 
SUBSTANCE 
TIME 
Example 
tree 
pnce 
process 
amount 
portion 
square 
pam 
oil "
                    src="assets/natural-language-processing/image185.png"
                    style="
                        width: 498.67px;
                        height: 150.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Word Sense Disambiguation (WSD)</span
            ><span>&nbsp;is a </span><span class="c22">classification</span
            ><span
                >&nbsp;task which predicts word senses in sentences. A WSD
                algorithm is typically trained using a </span
            ><span class="c22">semantic concordance</span
            ><span class="c0"
                >, a corpus in which each word is labelled with its word sense.
                Given each noun, verb, adverb, or adjective in the test set, the
                algorithm will predict the correct sense from the possible
                senses provided in WordNet.</span
            >
        </p>
        <ul class="c6 lst-kix_bt8umifpidoj-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c22">X</span
                ><span class="c0"
                    >&nbsp;= sentence = sequence of labelled words</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c22">Y</span
                ><span class="c0"
                    >&nbsp;= word senses = sequence of sense tags</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 494.67px;
                    height: 254.67px;
                "
                ><img
                    alt="electrici : 
using 
electricity 
electric2: 
tense 
electrics: 
thrilling 
O 
electric 
Y2 
guitar&#39; 
O 
guitar 
and 
Y3 
bassi : 
low range 
bass4: 
sea fish 
bass7: 
instrument 
bass 
playerl: 
In game 
player2: 
musician 
players: 
actor 
player 
stand&#39;: 
upright 
stand5&bull; 
stand*0.. 
put 
upright 
stand 
000 
off to one 
Y6 
sidel: 
relative 
region 
sides: 
of body 
sideli: 
slope 
side "
                    src="assets/natural-language-processing/image173.png"
                    style="
                        width: 494.67px;
                        height: 254.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >WSD algorithms are usually evaluated using the F1 score. A
                strong </span
            ><span class="c11">baseline</span
            ><span>&nbsp;involves selecting the </span
            ><span class="c22">most frequent sense</span
            ><span>&nbsp;for each word. Another baseline called</span
            ><span class="c22">&nbsp;one-sense-per-discourse</span
            ><span class="c0"
                >&nbsp;involves using the same sense each time a word appears in
                text.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The most accurate form of WSD uses the</span
            ><span class="c11">&nbsp;nearest neighbour</span
            ><span
                >&nbsp;algorithm. During training, it passes each sentence into
                a labelled dataset to build a </span
            ><span class="c22">contextual embedding</span
            ><span>&nbsp;for each token. For each token </span
            ><span class="c1">c</span><span class="c7 c1">i </span
            ><span>of each sense </span><span class="c1">c</span
            ><span>, it averages the embeddings to produce a </span
            ><span class="c22">sense embedding </span><span class="c1">v</span
            ><span class="c7 c1">s</span><span>&nbsp;for </span
            ><span class="c1">c</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 134.67px;
                    height: 40px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image212.png"
                    style="
                        width: 134.67px;
                        height: 40px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The algorithm computes an </span
            ><span class="c22">embedding </span><span class="c1">t </span
            ><span class="c0"
                >for each word in the test set, then finds its nearest neighbour
                (cosine distance) from the training set.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 482.67px;
                    height: 208px;
                "
                ><img
                    alt="O find4 
I 
find$ 
find? 
O 
Cfound th C&#39; ar 
ENCODER 
found the jar 
em 
empty "
                    src="assets/natural-language-processing/image44.png"
                    style="
                        width: 482.67px;
                        height: 208px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>For</span
            ><span class="c11">&nbsp;out-of-vocabulary (OOV)</span
            ><span
                >&nbsp;words, the algorithm can infer the missing embeddings
                using </span
            ><span class="c22">supersenses</span
            ><span>. It calculates the </span
            ><span class="c22">sense embedding</span
            ><span
                >&nbsp;for any higher-level word in the WordNet taxonomy by
                averaging the senses of its children. For each missing sense in
                WordNet </span
            ><img src="assets/natural-language-processing/image19.png" /><span>, let the </span
            ><span class="c22">synset embeddings</span><span>&nbsp;be </span
            ><img src="assets/natural-language-processing/image20.png" /><span>, the </span
            ><span class="c22">hypernym synset embeddings</span
            ><span>&nbsp;be </span><img src="assets/natural-language-processing/image21.png" /><span
                >, and the </span
            ><span class="c22">supersense synset embeddings</span
            ><span>&nbsp;be </span><img src="assets/natural-language-processing/image22.png" /><span
                class="c0"
                >.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The embedding for a</span><span class="c11">&nbsp;synset</span
            ><span class="c0"
                >&nbsp;is the average of its sense embeddings:</span
            >
        </p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 272px;
                    height: 40px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image242.png"
                    style="
                        width: 272px;
                        height: 40px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The embedding for a </span><span class="c11">hypernym</span
            ><span class="c0"
                >&nbsp;is the average of its synset embeddings:</span
            >
        </p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 304px;
                    height: 40px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image72.png"
                    style="
                        width: 304px;
                        height: 40px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The embedding for a </span><span class="c11">supersense</span
            ><span class="c0"
                >&nbsp;is the average of its synset embeddings:</span
            >
        </p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 298.67px;
                    height: 40px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image68.png"
                    style="
                        width: 298.67px;
                        height: 40px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title="" /></span
            ><span class="c0">&nbsp;</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <h1 class="c74" id="h.l3eij34ey589">
            <span class="c34 c29">Information Extraction</span>
        </h1>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Information extraction:</span
            ><span class="c0"
                >&nbsp;process of converting unstructured text into structured
                data, such as a relational database or set of extracted
                tuples</span
            >
        </p>
        <p class="c2">
            <span class="c11">Relation extraction:</span
            ><span class="c0"
                >&nbsp;process of finding semantic relations among text
                entities, such as parent-child, part-whole or geospatial
                relations</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Relation Extraction</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">relation</span
            ><span
                >&nbsp;consists of a set of ordered tuples over named entities. </span
            ><span class="c11">ACE</span
            ><span class="c0">&nbsp;contains 17 relations and 7 entities:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 441.33px;
                    height: 201.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image175.png"
                    style="
                        width: 441.33px;
                        height: 201.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 442.67px;
                    height: 78.67px;
                "
                ><img
                    alt="Relations 
Physical-Located 
Part-Whole-Subsidiary 
Person-Social-Family 
Org-AFF-Founder 
Ty pes 
PER-GPE 
ORG-ORG 
PER-PER 
PER.ORG 
Examples 
He was in Tennessee 
XYZ, the parent company of ABC 
Yoko&#39;s husband John 
Steve Jobs, co-founder of Apple.. "
                    src="assets/natural-language-processing/image148.png"
                    style="
                        width: 442.67px;
                        height: 78.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Wikipedia</span
            ><span
                >&nbsp;provides a database of relations contained in infoboxes.
                Relations appear as</span
            ><span class="c11">&nbsp;RDF </span
            ><span>(Resource Description Framework) triples appearing as </span
            ><span class="c11">subject-predicate-object</span
            ><span>&nbsp;expressions. </span><span class="c11">DBpedia</span
            ><span class="c0"
                >&nbsp;is an ontology derived from Wikipedia containing RDF
                triples.</span
            >
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <a id="t.e8310349ea7a68d415b130f211576862f0af7f4f"></a><a id="t.11"></a>
        <table class="c39">
            <tbody>
                <tr class="c60">
                    <td class="c62" colspan="1" rowspan="1">
                        <p class="c14"><span class="c28 c22">Subject</span></p>
                    </td>
                    <td class="c5" colspan="1" rowspan="1">
                        <p class="c14">
                            <span class="c28 c22">Predicate</span>
                        </p>
                    </td>
                    <td class="c78" colspan="1" rowspan="1">
                        <p class="c14"><span class="c28 c22">Object</span></p>
                    </td>
                </tr>
                <tr class="c60">
                    <td class="c62" colspan="1" rowspan="1">
                        <p class="c14">
                            <span class="c0">Golden Gate Park</span>
                        </p>
                    </td>
                    <td class="c5" colspan="1" rowspan="1">
                        <p class="c14"><span class="c0">location</span></p>
                    </td>
                    <td class="c78" colspan="1" rowspan="1">
                        <p class="c14"><span class="c0">San Francisco</span></p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">WordNet</span
            ><span
                >&nbsp;offers ontological relations between words and concepts
                such as hypernyms (is-a) and hyponyms (instance-of). The </span
            ><span class="c11">TACRED</span
            ><span class="c0"
                >&nbsp;dataset contains relations triples associated with people
                and organisations, represented as labelled sentences.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Hearst patterns</span
            ><span class="c0"
                >&nbsp;are hand-crafted lexico-syntactic patterns derived using
                regular expressions. These have high precision but low
                recall.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 550.67px;
                    height: 84px;
                "
                ><img
                    alt="NP {, NP}* {,} (andlor) other NPH 
NPH such as {NP,}* {(orland)} NP 
such NPH as {NP,}* {(orland)} NP 
NPH {,} including {NP,}* {(orland)} NP 
NPH {,} especially {NP}* {(orland)} NP 
temples, treasuries, and other important civic buildings 
red algae such as Gelidium 
such authors as Herrick, Goldsmith, and Shakespeare 
common-law countries, including Canada and England 
European countries, especially France, England, and Spain "
                    src="assets/natural-language-processing/image31.png"
                    style="
                        width: 550.67px;
                        height: 84px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Supervised Learning</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Supervised learning</span
            ><span>&nbsp;approaches are </span><span class="c22">trained</span
            ><span
                >&nbsp;on an annotated corpus containing relations and entities,
                and the annotated texts are used to train </span
            ><span class="c22">classifiers</span
            ><span
                >&nbsp;to annotate unseen text. The supervised learning
                algorithm finds </span
            ><span class="c22">pairs of named entities</span
            ><span>&nbsp;in a sentence and</span
            ><span class="c22">&nbsp;classifies</span
            ><span class="c0">&nbsp;each pair.</span>
        </p>
        <ul class="c6 lst-kix_5d4bennhnt6z-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c22">X</span
                ><span class="c0"
                    >&nbsp;= feature set for entity pair (embedding/one-hot
                    vector)</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c22">Y</span
                ><span>&nbsp;= prediction for the relation between </span
                ><span class="c1">e1 </span><span>and </span
                ><span class="c8 c1">e2</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 474.67px;
                    height: 134.67px;
                "
                ><img
                    alt="function FINDRELATIONS(words) returns relations 
relations 4&mdash; nil 
entities 4&mdash; FINDENTITIES(words) 
forall entity pairs (el , e2) in entities do 
if , e2) 
relations -e relations+CLASSIFYRELATION(e/ , e2) "
                    src="assets/natural-language-processing/image164.png"
                    style="
                        width: 474.67px;
                        height: 134.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Feature-based classifiers</span
            ><span>&nbsp;typically use</span
            ><span class="c11">&nbsp;word features</span
            ><span>&nbsp;such as </span><span class="c22">embeddings</span
            ><span>, </span><span class="c22">one-hot vectors</span
            ><span>, </span><span class="c22">n-grams, lemmas</span
            ><span>, </span><span class="c11">named entity features</span
            ><span>&nbsp;such as </span><span class="c22">NER tags</span
            ><span>&nbsp;and</span><span class="c22">&nbsp;entity levels</span
            ><span>, and </span><span class="c11">syntactical features</span
            ><span>&nbsp;such as </span><span class="c22">dependencies</span
            ><span>&nbsp;or </span><span class="c22">dependency distances</span
            ><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Neural supervised relation classifiers</span
            ><span
                >&nbsp;perform supervised classification on words in a sentence.
                A typical </span
            ><span class="c22">transformer-encoder</span
            ><span
                >&nbsp;algorithm uses a pretrained encoder such as BERT to </span
            ><span class="c22">encode</span
            ><span>&nbsp;sentences with labels. The encoder replaces </span
            ><span class="c22">subject</span><span>&nbsp;and </span
            ><span class="c22">object</span
            ><span class="c0"
                >&nbsp;entities with NER tags to avoid overfitting. Neural
                classifiers perform well if the test set is similar to the
                training set. However, supervised models do not generalise well
                to different types of text.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 290.67px;
                    height: 112px;
                "
                ><img
                    alt="p(relation ISUBJ,OBJ) 
Linear 
Classifier 
ENCODER BER 
[CLS] [SUBJ_PERSON] was born in [OBJ_LOC] 
Michigan "
                    src="assets/natural-language-processing/image231.png"
                    style="
                        width: 290.67px;
                        height: 112px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Semi-Supervised Learning</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Bootstrapping</span
            ><span>&nbsp;uses a small set of high quality </span
            ><span class="c22">seed tuples</span><span>&nbsp;of the form </span
            ><span class="c1">(r, e1, e2)</span
            ><span
                >&nbsp;to find sentences containing both entities. It extracts
                and</span
            ><span class="c22">&nbsp;generalises</span
            ><span class="c0"
                >&nbsp;the context around the entities to learn new
                patterns.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 473.33px;
                    height: 182.67px;
                "
                ><img
                    alt="function BOOTSTRAP(Re/ation R) returns new relation tuples 
tuples Gather a set of seed tuples that have relation R 
iterate 
sentences e&mdash; find sentences that contain entities in tuples 
patterns generalize the context between and around entities in sentences 
novpairs 4&mdash; use patterns to grep for more tuples 
novpairs e&mdash; newpairs with high confidence 
tuples e tuples + newpairs 
return tuples "
                    src="assets/natural-language-processing/image69.png"
                    style="
                        width: 473.33px;
                        height: 182.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The bootstrapping algorithm also assigns </span
            ><span class="c22">confidence values</span
            ><span>&nbsp;to new tuples to avoid </span
            ><span class="c11">semantic drift</span
            ><span class="c0"
                >. In semantic drift, an incorrect pattern leads to the
                introduction and propagation of incorrect tuples, causing the
                meaning of the relation to &#39;drift&#39;.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Distant Supervision</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Distant supervision</span
            ><span class="c0"
                >&nbsp;combines the advantages of semi-supervised learning and
                bootstrapping. As distant supervision doesn&#39;t use a labelled
                training corpus, it isn&#39;t affected by semantic drift.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ul class="c6 lst-kix_4o2h3efmbi6l-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c28 c22">Training</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_4o2h3efmbi6l-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >uses a large database containing many seed samples to
                    create noisy pattern features</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >combines features in a supervised classifier</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_4o2h3efmbi6l-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c28 c22">Relation Extraction</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_4o2h3efmbi6l-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >finds relations from databases such as DBpedia and
                    Freebase</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >a NER tagger is run on some text to extract sentences with
                    matching relations</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span>training instances are extracted in the form </span
                ><span class="c8 c1">(r, e1, e2)</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_4o2h3efmbi6l-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c28 c22">Prediction</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_4o2h3efmbi6l-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >applies feature-based neural classification with a rich set
                    of features</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 474.67px;
                    height: 166.67px;
                "
                ><img
                    alt="function DISTANT SUPERVISION(Database D, Text T) returns relation classifier C 
foreach relation R 
foreach tuple (el,e2) of entities with relation R in D 
sentences e Sentences in T that contain el and e2 
features in sentences 
observations observations + new training tuple (el, e2, f, R) 
C e&mdash; Train supervised classifier on observations 
return C "
                    src="assets/natural-language-processing/image71.png"
                    style="
                        width: 474.67px;
                        height: 166.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Unsupervised Learning</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Unsupervised relation extraction</span
            ><span
                >&nbsp;finds relations without using labelled training data.
                This task is often called </span
            ><span class="c11">Open Information Extraction (Open IE)</span
            ><span>. The </span><span class="c11">ReVerb</span
            ><span class="c0"
                >&nbsp;algorithm extracts relations from sentences in 4
                steps:</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <ol class="c6 lst-kix_o694btkswhy5-0 start" start="1">
            <li class="c2 c18 li-bullet-0">
                <span>Perform</span
                ><span class="c22">&nbsp;POS and NER tagging</span
                ><span>&nbsp;on a sentence </span><span class="c8 c1">s</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>For each verb in </span><span class="c1">s</span
                ><span>, find the </span><span class="c22">longest phrase </span
                ><span class="c1">w</span
                ><span class="c0"
                    >&nbsp;that starts with a verb and satisfies the
                    constraints</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>For each phrase </span><span class="c1">w</span
                ><span>, find the </span
                ><span class="c22">nearest noun phrase </span
                ><span class="c1">x </span
                ><span>to the left which is not a relative pronoun, </span
                ><span class="c1">wh</span
                ><span>-word, or existential word. Find the </span
                ><span class="c22">nearest noun phrase </span
                ><span class="c1">y </span
                ><span class="c0">to the right of the verb</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>Assign and return a</span
                ><span class="c22">&nbsp;confidence value </span
                ><span class="c1">c </span><span>to the relation </span
                ><span class="c1">r = (x,w,y) </span><span>using a </span
                ><span class="c28 c22">confidence classifier</span>
            </li>
        </ol>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                >A relation is only accepted if it meets syntactic and lexical </span
            ><span class="c11">constraints</span
            ><span
                >. These ensure that it is a verb-initial sequence that may also
                include nouns. The constraints are based on a </span
            ><span class="c11">dictionary </span><span class="c11 c1">D </span
            ><span class="c0"
                >that is used to prune irrelevant strings. This eliminates
                relations that do not occur often enough to form distinct
                examples.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10"><span class="c28 c22">V | VP | VW*P</span></p>
        <p class="c2 c10">
            <span class="c0">V = verb particle? + adverb?</span>
        </p>
        <p class="c2 c10">
            <span class="c0"
                >W = noun | &nbsp;pronoun | adjective | adverb |
                determiner</span
            >
        </p>
        <p class="c2 c10">
            <span class="c0">P = prep | particle | marker</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Temporal Expression Extraction</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Temporal expression extraction</span
            ><span>&nbsp;finds expressions containing dates and times. </span
            ><span class="c22">Relative expressions</span
            ><span
                >&nbsp;map to particular times through some reference point. </span
            ><span class="c22">Durations</span
            ><span class="c0"
                >&nbsp;denote spans of time at varying levels of
                granularity.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 442.67px;
                    height: 81.33px;
                "
                ><img
                    alt="Absolute 
April 24, 1916 
The summer of &#39;77 
10:15 AM 
The 3rd quarter of 2006 
Relative 
yesterday 
next semester 
two weeks from yesterday 
last quarter 
Durations 
four hours 
three weeks 
six days 
the last three quarters "
                    src="assets/natural-language-processing/image194.png"
                    style="
                        width: 442.67px;
                        height: 81.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Rule-based systems can extract these expressions using</span
            ><span class="c11">&nbsp;lexical triggers</span
            ><span>&nbsp;such as nouns, verbs, and adjectives. </span
            ><span class="c22">Sequence labelling</span
            ><span
                >&nbsp;approaches use IOB tagging to label named entities in
                sentences.</span
            ><span class="c11">&nbsp;Features</span
            ><span class="c0"
                >&nbsp;are extracted from tokens and their contexts, and a
                sequence labelling model is trained on the data. Typical
                features include word shape, POS tags, NER tags, context, and
                lexical triggers.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 441.33px;
                    height: 113.33px;
                "
                ><img
                    alt="Feature 
Token 
Tokens in window 
Shape 
POS 
Chunk tags 
Lexical triggers 
Explanation 
The target token to be labeled 
Bag of tokens in the window around a target 
Character shape features 
Parts of speech of target and window words 
Base phrase chunk tag for target and words in a window 
Presence in a list of temporal terms "
                    src="assets/natural-language-processing/image122.png"
                    style="
                        width: 441.33px;
                        height: 113.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Temporal normalisation</span
            ><span
                >&nbsp;is the process of mapping a temporal expression to a
                specific point in time or duration. Most approaches for this
                task are</span
            ><span class="c22">&nbsp;rule-based</span
            ><span>&nbsp;and match regular expressions such as </span
            ><span class="c1">YYYY-MM-DD</span><span>&nbsp;and </span
            ><span class="c1">HH:MM:SS</span><span>. The</span
            ><span class="c11">&nbsp;temporal anchor</span
            ><span class="c0"
                >&nbsp;is the central point in time to which all other temporal
                expressions are compared.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Event extraction</span
            ><span>&nbsp;is used to identify events in texts. An</span
            ><span class="c22">&nbsp;event</span
            ><span
                >&nbsp;is an expression denoted an event or state that can be
                assigned to a temporal expression. Events are classified as </span
            ><span class="c22">actions</span><span>, </span
            ><span class="c22">states</span><span>, </span
            ><span class="c22">reporting events</span
            ><span>&nbsp;(say, tell, explain), </span
            ><span class="c22">perception events</span
            ><span
                >&nbsp;(see, observe), and more. Event extraction is usually
                performed using supervised learning. </span
            ><span class="c11">Sequence labelling</span
            ><span>&nbsp;models can use</span
            ><span class="c22">&nbsp;IOB tagging</span
            ><span>&nbsp;to classify event classes and attributes. </span
            ><span class="c22">Feature-based models</span
            ><span class="c0"
                >&nbsp;use information such as POS tags, tense, and word
                forms.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 546.67px;
                    height: 150.67px;
                "
                ><img
                    alt="Feature 
Character affixes 
Nominalization suffix 
Part of speech 
Light verb 
Subject syntactic category 
Morphological stem 
Verb root 
WordNet hypernyms 
Explanation 
Character-level prefixes and suffixes of target word 
Character-level suffixes for nominalizations (e.g., -tion) 
Part of speech of the target word 
Binary feature indicating that the target is governed by a light verb 
Syntactic category of the subject of the sentence 
Stemmed version of the target word 
Root form of the verb basis for a nominalization 
Hypernym set for the target "
                    src="assets/natural-language-processing/image93.png"
                    style="
                        width: 546.67px;
                        height: 150.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Template Filling</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">script</span
            ><span>&nbsp;is an </span><span class="c22">abstract</span
            ><span
                >&nbsp;sequence of events, participants, and roles. Scripts can
                be represented as </span
            ><span class="c11">templates</span
            ><span>&nbsp;consisting of fixed sets of </span
            ><span class="c11">slots</span
            ><span>&nbsp;that take inputs called</span
            ><span class="c11">&nbsp;slot-fillers</span
            ><span>. The task of</span
            ><span class="c11">&nbsp;template filling</span
            ><span class="c0"
                >&nbsp;involves finding documents that contain scripts and
                filling slots with slot-fillers extracted from the text.</span
            >
        </p>
        <ul class="c6 lst-kix_lonii9w9lawn-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Template Recognition</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_lonii9w9lawn-1 start">
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >training data consists predefined templates with annotated
                    slot-fillers</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0">matching scripts are found in some text</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >features are extracted from each labelled sentence that
                    fills a slot</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >features include tokens, embeddings, word shapes, POS tags,
                    and NER tags</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >a classifier is trained to find templates</span
                >
            </li>
        </ul>
        <ul class="c6 lst-kix_lonii9w9lawn-0">
            <li class="c2 c18 li-bullet-0">
                <span class="c3">Role-Filler Extraction</span>
            </li>
        </ul>
        <ul class="c6 lst-kix_lonii9w9lawn-1 start">
            <li class="c2 c32 li-bullet-0">
                <span>training data contains roles such as </span
                ><span class="c8 c1">airline, amount</span>
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >features are extracted from each labelled sentence that
                    contains a role</span
                >
            </li>
            <li class="c2 c32 li-bullet-0">
                <span class="c0"
                    >a binary role classifier is trained on the labelled
                    training data</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <h1 class="c74" id="h.4vtctm59oqhv">
            <span class="c34 c29">Semantic Role Labelling</span>
        </h1>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Semantic roles</span
            ><span
                >&nbsp;are relations between arguments and predicates,
                representing abstract concepts such as causation, experience,
                and effect. </span
            ><span class="c22">Proto-agent</span
            ><span
                >&nbsp;arguments display agent-like properties such as being
                violationally involved in an event and causing an event or
                change of state. </span
            ><span class="c22">Proto-patient </span
            ><span class="c0"
                >arguments display patient-like properties such as undergoing a
                change of state and being affected by other participants.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 418.67px;
                    height: 165.33px;
                "
                ><img
                    alt="Thematic Role 
AGENT 
EXPERIENCER 
FORCE 
THEME 
RESULT 
CONTENT 
INSTRUMENT 
BENEFICIARY 
SOURCE 
GOAL 
Definition 
The volitional causer of an event 
The experiencer of an event 
The non-volitional causer of the event 
The participant most directly affected by an event 
The end product of an event 
The proposition or content of a propositional event 
An instrument used in an event 
The beneficiary of an event 
The origin of the object of a transfer event 
The destination of an object of a transfer event "
                    src="assets/natural-language-processing/image132.png"
                    style="
                        width: 418.67px;
                        height: 165.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">PropBank</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Propbank</span
            ><span>&nbsp;is a database of sentences annotated with </span
            ><span class="c22">semantic roles</span
            ><span>. Roles are defined separately for each possible </span
            ><span class="c22">word sense</span
            ><span
                >. Each verb sense has a specific set of rules of the form </span
            ><span class="c1">Arg0, Arg1, Arg2</span
            ><span class="c0">&hellip;</span>
        </p>
        <ul class="c6 lst-kix_irct9hj6xfg4-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c11 c1">Arg0</span><span>&nbsp;- </span
                ><span class="c28 c22">proto-agent</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11 c1">Arg1 </span><span>-</span
                ><span class="c28 c22">&nbsp;proto-patient</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11 c1">Argx </span><span>-</span
                ><span class="c8 c1">&nbsp;optional</span>
            </li>
        </ul>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 392px;
                    height: 132px;
                "
                ><img
                    alt="(19.11) agree.01 
Argo: Agreer 
Argl: 
Arg2: 
Ex l: 
Ex2: 
Proposition 
Other entity agreeing 
[Argo The group] agreed [Arg] it wouldn&#39;t make an offer]. 
[ArgM-TMP Usually] [Argo John] agrees [Arg2 with Mary] 
[Argl on everything]. "
                    src="assets/natural-language-processing/image152.png"
                    style="
                        width: 392px;
                        height: 132px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>PropBank provides additional arguments called </span
            ><span class="c11 c1">ArgMs</span><span>&nbsp;which represent </span
            ><span class="c22">modifications</span><span>&nbsp;or </span
            ><span class="c22">adjunct meanings</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <a id="t.4cfa5e6540cf7b9523cbf85a792bb63def85db26"></a><a id="t.12"></a>
        <table class="c39">
            <tbody>
                <tr class="c17">
                    <td class="c12" colspan="1" rowspan="1">
                        <p class="c2"><span class="c28 c22">Modifier</span></p>
                    </td>
                    <td class="c59" colspan="1" rowspan="1">
                        <p class="c2"><span class="c28 c22">Type</span></p>
                    </td>
                    <td class="c16" colspan="1" rowspan="1">
                        <p class="c2"><span class="c28 c22">Meaning</span></p>
                    </td>
                    <td class="c45" colspan="1" rowspan="1">
                        <p class="c2"><span class="c28 c22">Example</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c12" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">TMP</span></p>
                    </td>
                    <td class="c59" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Temporal</span></p>
                    </td>
                    <td class="c16" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c22">when</span
                            ><span class="c0">&nbsp;an action takes place</span>
                        </p>
                    </td>
                    <td class="c45" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">yesterday evening, now</span>
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c12" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">LOC</span></p>
                    </td>
                    <td class="c59" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Locative</span></p>
                    </td>
                    <td class="c16" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c22">where</span
                            ><span class="c0">&nbsp;action takes place</span>
                        </p>
                    </td>
                    <td class="c45" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >at the museum in San Francisco</span
                            >
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c12" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">MNR</span></p>
                    </td>
                    <td class="c59" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Manner</span></p>
                    </td>
                    <td class="c16" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c22">how</span
                            ><span class="c0"
                                >&nbsp;an action is performed</span
                            >
                        </p>
                    </td>
                    <td class="c45" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >clearly, with much enthusiasm</span
                            >
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c12" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">DIR</span></p>
                    </td>
                    <td class="c59" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Directional</span></p>
                    </td>
                    <td class="c16" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">motion from source to goal</span>
                        </p>
                    </td>
                    <td class="c45" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">down to London</span></p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c12" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">CAU</span></p>
                    </td>
                    <td class="c59" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Cause</span></p>
                    </td>
                    <td class="c16" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">reason for an action</span>
                        </p>
                    </td>
                    <td class="c45" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">as a result of the interview</span>
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c12" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">REC</span></p>
                    </td>
                    <td class="c59" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Reciprocal</span></p>
                    </td>
                    <td class="c16" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">reflexes and reciprocals</span>
                        </p>
                    </td>
                    <td class="c45" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0"
                                >himself, themselves, together</span
                            >
                        </p>
                    </td>
                </tr>
                <tr class="c17">
                    <td class="c12" colspan="1" rowspan="1">
                        <p class="c2"><span class="c3">ADV</span></p>
                    </td>
                    <td class="c59" colspan="1" rowspan="1">
                        <p class="c2"><span class="c0">Adverbial</span></p>
                    </td>
                    <td class="c16" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">miscellaneous modifier</span>
                        </p>
                    </td>
                    <td class="c45" colspan="1" rowspan="1">
                        <p class="c2">
                            <span class="c0">happily, she attended</span>
                        </p>
                    </td>
                </tr>
            </tbody>
        </table>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">FrameNet</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">FrameNet</span><span>&nbsp;is a </span
            ><span class="c22">semantic role-labelling</span
            ><span>&nbsp;database that stores relations as frames. A</span
            ><span class="c11">&nbsp;frame</span
            ><span
                >&nbsp;is a background knowledge structure that connects a set
                of words using </span
            ><span class="c22">semantic roles</span><span>&nbsp;called </span
            ><span class="c11">frame elements</span
            ><span>. For example, the </span><span class="c11">role </span
            ><span class="c1">change_position_on_a_scale</span
            ><span
                >&nbsp;consists of words that indicate a change of position on a
                scale </span
            ><span class="c1">attribute</span
            ><span>&nbsp;from a starting point </span
            ><span class="c1">initial_value</span
            ><span>&nbsp;to an end point </span
            ><span class="c1">final_value</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 406.67px;
                    height: 140px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image161.png"
                    style="
                        width: 406.67px;
                        height: 140px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 502.67px;
                    height: 258.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image145.png"
                    style="
                        width: 502.67px;
                        height: 258.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Semantic Role Labelling</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Semantic Role Labelling (SRL)</span
            ><span
                >&nbsp;is the task of extracting semantic roles for arguments
                and predicates in a sentence. A feature-based SRL algorithm
                extracts </span
            ><span class="c22">features</span
            ><span>&nbsp;from labelled words in a parse tree. A </span
            ><span class="c22">classifier</span
            ><span>&nbsp;is then trained to predict a </span
            ><span class="c22">semantic role</span
            ><span class="c0"
                >&nbsp;for each constituent using the features.</span
            >
        </p>
        <ul class="c6 lst-kix_11a5zwh1mb1e-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c22">X </span
                ><span class="c0">= feature template for a parse tree</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c22">Y </span
                ><span class="c0">= sequence of SRL tags</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 426.67px;
                    height: 122.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image189.png"
                    style="
                        width: 426.67px;
                        height: 122.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0"
                >The classification task can also be broken down into multiple
                steps:</span
            >
        </p>
        <ul class="c6 lst-kix_cj1d5ceixf9-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Pruning</span
                ><span class="c0"
                    >&nbsp;- removal of irrelevant words before training</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Identification</span
                ><span
                    >&nbsp;- binary classification of each node as an argument
                    to be labelled or </span
                ><span class="c8 c1">none</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Classification</span
                ><span class="c0"
                    >&nbsp;- multiclass classification of constituents labelled
                    as arguments</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Global Assignment</span
                ><span class="c0"
                    >&nbsp;- optimal role assignment using Viterbi or other
                    constraint-based methods</span
                >
            </li>
        </ul>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 596px;
                    height: 242.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image221.png"
                    style="
                        width: 596px;
                        height: 242.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Most SRL systems use a generalisation of</span
            ><span class="c11">&nbsp;core feature templates</span
            ><span>. For the sentence</span
            ><span class="c1"
                >&nbsp;&quot;The San Francisco Examiner issued a special edition
                around noon yesterday&quot;</span
            ><span class="c0">, the following features could be used:</span>
        </p>
        <ul class="c6 lst-kix_gco3y0qg3t3c-0 start">
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c22">governing predicate</span
                ><span>, in this case the verb </span
                ><span class="c8 c1">issued</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c22">phrase type</span
                ><span>&nbsp;of the constituent, in this case </span
                ><span class="c1">NP</span><span>&nbsp;or </span
                ><span class="c8 c1">NP-SBJ</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c22">headword </span
                ><span>of the constituent, such as </span
                ><span class="c8 c1">Examiner</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c22">part of speech </span
                ><span>of the headword, in this case </span
                ><span class="c8 c1">NNP</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c22">path </span
                ><span
                    >in the parse tree from the constituent to the predicate,
                    such as </span
                ><span class="c8 c1">NP-S-VP-VBD</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c22">voice </span
                ><span>of the clause, in this case </span
                ><span class="c8 c1">active</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c22">position </span
                ><span>of the constituent, such as </span
                ><span class="c8 c1">before, after</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span>the </span><span class="c22">subcategorisation</span
                ><span class="c22">&nbsp;</span
                ><span>of the predicate, such as </span
                ><span class="c1">VP </span><span class="c1">&rarr;</span
                ><span class="c8 c1">&nbsp;VBD NP PP</span>
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c26">Neural Networks for Semantic Role Labelling</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">neural sequence labelling</span
            ><span>&nbsp;algorithm aims to compute the </span
            ><span class="c22">most probable tag sequence </span
            ><img src="assets/natural-language-processing/image3.png" /><span class="c22">&nbsp;</span
            ><span>given an input sequence </span><span class="c1">w</span
            ><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 181.33px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image178.png"
                    style="
                        width: 181.33px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Each</span><span class="c11">&nbsp;input</span
            ><span>&nbsp;word is mapped to pretrained BERT </span
            ><span class="c11">embeddings</span
            ><span>&nbsp;and associated with a </span
            ><span class="c11">flag</span
            ><span
                >&nbsp;indicating whether it is a predicate. The concatenated
                embeddings are passed through an </span
            ><span class="c11">LSTM network</span
            ><span>. The output embeddings and predicate embeddings are </span
            ><span class="c11">concatenated</span
            ><span>&nbsp;and passed through an</span
            ><span class="c11">&nbsp;MLP</span><span>&nbsp;with a</span
            ><span class="c11">&nbsp;softmax</span
            ><span>&nbsp;function which outputs a </span
            ><span class="c11">probability</span
            ><span class="c0">&nbsp;for each tag.</span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 461.33px;
                    height: 221.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image119.png"
                    style="
                        width: 461.33px;
                        height: 221.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp; </span></p>
        <h1 class="c74" id="h.gszlvig04zp2">
            <span class="c34 c29">Question Answering</span>
        </h1>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Information Retrieval (IR)</span
            ><span class="c0"
                >&nbsp;is the retrieval of media based on the information a user
                needs. The user provides a query to the retrieval system, which
                returns a set of relevant documents from a database. Documents
                include web pages, scientific papers, news articles, or
                paragraphs.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">IR-Based Factoid QA</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">IR-Based QA/Open Domain QA</span
            ><span class="c0"
                >&nbsp;answers questions by finding short text segments from
                documents such as web pages. Factoid questions always have short
                answers such as noun phrases and verb phrases.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 472px;
                    height: 94.67px;
                "
                ><img
                    alt="Question 
Where is the Louvre Museum located? 
What are the names of Odin&#39;s ravens? 
What kind of nuts are used in marzipan? 
What instrument did Max Roach play? 
What&#39;s the official language of Algeria? 
Answer 
in Paris, France 
Huginn and Muninn 
almonds 
drums 
Arabic "
                    src="assets/natural-language-processing/image75.png"
                    style="
                        width: 472px;
                        height: 94.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Datasets</span
            ><span
                >&nbsp;for IR-based QA are typically created by developing </span
            ><span class="c22">reading comprehension datasets</span
            ><span>&nbsp;containing tuples of the form </span
            ><span class="c1">(passage, question, answer)</span
            ><span class="c0"
                >. This type of data can be used to train a reader to predict a
                span in the passage as the answer, given a passage and
                question.</span
            >
        </p>
        <ul class="c6 lst-kix_kskyc859j45w-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c11">SQuAD</span
                ><span class="c0"
                    >&nbsp;contains 100,000 question-answer pairs based on 500+
                    articles from Wikipedia and associated questions whose
                    answers are text spans from the passages</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">HotspotQA</span
                ><span class="c0"
                    >&nbsp;contains 300,000 crowd-sourced questions and answers.
                    Some of these are multi-hop questions requiring multiple
                    documents to answer</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Natural Questions</span
                ><span class="c0"
                    >&nbsp;incorporates 300,000 real anonymised Google search
                    queries, presented as a query, Wikipedia page, and annotated
                    answer</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">TyDi QA</span
                ><span class="c0"
                    >&nbsp;is a multilingual database over 11 languages
                    containing 240,000 question and answer pairs from Wikipedia
                    and crowd-sourcing</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>IR-Based QA used the </span
            ><span class="c11">retrieve and read</span
            ><span>&nbsp;model. In the </span><span class="c22">retrieval</span
            ><span
                >&nbsp;stage, it finds relevant passages from a text database.
                In the</span
            ><span class="c22">&nbsp;read</span><span>&nbsp;stage, a </span
            ><span class="c22">neural reading comprehension</span
            ><span class="c0"
                >&nbsp;algorithm passes over each passage to find spans of text
                that are likely to answer the question.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 472px;
                    height: 129.33px;
                "
                ><img
                    alt="Q: When was 
the premiere of 
The Magic Flute? 
Retriever 
Indexed Docs 
Reader 
BERT 
A: 
1791 
Relevant "
                    src="assets/natural-language-processing/image188.png"
                    style="
                        width: 472px;
                        height: 129.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c56 c80">
            <span class="c22">Retrieve and read</span
            ><span>&nbsp;is typically performed by </span
            ><span class="c11">span labelling</span
            ><span
                >. The algorithm identifies a text span in a passage that
                constitutes an answer. A </span
            ><span class="c22">neural algorithm</span
            ><span>&nbsp;is given a question </span><span class="c1">q </span
            ><span>containing </span><span class="c1">n </span
            ><span>tokens </span><span class="c1">q</span
            ><span class="c7 c1">1</span><span class="c1">&hellip;q</span
            ><span class="c7 c1">n</span><span>&nbsp;and a passage </span
            ><span class="c1">p </span><span>of </span><span class="c1">m </span
            ><span>tokens </span><span class="c1">p</span
            ><span class="c7 c1">1</span><span class="c1">&hellip;p</span
            ><span class="c7 c1">m</span
            ><span>. It then computes the probability </span
            ><span class="c11 c1">P(a|q,p) </span
            ><span>that each possible text span </span><span class="c1">a </span
            ><span>is the answer. If each span</span
            ><span class="c22">&nbsp;starts</span><span>&nbsp;at position </span
            ><span class="c1">a</span><span class="c7 c1">s</span
            ><span>&nbsp;and </span><span class="c22">ends</span
            ><span>&nbsp;at position </span><span class="c1">a</span
            ><span class="c7 c1">e</span
            ><span>, the probability can be estimated as </span
            ><span class="c11 c1">P(a|q,p) = P</span
            ><span class="c7 c11 c1">start</span><span class="c11 c1">(a</span
            ><span class="c7 c11 c1">s</span><span class="c11 c1">|q,p) P</span
            ><span class="c7 c11 c1">end</span><span class="c11 c1">(a</span
            ><span class="c7 c11 c1">e</span><span class="c11 c1">|q,p)</span
            ><span>. For each token </span><span class="c1">p</span
            ><span class="c7 c1">i</span
            ><span>&nbsp;in the passage, the algorithm computes the</span
            ><span class="c22">&nbsp;start</span><span>&nbsp;and</span
            ><span class="c22">&nbsp;end</span><span>&nbsp;probabilities </span
            ><span class="c11 c1">p</span><span class="c7 c11 c1">start</span
            ><span>&nbsp;and </span><span class="c11 c1">p</span
            ><span class="c7 c11 c1">end</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A standard </span><span class="c22">baseline</span
            ><span>&nbsp;algorithm will pass the </span
            ><span class="c11">question and passage</span
            ><span
                >&nbsp;to an encoder such as BERT as strings separated with the
                [SEP] token. It then trains a network layer to</span
            ><span class="c11">&nbsp;predict</span><span>&nbsp;the</span
            ><span class="c22">&nbsp;start</span><span>&nbsp;and</span
            ><span class="c22">&nbsp;end</span
            ><span
                >&nbsp;position of the text span. These are represented by the </span
            ><span class="c11">vector embeddings </span
            ><span class="c1">S </span><span>and </span><span class="c1">E</span
            ><span>. For each </span><span class="c11">output token </span
            ><span class="c11 c1">p</span><span class="c7 c11 c1">i</span
            ><span>, the algorithm computes the </span
            ><span class="c22">dot product</span><span>&nbsp;between </span
            ><span class="c1">S </span><span>and </span><span class="c1">p</span
            ><span class="c7 c1">i</span><span>, then uses</span
            ><span class="c22">&nbsp;softmax</span
            ><span>&nbsp;to find its </span><span class="c11">probability</span
            ><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 193.33px;
                    height: 37.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image204.png"
                    style="
                        width: 193.33px;
                        height: 37.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 188px;
                    height: 37.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image105.png"
                    style="
                        width: 188px;
                        height: 37.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">score</span
            ><span>&nbsp;for a </span><span class="c22">span</span
            ><span>&nbsp;from position </span><span class="c1">i </span
            ><span>to </span><span class="c1">j </span><span>is </span
            ><img src="assets/natural-language-processing/image23.png" /><span class="c1">.</span
            ><span>The </span><span class="c22">output</span
            ><span>&nbsp;with the highest </span
            ><span class="c11">probability</span
            ><span>&nbsp;is selected as the final model</span
            ><span class="c11">&nbsp;prediction</span
            ><span>. The model is trained using</span
            ><span class="c11">&nbsp;log-loss</span
            ><span>&nbsp;with respect to the </span
            ><span class="c22">start</span><span>&nbsp;and</span
            ><span class="c22">&nbsp;end</span
            ><span class="c0">&nbsp;positions for each text span.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 218.67px;
                    height: 18.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image76.png"
                    style="
                        width: 218.67px;
                        height: 18.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 398.67px;
                    height: 216px;
                "
                ><img
                    alt="Pstarti 
Encoder (BERT) 
q [SEP] PI 
Pendl 
P 
[CLS] 
Question 
Passage "
                    src="assets/natural-language-processing/image177.png"
                    style="
                        width: 398.67px;
                        height: 216px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Entity Linking</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c11">Entity linking</span
            ><span
                >&nbsp;is the task of associating a mention in text with a
                representation of a real-world entity. </span
            ><span class="c11">TAGME</span
            ><span
                >&nbsp;is a simple baseline algorithm which uses data from
                Wikipedia pages. It first creates a </span
            ><span class="c11">catalog</span
            ><span
                >&nbsp;of Wikipedia pages and indexes them in a standard IR
                engine. For each Wikipedia page </span
            ><span class="c1">e</span><span>, the algorithm computes an </span
            ><span class="c22">in-link</span
            ><span
                >&nbsp;count, which is the total number of links from other
                pages that point to </span
            ><span class="c1">e</span
            ><span
                >. Finally, the algorithm requires an anchor dictionary.
                The</span
            ><span class="c11">&nbsp;anchor dictionary</span
            ><span>&nbsp;contains the </span
            ><span class="c11">anchor texts</span
            ><span
                >&nbsp;for each page. These are hyperlinked text spans from
                other pages that point to the current page </span
            ><span class="c1">e</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Given an input question, TAGME detects mentions by</span
            ><span class="c11">&nbsp;querying</span><span>&nbsp;the </span
            ><span class="c11">anchor dictionary</span
            ><span
                >&nbsp;for each token sequence. If the mention is ambiguous
                (points to more than one page), the algorithm </span
            ><span class="c11">disambiguates</span
            ><span
                >&nbsp;the mention by calculating a score based on two metrics.
                The </span
            ><span class="c11">prior probability </span
            ><span class="c1">P(e|a) </span
            ><span
                >is the probability that the span refers to the entity. For each
                page, the probability that an anchor</span
            ><span class="c1">&nbsp;a </span><span>points to </span
            ><span class="c1">e </span
            ><span>is the ratio of pages linked to </span
            ><span class="c1">e </span><span>with anchor text </span
            ><span class="c1">a </span
            ><span>and the total number of occurrences of </span
            ><span class="c1">a </span><span class="c0">as an anchor:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 362.67px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image166.png"
                    style="
                        width: 362.67px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Given a question </span><span class="c1">q</span
            ><span>, the </span><span class="c11">relatedness</span
            ><span>&nbsp;for an entity </span><span class="c1">e </span
            ><span>and anchor </span><span class="c1">a </span
            ><span>is the weighted average relatedness between </span
            ><span class="c1">e </span><span>and all other entities in </span
            ><span class="c1">q</span><span>. </span
            ><span class="c1">in(x) </span
            ><span>is the set of Wikipedia pages pointing to </span
            ><span class="c1">x </span><span>and </span
            ><span class="c1">W </span
            ><span
                >is the set of all Wikipedia pages in the collection. To
                calculate the relatedness between two entities </span
            ><span class="c1">A</span><span>&nbsp;and </span
            ><span class="c1">B</span><span class="c0">:</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 470.67px;
                    height: 36px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image96.png"
                    style="
                        width: 470.67px;
                        height: 36px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>To compute a</span><span class="c11">&nbsp;score</span
            ><span>&nbsp;for each entity, TAGME combines </span
            ><span class="c22">relatedness</span><span>&nbsp;and </span
            ><span class="c22">prior probability</span
            ><span>&nbsp;to select entities with high</span
            ><span class="c22">&nbsp;relatedness</span
            ><span
                >&nbsp;scores, then selecting the entity with the highest </span
            ><span class="c22">prior probability</span
            ><span class="c0">&nbsp;from this set.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Neural Entity Linking</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>A </span><span class="c11">neural graph-based linking</span
            ><span>&nbsp;algorithm encodes a </span
            ><span class="c22">candidate span</span><span>, encodes an </span
            ><span class="c22">entity</span><span>, and computes the </span
            ><span class="c22">dot product</span
            ><span>&nbsp;between the encodings. This allows </span
            ><span class="c22">embeddings</span
            ><span
                >&nbsp;for all the entities in the knowledge base to be
                pre-computed and cached. Given a question </span
            ><span class="c1">q</span
            ><span
                >&nbsp;and a set of candidate mentions from Wikipedia text, the
                algorithm outputs tuples of the form </span
            ><span class="c11 c1">(e, m</span><span class="c7 c11 c1">s</span
            ><span class="c11 c1">, m</span><span class="c7 c11 c1">e</span
            ><span class="c11 c1">)</span><span>&nbsp;where </span
            ><span class="c1">e</span><span>&nbsp;= entity, </span
            ><span class="c1">m</span><span class="c7 c1">s </span
            ><span>= mention start, </span><span class="c1">m</span
            ><span class="c7 c1">e</span
            ><span class="c0">&nbsp;= mention end.</span>
        </p>
        <ul class="c6 lst-kix_im63tupg9cyx-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c22">X </span
                ><span class="c0">= question, entity candidate</span>
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c22">Y</span
                ><span class="c0"
                    >&nbsp;= entity mention classifier + entity linker</span
                >
            </li>
        </ul>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 509.33px;
                    height: 236px;
                "
                ><img
                    alt='Q : When did Shaq come to the nba? 
P("Shaquille O&#39; Neal"lQ, &#39;shaq&#39; is a mention) 
P(&#39;shaq&#39; is a mentionlQ) 
Mention classifier + scorer 
Question Encoder 
Inner product 
Entity Encoder '
                    src="assets/natural-language-processing/image171.png"
                    style="
                        width: 509.33px;
                        height: 236px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>To get a </span><span class="c1">h</span
            ><span>-dimensional </span><span class="c11">embedding</span
            ><span class="c0"
                >&nbsp;for each question token, the algorithm runs the question
                through BERT.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 278.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image56.png"
                    style="
                        width: 278.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>It then computes the probability of each span </span
            ><span class="c1">[i,j] </span><span>in </span
            ><span class="c1">q </span><span>being an </span
            ><span class="c22">entity mention</span
            ><span
                >. First it must compute the score for the span being the </span
            ><span class="c11">start or end of a mention</span
            ><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span class="c1"
                >&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span
            ><span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 198.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image203.png"
                    style="
                        width: 198.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c25 c10"><span class="c0"></span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 564px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image27.png"
                    style="
                        width: 564px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c1">w</span><span class="c7 c1">start</span
            ><span class="c7 c1">&nbsp;</span><span>and </span
            ><span class="c1">w</span><span class="c7 c1">end</span
            ><span
                >&nbsp;are vectors learned during training. Another trained
                embedding </span
            ><span class="c1">w</span><span class="c7 c1">mention</span
            ><span>&nbsp;is used to compute a score for each token being </span
            ><span class="c11">part of a mention</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 218.67px;
                    height: 17.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image40.png"
                    style="
                        width: 218.67px;
                        height: 17.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The three scores are combined to create the</span
            ><span class="c11">&nbsp;</span><span class="c11">mention</span
            ><span class="c11">&nbsp;probability</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 354.67px;
                    height: 53.33px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image222.png"
                    style="
                        width: 354.67px;
                        height: 53.33px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The algorithm</span><span class="c11">&nbsp;links</span
            ><span
                >&nbsp;mentions to entities by computing dot product similarity
                between each entity </span
            ><span class="c1">e </span><span>and span </span
            ><span class="c1">[i,j]</span><span class="c0">.</span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 198.67px;
                    height: 46.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image33.png"
                    style="
                        width: 198.67px;
                        height: 46.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 166.67px;
                    height: 18.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image228.png"
                    style="
                        width: 166.67px;
                        height: 18.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>Finally, it takes a </span><span class="c11">softmax</span
            ><span class="c0"
                >&nbsp;distribution over entities for each span.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 236px;
                    height: 34.67px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image239.png"
                    style="
                        width: 236px;
                        height: 34.67px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2"><span class="c26">Evaluation of Factoid Answers</span></p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span>The </span><span class="c11">mean reciprocal rank (MRR)</span
            ><span>&nbsp;compares a set of </span
            ><span class="c22">gold answers</span><span>&nbsp;and a </span
            ><span class="c22">ranked output of answers</span
            ><span
                >&nbsp;by calculating the reciprocal. The score of a system is
                the </span
            ><span class="c22">average</span
            ><span class="c0"
                >&nbsp;of the MRR scores across questions in a set.</span
            >
        </p>
        <p class="c2"><span class="c0">&nbsp;</span></p>
        <p class="c2 c10">
            <span
                style="
                    overflow: hidden;
                    display: inline-block;
                    margin: 0px 0px;
                    border: 0px solid #000000;
                    transform: rotate(0rad) translateZ(0px);
                    -webkit-transform: rotate(0rad) translateZ(0px);
                    width: 212px;
                    height: 48px;
                "
                ><img
                    alt=""
                    src="assets/natural-language-processing/image113.png"
                    style="
                        width: 212px;
                        height: 48px;
                        margin-left: 0px;
                        margin-top: 0px;
                        transform: rotate(0rad) translateZ(0px);
                        -webkit-transform: rotate(0rad) translateZ(0px);
                    "
                    title=""
            /></span>
        </p>
        <p class="c2 c10"><span class="c0">&nbsp;</span></p>
        <p class="c2">
            <span class="c0"
                >Reading comprehension systems are often evaluated using two
                metrics:</span
            >
        </p>
        <ul class="c6 lst-kix_t1vynowmw5mg-0 start">
            <li class="c2 c18 li-bullet-0">
                <span class="c11">Exact Match</span
                ><span class="c0"
                    >&nbsp;- the percentage of predicted answers that match the
                    gold answer exactly</span
                >
            </li>
            <li class="c2 c18 li-bullet-0">
                <span class="c11">F1 Score</span
                ><span class="c0"
                    >&nbsp;- the average overlap between predicted and gold
                    answers</span
                >
            </li>
        </ul>
        <p class="c2 c25"><span class="c0"></span></p>
        <hr />
        <p class="c2 c25"><span class="c0"></span></p>
        <p class="c2 c25"><span class="c0"></span></p>
        <div class="c85">
            <p class="c56">
                <a href="#cmnt_ref1" id="cmnt1">[a]</a
                ><span class="c0"
                    >&lambda;(w_i-1) should be&nbsp;&lambda;_i-1&nbsp;</span
                >
            </p>
            <p class="c56 c25"><span class="c0"></span></p>
            <p class="c56">
                <span class="c0"
                    >this addition&nbsp;bit&nbsp;is basically interpolation as
                    far as I can tell</span
                >
            </p>
        </div>
    </body>
</html>
